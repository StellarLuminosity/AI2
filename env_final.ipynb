{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuOF52AMoQTH"
      },
      "source": [
        "# Welcome to UTMIST AI2!\n",
        "\n",
        "[Technical Guide Notebook](https://colab.research.google.com/drive/1qMs336DclBwdn6JBASa5ioDIfvenW8Ha?usp=sharing#scrollTo=-XAOXXMPTiHJ)\n",
        "\n",
        "[Introductory RL Notebook](https://colab.research.google.com/drive/1JRQFLU5jkMrIJ5cWs3xKEO0e9QKuE0Hi#scrollTo=9UCawVuAI3k0)\n",
        "\n",
        "[Discord Server](https://discord.com/invite/TTGB62BE9U)\n",
        "\n",
        "The link to the **LATEST VERSION** of this Colab will always be [here](https://docs.google.com/document/d/1SvlgQSUMLoO3cNx26hzViZOYVPAGa3ECrUSvUyVKvR4/edit?usp=sharing).\n",
        "\n",
        "Credits:\n",
        "- General Event Organization: Asad, Efe, Andrew, Matthew, Kaden\n",
        "- Notebook code: Kaden, Martin, Andrew\n",
        "- Notebook art/animations: EchoTecho, Andy\n",
        "- Website code: Zain, Sarva, Adam, Aina\n",
        "- Workshops: Jessica, Jingmin, Asad, Tyler, Wai Lim, Napasorn, Sara, San, Alden\n",
        "- Tournament Server: Ambrose, Doga, Steven\n",
        "- Technical guide + Conference brochure: Matthew, Caitlin, Lucie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNxUlQXyFNKb"
      },
      "source": [
        "# PATCH: Run this cell first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "zYnCLbw8FS3x"
      },
      "outputs": [],
      "source": [
        "# # Delete assets.zip and /content/assets/\n",
        "# import shutil, gdown, os\n",
        "# import zipfile\n",
        "\n",
        "# if os.path.exists('assets'):\n",
        "#     shutil.rmtree('assets')\n",
        "# if os.path.exists('assets.zip'):\n",
        "#     os.remove('assets.zip')\n",
        "\n",
        "# # Redownload from Drive\n",
        "# data_path = \"assets.zip\"\n",
        "# print(\"Downloading assets.zip...\")\n",
        "# url = \"https://drive.google.com/file/d/1F2MJQ5enUPVtyi3s410PUuv8LiWr8qCz/view?usp=sharing\"\n",
        "# gdown.download(url, output=data_path, fuzzy=True)\n",
        "\n",
        "# # Unzip\n",
        "# with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall('.')\n",
        "\n",
        "# # Delete attacks.zip and /content/attacks/\n",
        "# if os.path.exists('attacks'):\n",
        "#     shutil.rmtree('attacks')\n",
        "# if os.path.exists('attacks.zip'):\n",
        "#     os.remove('attacks.zip')\n",
        "\n",
        "# # Redownload from Drive\n",
        "# data_path = \"attacks.zip\"\n",
        "# print(\"Downloading attacks.zip...\")\n",
        "# url = \"https://drive.google.com/file/d/1LAOL8sYCUfsCk3TEA3vvyJCLSl0EdwYB/view?usp=sharing\"\n",
        "# gdown.download(url, output=data_path, fuzzy=True)\n",
        "\n",
        "# # Unzip\n",
        "# with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
        "#     zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL4UJviLrA9D"
      },
      "source": [
        "# pip installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0d1cDyIvNRB7"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "rKYrsgxVI0LV"
      },
      "outputs": [],
      "source": [
        "# # Download the requirements.txt from Google Drive\n",
        "# import gdown, os\n",
        "# data_path = \"requirements.txt\"\n",
        "# if not os.path.isfile(data_path):\n",
        "#     print(\"Downloading requirements.txt...\")\n",
        "#     url = \"https://drive.google.com/file/d/1-4f6NGWtejcn6Q9wUETelVXMFWaA5X0D/view?usp=sharing\"\n",
        "#     gdown.download(url, output=data_path, fuzzy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "z8SDSRGrRFkf"
      },
      "outputs": [],
      "source": [
        "# Malachite and RL requirements\n",
        "#!pip install torch==2.4.1 gymnasium pygame==2.6.1 pymunk==6.2.1 scikit-image scikit-video sympy==1.5.1 stable_baselines3 sb3-contrib\n",
        "#!pip install memory_profiler==0.61.0\n",
        "#!pip install torch==2.4.1 triton==3.0.0 gymnasium pygame==2.6.1 pymunk==6.2.1 scikit-image scikit-video sympy==1.5.1 stable_baselines3 sb3-contrib jupyter gdown opencv-python\n",
        "\n",
        "#!pip freeze > /content/requirements_v0.txt\n",
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjJxbq1oq8qj"
      },
      "source": [
        "# Competition Code (DO NOT EDIT)\n",
        "Please **run this cell** to set your Jupyter Notebook up with all necessary code. Then feel free to move to the `SUBMISSION` sections of this notebook for further instruction.\n",
        "\n",
        "Note: This cell may take time to run, as it installs the necessary modules then imports them.\n",
        "\n",
        "## Summary of content:\n",
        "\n",
        "These cells contain our custom Multi-Agent Reinforcement Learning (MARL) Solution, Malachite, alongside an implementation of a 1v1 platform fighter we've titled Warehouse Brawl. You may look through these cells to get a better sense for the dynamics and functionality of the enviroment, as wel as the various pip installs and modules available for use in your own code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFPXjtaYpiPY"
      },
      "source": [
        "## Malachite (DO NOT MODIFY UNLESS YOU KNOW WHAT YOU'RE DOING)\n",
        "The following cells store some code for our custom Multi-Agent Reinforcement Learning (MARL) Solution, called Malachite. It extends some Stable-Baselines 3 functionality to Multi-Agent systems in the context of the AI2 Tournament.\n",
        "\n",
        "You would only want to modify this if you want to add custom rewards and are dissatisfied with the current flexible rewards system. That's ok! But note that this default environment is what will be used in the tournament, and you will **NOT** have access to any additional data or modifications you may choose to make here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7ekY33Ka_h8"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "RWDiccN4uNeo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from typing import TYPE_CHECKING, Any, Generic, \\\n",
        " SupportsFloat, TypeVar, Type, Optional, List, Dict, Callable\n",
        "from enum import Enum, auto\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, field, MISSING\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from typing import Tuple, Any\n",
        "\n",
        "from PIL import Image, ImageSequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gdown, os, math, random, shutil, json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import gymnasium\n",
        "from gymnasium import spaces\n",
        "\n",
        "import pygame\n",
        "import pygame.gfxdraw\n",
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
        "from pymunk.vec2d import Vec2d\n",
        "\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "import skvideo\n",
        "import skvideo.io\n",
        "from IPython.display import Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EUO-SS6bCRS"
      },
      "source": [
        "### MalachiteEnv Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "t0TsFsnsp0mD"
      },
      "outputs": [],
      "source": [
        "ObsType = TypeVar(\"ObsType\")\n",
        "ActType = TypeVar(\"ActType\")\n",
        "AgentID = TypeVar(\"AgentID\")\n",
        "\n",
        "# Reference PettingZoo AECEnv\n",
        "class MalachiteEnv(ABC, Generic[ObsType, ActType, AgentID]):\n",
        "\n",
        "    agents: list[AgentID]\n",
        "\n",
        "    action_spaces: dict[AgentID, gymnasium.spaces.Space]\n",
        "    observation_spaces: dict[\n",
        "        AgentID, gymnasium.spaces.Space\n",
        "    ]\n",
        "\n",
        "    # Whether each agent has just reached a terminal state\n",
        "    terminations: dict[AgentID, bool]\n",
        "    truncations: dict[AgentID, bool]\n",
        "    rewards: dict[AgentID, float]  # Reward from the last step for each agent\n",
        "    # Cumulative rewards for each agent\n",
        "    _cumulative_rewards: dict[AgentID, float]\n",
        "    infos: dict[\n",
        "        AgentID, dict[str, Any]\n",
        "    ]  # Additional information from the last step for each agent\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step(self, action: dict[AgentID, ActType]) -> tuple[ObsType,]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self, seed: int | None = None, options: dict | None = None) -> None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def observe(self, agent: AgentID) -> ObsType | None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def render(self) -> None | np.ndarray | str | list:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def close(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def show_image(self, image: np.ndarray) -> None:\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def observation_space(self, agent: AgentID) -> gymnasium.spaces.Space:\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def action_space(self, agent: AgentID) -> gymnasium.spaces.Space:\n",
        "        return self.action_spaces[agent]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBfJLZ-jpf8M"
      },
      "source": [
        "## Environment (DO NOT MODIFY)\n",
        "Defines the environment for the game. Code adapted from the following sources:\n",
        "- [Shootout AI](https://github.com/ajwm8103/shootoutai/tree/main)\n",
        "- [Diffusion Policy](https://diffusion-policy.cs.columbia.edu/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkU-Jg8iZSTC"
      },
      "source": [
        "### Low High Class\n",
        "Helps structure observation and action spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "3QHEnEBSZRky"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ActHelper():\n",
        "    low: list[Any] = field(default_factory=list)\n",
        "    high: list[Any] = field(default_factory=list)\n",
        "    sections: Dict[str, int] = field(default_factory=dict)\n",
        "\n",
        "    def get_as_np(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Return the low and high bounds as NumPy arrays.\"\"\"\n",
        "        return np.array(self.low), np.array(self.high)\n",
        "\n",
        "    def get_as_box(self) -> spaces.Box:\n",
        "        lowarray, higharray = self.get_as_np()\n",
        "        return spaces.Box(\n",
        "            low=lowarray,\n",
        "            high=higharray,\n",
        "            shape=lowarray.shape,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def zeros(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns a zeros vector with the same total dimension as defined by the low vector.\n",
        "        \"\"\"\n",
        "        return np.zeros(len(self.low))\n",
        "\n",
        "    def add_key(self, name: str):\n",
        "        \"\"\"\n",
        "        Adds a new section with a label to the overall low and high lists.\n",
        "\n",
        "        Parameters:\n",
        "            name: A string that identifies the section (e.g., \"global_position\").\n",
        "            low_values: A list of low values for this section.\n",
        "            high_values: A list of high values for this section.\n",
        "\n",
        "        The method appends the values to the overall lists and records the indices\n",
        "        where this section is stored. This is later used for observation parsing.\n",
        "        \"\"\"\n",
        "        name = name.lower()\n",
        "        self.low += [0]\n",
        "        self.high += [1]\n",
        "        self.sections[name] = len(self.low)-1\n",
        "\n",
        "    def press_keys(self, keys: str | List[str], action: Optional[np.ndarray]=None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Set a part of the action vector corresponding to the named section.\n",
        "\n",
        "        Parameters:\n",
        "            action: The full action vector (np.ndarray) that will be modified.\n",
        "            partial_action: The values to set for the section.\n",
        "            name: The section name whose slice is to be replaced.\n",
        "\n",
        "        Returns:\n",
        "            The updated action vector.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the partial action's size does not match the section size.\n",
        "        \"\"\"\n",
        "        if isinstance(keys, str):\n",
        "            keys = [keys]\n",
        "        if action is None:\n",
        "            action = self.zeros()\n",
        "\n",
        "        for key in keys:\n",
        "            key = key.lower()\n",
        "            if key not in self.sections:\n",
        "                raise KeyError(f\"Key '{key}' not found in keys: {self.sections.keys()}\")\n",
        "            action[self.sections[key]] = 1\n",
        "        return action\n",
        "\n",
        "    def print_all_sections(self) -> None:\n",
        "        \"\"\"\n",
        "        Prints the names and indices of all sections.\n",
        "        \"\"\"\n",
        "        for name, (start, end) in self.sections.items():\n",
        "            print(f\"{name}: {end - start}\")\n",
        "\n",
        "@dataclass\n",
        "class ObsHelper():\n",
        "    low: list[Any] = field(default_factory=list)\n",
        "    high: list[Any] = field(default_factory=list)\n",
        "    sections: Dict[str, Tuple[int, int]] = field(default_factory=dict)\n",
        "\n",
        "    def get_as_np(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Return the low and high bounds as NumPy arrays.\"\"\"\n",
        "        return np.array(self.low), np.array(self.high)\n",
        "\n",
        "    def get_as_box(self) -> spaces.Box:\n",
        "        lowarray, higharray = self.get_as_np()\n",
        "        return spaces.Box(\n",
        "            low=lowarray,\n",
        "            high=higharray,\n",
        "            shape=lowarray.shape,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def zeros(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns a zeros vector with the same total dimension as defined by the low vector.\n",
        "        \"\"\"\n",
        "        return np.zeros(len(self.low))\n",
        "\n",
        "    def add_section(self, low_values: List[Any], high_values: List[Any], name: str) :\n",
        "        \"\"\"\n",
        "        Adds a new section with a label to the overall low and high lists.\n",
        "\n",
        "        Parameters:\n",
        "            name: A string that identifies the section (e.g., \"global_position\").\n",
        "            low_values: A list of low values for this section.\n",
        "            high_values: A list of high values for this section.\n",
        "\n",
        "        The method appends the values to the overall lists and records the indices\n",
        "        where this section is stored. This is later used for observation parsing.\n",
        "        \"\"\"\n",
        "        name = name.lower()\n",
        "        start_idx = len(self.low)  # Starting index for this section.\n",
        "        self.low += low_values\n",
        "        self.high += high_values\n",
        "        end_idx = len(self.low)    # Ending index (exclusive) for this section.\n",
        "        self.sections[name] = (start_idx, end_idx)\n",
        "\n",
        "    def get_section(self, obs: np.ndarray, name: str) -> np.ndarray:\n",
        "        start, end = self.sections[name]\n",
        "        return obs[start:end]\n",
        "\n",
        "    def print_all_sections(self) -> None:\n",
        "        \"\"\"\n",
        "        Prints the names and indices of all sections.\n",
        "        \"\"\"\n",
        "        for name, (start, end) in self.sections.items():\n",
        "            print(f\"{name}: {end - start}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_2qwFINY3f"
      },
      "source": [
        "### KeyIconPanel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "HyX2pwmPNaEB"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "\n",
        "class KeyIconPanel():\n",
        "    def __init__(self, side: str, edge_percentage: float,\n",
        "                 width_percentage: float, height_percentage: float,\n",
        "                 font_size: int = 12):\n",
        "        \"\"\"\n",
        "        :param side: \"left\" or \"right\". Determines which edge (far left or far right) is positioned at the given percentage.\n",
        "        :param edge_percentage: Fraction of the screen width at which the far edge of the panel is placed.\n",
        "                                For \"left\", this is the left edge; for \"right\", this is the right edge.\n",
        "        :param width_percentage: Panel width as a fraction of screen width.\n",
        "        :param height_percentage: Panel height as a fraction of screen height.\n",
        "        :param font_size: Font size for the key labels.\n",
        "        \"\"\"\n",
        "        self.side = side.lower()\n",
        "        self.edge_percentage = edge_percentage\n",
        "        self.width_percentage = width_percentage\n",
        "        self.height_percentage = height_percentage\n",
        "        self.font_size = font_size\n",
        "        # Define the keys in order: first 4 (W, A, S, D), then space, then 5 (G, H, J, K, L)\n",
        "        self.keys = [\"W\", \"A\", \"S\", \"D\", \"Space\", \"G\", \"H\", \"J\", \"K\", \"L\"]\n",
        "\n",
        "    def draw_key_icon(self, surface, rect: pygame.Rect, key_label: str, pressed: bool, font):\n",
        "        \"\"\"\n",
        "        Draws a key icon in the specified rect.\n",
        "          - Draws a rectangle with a 2-pixel border.\n",
        "          - If pressed, the border and text are red; if not, they are white.\n",
        "        \"\"\"\n",
        "        color = (255, 0, 0) if pressed else (255, 255, 255)\n",
        "        # Draw the rectangle outline\n",
        "        pygame.draw.rect(surface, color, rect, 1)\n",
        "        # Render the key label (centered)\n",
        "        text_surface = font.render(key_label, True, color)\n",
        "        text_rect = text_surface.get_rect(center=rect.center)\n",
        "        surface.blit(text_surface, text_rect)\n",
        "\n",
        "    def draw(self, camera, input_vector: np.ndarray):\n",
        "        \"\"\"\n",
        "        Draws the panel and key icons onto the given canvas.\n",
        "\n",
        "        :param canvas: The pygame.Surface on which to draw.\n",
        "        :param screen_size: Tuple (screen_width, screen_height).\n",
        "        :param input_vector: np.ndarray of booleans or 0/1 with length 10 in the order [W, A, S, D, Space, G, H, J, K, L].\n",
        "        \"\"\"\n",
        "        canvas = camera.canvas\n",
        "        screen_width, screen_height = camera.window_width, camera.window_height\n",
        "\n",
        "        # Calculate panel dimensions\n",
        "        panel_width = screen_width * self.width_percentage\n",
        "        panel_height = screen_height * self.height_percentage\n",
        "\n",
        "        # Determine panel x based on side\n",
        "        if self.side == \"left\":\n",
        "            x = screen_width * self.edge_percentage\n",
        "        elif self.side == \"right\":\n",
        "            x = screen_width * self.edge_percentage - panel_width\n",
        "        else:\n",
        "            # Default to centered horizontally if side is invalid.\n",
        "            x = (screen_width - panel_width) / 2\n",
        "\n",
        "        # For vertical placement, we'll position the panel at 10% from the top.\n",
        "        y = screen_height * 0.2\n",
        "        panel_rect = pygame.Rect(int(x), int(y), int(panel_width), int(panel_height))\n",
        "        # Draw panel background and border\n",
        "        pygame.draw.rect(canvas, (50, 50, 50), panel_rect)  # dark gray background\n",
        "        pygame.draw.rect(canvas, (255, 255, 255), panel_rect, 2)  # white border\n",
        "\n",
        "        # Create a font for the key icons.\n",
        "        font = pygame.font.Font(None, self.font_size)\n",
        "        # Divide the panel vertically into 3 rows.\n",
        "        row_height = panel_rect.height / 3\n",
        "\n",
        "        # Row 1: WASD (first 4 keys)\n",
        "        row1_keys = self.keys[0:4]\n",
        "        row1_count = len(row1_keys)\n",
        "        for idx, key in enumerate(row1_keys):\n",
        "            cell_width = panel_rect.width / row1_count\n",
        "            cell_rect = pygame.Rect(\n",
        "                panel_rect.x + idx * cell_width,\n",
        "                panel_rect.y,\n",
        "                cell_width,\n",
        "                row_height\n",
        "            )\n",
        "            # Add padding for the icon.\n",
        "            icon_rect = cell_rect.inflate(-2, -2)\n",
        "            pressed = bool(input_vector[idx])\n",
        "            self.draw_key_icon(canvas, icon_rect, key, pressed, font)\n",
        "\n",
        "        # Row 2: Spacebar (only one icon)\n",
        "        cell_rect = pygame.Rect(\n",
        "            panel_rect.x,\n",
        "            panel_rect.y + row_height,\n",
        "            panel_rect.width,\n",
        "            row_height\n",
        "        )\n",
        "        # Center the spacebar icon in its cell.\n",
        "        icon_rect = cell_rect.inflate(-2, -2)\n",
        "        pressed = bool(input_vector[4])\n",
        "        self.draw_key_icon(canvas, icon_rect, \"Space\", pressed, font)\n",
        "\n",
        "        # Row 3: GHJKL (last 5 keys)\n",
        "        row3_keys = self.keys[5:10]\n",
        "        row3_count = len(row3_keys)\n",
        "        for idx, key in enumerate(row3_keys):\n",
        "            cell_width = panel_rect.width / row3_count\n",
        "            cell_rect = pygame.Rect(\n",
        "                panel_rect.x + idx * cell_width,\n",
        "                panel_rect.y + 2 * row_height,\n",
        "                cell_width,\n",
        "                row_height\n",
        "            )\n",
        "            icon_rect = cell_rect.inflate(-2, -2)\n",
        "            pressed = bool(input_vector[5 + idx])\n",
        "            self.draw_key_icon(canvas, icon_rect, key, pressed, font)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbPFpq8lNXdN"
      },
      "source": [
        "### UIHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "_OpSupi63L1S"
      },
      "outputs": [],
      "source": [
        "class UIHandler():\n",
        "\n",
        "    def __init__(self, camera):\n",
        "        # Score images\n",
        "\n",
        "        SCALE_FACTOR = 0.11\n",
        "        self.agent_1_score = pygame.image.load('assets/ui/player1ui.png')\n",
        "        self.agent_1_score = pygame.transform.scale(self.agent_1_score, (int(SCALE_FACTOR * self.agent_1_score.get_width()), int(SCALE_FACTOR * self.agent_1_score.get_height())))\n",
        "        self.agent_2_score = pygame.image.load('assets/ui/player2ui.png')\n",
        "        self.agent_2_score = pygame.transform.scale(self.agent_2_score, (int(SCALE_FACTOR * self.agent_2_score.get_width()), int(SCALE_FACTOR * self.agent_2_score.get_height())))\n",
        "\n",
        "        # Life and death images\n",
        "        SCALE_FACTOR_2 = SCALE_FACTOR * 0.375\n",
        "        self.life = pygame.image.load('assets/ui/alicon_alive.png')\n",
        "        self.life = pygame.transform.scale(self.life, (int(SCALE_FACTOR_2 * self.life.get_width()), int(SCALE_FACTOR_2 * self.life.get_height())))\n",
        "        self.death = pygame.image.load('assets/ui/alicon_dead.png')\n",
        "        self.death = pygame.transform.scale(self.death, (int(SCALE_FACTOR_2 * self.death.get_width()), int(SCALE_FACTOR_2 * self.death.get_height())))\n",
        "\n",
        "        self.score_width, self.score_height = self.agent_1_score.get_size()\n",
        "        self.agent_1_score_pos = (10, -10)  # Top-left\n",
        "        self.agent_2_score_pos = (camera.window_width - self.score_width - 10, -10)  # Top-right\n",
        "\n",
        "    def render(self, camera, env):\n",
        "        canvas = camera.canvas\n",
        "\n",
        "        # Score UI positions\n",
        "\n",
        "\n",
        "        # Draw Score UI\n",
        "\n",
        "        canvas.blit(self.agent_1_score, self.agent_1_score_pos)\n",
        "        canvas.blit(self.agent_2_score, self.agent_2_score_pos)\n",
        "\n",
        "        # Agent lives\n",
        "        spacing = self.score_width / 3\n",
        "        for i in range(len(env.players)):\n",
        "            for j in range(env.players[i].stocks):\n",
        "                canvas.blit(self.life, (10+j*spacing + i*(camera.window_width - 1.2 * self.score_width), self.score_height - 30))\n",
        "\n",
        "            # Agent deaths\n",
        "            for j in range(3 - env.players[i].stocks):\n",
        "                canvas.blit(self.death, (10 + 2*spacing - j*spacing + i*(camera.window_width - 1.2 * self.score_width), self.score_height - 30))\n",
        "\n",
        "        self.display_percentages(camera, env)\n",
        "        self.display_team_name(camera, env)\n",
        "\n",
        "    def display_team_name(self, camera, env):\n",
        "        # Define the team name and the bounding rectangle for the text.\n",
        "        team_name = \"Testing this team name\"\n",
        "        # These values can be adjusted to suit your UI layout:\n",
        "        team_rect_1 = pygame.Rect(self.agent_1_score_pos[0] + 0.2 * self.score_width,\n",
        "                                self.agent_1_score_pos[1] + 0.75 * self.score_height,\n",
        "                                0.8 * self.score_width,\n",
        "                                0.2 * self.score_height)\n",
        "        team_rect_2 = pygame.Rect(self.agent_2_score_pos[0] + 0 * self.score_width,\n",
        "                                self.agent_2_score_pos[1] + 0.75 * self.score_height,\n",
        "                                0.8 * self.score_width,\n",
        "                                0.2 * self.score_height)\n",
        "        team_rects = [team_rect_1, team_rect_2]\n",
        "\n",
        "        # Create a font (same as used for percentages or adjust as needed)\n",
        "        font = pygame.font.Font(None, 20)\n",
        "\n",
        "        for i, team_rect in enumerate(team_rects):\n",
        "            # Render the team name and check if it fits in the rectangle.\n",
        "            text = env.agent_1_name if i == 0 else env.agent_2_name\n",
        "            text_surface = font.render(text, True, (255, 255, 255))\n",
        "\n",
        "            # If the text is too wide, shorten it and add an ellipsis.\n",
        "            if text_surface.get_width() > team_rect.width:\n",
        "                # Remove characters until it fits, then add ellipsis.\n",
        "                while text_surface.get_width() > team_rect.width and len(text) > 0:\n",
        "                    text = text[:-1]\n",
        "                    text_surface = font.render(text + \"...\", True, (255, 255, 255))\n",
        "                text = text + \"...\"\n",
        "                text_surface = font.render(text, True, (255, 255, 255))\n",
        "\n",
        "            # Draw a red rectangle outline for the team name.\n",
        "            pygame.draw.rect(camera.canvas, (255, 0, 0), team_rect, 2)\n",
        "\n",
        "            # Center the text in the rectangle and draw it.\n",
        "            text_rect = text_surface.get_rect(center=team_rect.center)\n",
        "            camera.canvas.blit(text_surface, text_rect)\n",
        "\n",
        "    # Percentages (like SSBU)\n",
        "    def display_percentages(self, camera, env):\n",
        "        WHITE = (255, 255, 255)\n",
        "        ORANGE = (255, 165, 0)\n",
        "        RED = (255, 0, 0)\n",
        "        YELLOW = (255, 255, 0)\n",
        "        DARK_RED = (139, 0, 0)\n",
        "\n",
        "        # Agent percentage text\n",
        "        font = pygame.font.Font(None, 35)\n",
        "        # render text & text colours:\n",
        "        for i in range(len(env.players)):\n",
        "            COLOUR = WHITE\n",
        "            if 50 < env.players[i].damage < 100:\n",
        "                COLOUR = YELLOW\n",
        "            elif 100 <= env.players[i].damage < 150:\n",
        "                COLOUR = ORANGE\n",
        "            elif 150 <= env.players[i].damage < 200:\n",
        "                COLOUR = RED\n",
        "            elif env.players[i].damage >= 200:\n",
        "                COLOUR = DARK_RED\n",
        "            percentage = env.players[i].damage * 5 / 7\n",
        "            text_surface = font.render(f'{percentage:.1f}%', True, COLOUR)\n",
        "            # text_rect_background = pygame.draw.rect(self.screen, (255,255,255), (220+i*100, 75, 70, 56))\n",
        "            # text_rect_background_border = pygame.draw.rect(self.screen, (0, 0, 0), (220+i*100, 75, 70, 56), 3)\n",
        "            text_rect = text_surface.get_rect(center=(self.score_width + i*(camera.window_width - 2 * self.score_width), self.score_height * 1.5/4))\n",
        "            camera.canvas.blit(text_surface, text_rect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygKmVZpfsB8c"
      },
      "source": [
        "### Camera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "CK5erYyisBKw"
      },
      "outputs": [],
      "source": [
        "class CameraResolution(Enum):\n",
        "    LOW = 1\n",
        "    MEDIUM = 2\n",
        "    HIGH = 3\n",
        "\n",
        "class RenderMode(Enum):\n",
        "    NONE = 0\n",
        "    RGB_ARRAY = 1\n",
        "    PYGAME_WINDOW = 2\n",
        "\n",
        "class Camera():\n",
        "    screen_width_tiles: float = 29.8\n",
        "    screen_height_tiles: float = 16.8\n",
        "    pixels_per_tile: float = 43\n",
        "    is_rendering: bool = False\n",
        "    space: pymunk.Space\n",
        "    pos: list[int] = [0,0]\n",
        "    zoom: float = 2.0\n",
        "\n",
        "\n",
        "    def reset(self, env):\n",
        "        self.space = env.space\n",
        "        self.objects = env.objects\n",
        "        self.resolution = env.resolution\n",
        "        self.resolutions = {\n",
        "            CameraResolution.LOW: (480, 720),\n",
        "            CameraResolution.MEDIUM: (720, 1280),\n",
        "            CameraResolution.HIGH: (1080, 1920)\n",
        "        }\n",
        "\n",
        "        self.window_height, self.window_width = self.resolutions[self.resolution]\n",
        "\n",
        "        # WIDTH HEIGHT in Pixels\n",
        "        #screen_width_tiles: float = 29.8\n",
        "        #screen_height_tiles: float = 16.8\n",
        "        self.pixels_per_tile = self.window_width // self.screen_width_tiles\n",
        "\n",
        "        #self.window_width = self.screen_width_tiles * self.pixels_per_tile\n",
        "        #self.window_height = self.screen_height_tiles * self.pixels_per_tile\n",
        "        self.steps = 0\n",
        "\n",
        "    def scale_gtp(self) -> float:\n",
        "        return self.pixels_per_tile * self.zoom\n",
        "\n",
        "    def _setup_render(self, mode) -> None:\n",
        "        pygame.init()\n",
        "\n",
        "        self.ui_handler = UIHandler(self)\n",
        "\n",
        "        self.key_panel_1 = KeyIconPanel(side=\"left\", edge_percentage=0.22, width_percentage=0.12, height_percentage=0.08)\n",
        "        self.key_panel_2 = KeyIconPanel(side=\"right\", edge_percentage=0.78, width_percentage=0.12, height_percentage=0.08)\n",
        "\n",
        "        if mode == RenderMode.PYGAME_WINDOW:\n",
        "            pygame.display.set_caption(\"Env\")\n",
        "            self.canvas = pygame.display.set_mode((self.window_width, self.window_height))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        # Define font\n",
        "        self.font50 = pygame.font.Font(None, 50)  # Use the default font with size 50\n",
        "        self.font = pygame.font.Font(None, 50)\n",
        "\n",
        "    def process(self) -> None:\n",
        "        self.steps += 1\n",
        "\n",
        "    def ptg(self, x, y=None) -> tuple[int, int]:\n",
        "        if isinstance(x, list) or isinstance(x, tuple) or isinstance(x, np.ndarray):\n",
        "            x, y = x\n",
        "        elif isinstance(x, pymunk.Vec2d):\n",
        "            x, y = x.x, x.y\n",
        "\n",
        "        scale_cst = self.scale_gtp()\n",
        "        new_x = -self.screen_width_tiles / 2 + int(x / scale_cst)\n",
        "        new_y = self.screen_height_tiles / 2 - int(y / scale_cst)\n",
        "        return new_x, new_y\n",
        "\n",
        "    def gtp(self, x, y=None) -> tuple[float, float]:\n",
        "        if isinstance(x, list) or isinstance(x, tuple) or isinstance(x, np.ndarray):\n",
        "            x, y = x\n",
        "        elif isinstance(x, pymunk.Vec2d):\n",
        "            x, y = x.x, x.y\n",
        "\n",
        "        scale_cst = self.scale_gtp()\n",
        "        new_x = self.window_width / 2 + (x - self.pos[0]) * scale_cst\n",
        "        new_y = self.window_height / 2 + (y -self.pos[1]) * scale_cst\n",
        "\n",
        "        #new_x = self.window_width / 2 + x * self.pixels_per_tile\n",
        "        #new_y = self.window_height / 2 + y * self.pixels_per_tile\n",
        "        return new_x, new_y\n",
        "\n",
        "    def get_frame(self, env, mode=RenderMode.RGB_ARRAY, has_hitboxes=False):\n",
        "        if not self.is_rendering:\n",
        "            self._setup_render(mode)\n",
        "            self.is_rendering = True\n",
        "\n",
        "\n",
        "        # Expose the canvas for editing\n",
        "        if mode == RenderMode.RGB_ARRAY:\n",
        "            self.canvas = pygame.Surface((self.window_width, self.window_height))\n",
        "        #canvas = pygame.display.set_mode((self.window_width, self.window_height))\n",
        "        self.canvas.fill((0, 0, 0))\n",
        "\n",
        "        # Transform PyMunk objects to have (0,0) at center, and such that units are appropriate\n",
        "        #center_x = self.window_width // 2\n",
        "        #center_y = self.window_height // 2\n",
        "        #scale = self.pixels_per_tile\n",
        "        #transform = pymunk.Transform.identity().translated(center_x, center_y).scaled(scale)\n",
        "\n",
        "        #center_x = self.screen_width_tiles // 2 - self.pos[0]\n",
        "        #center_y = self.screen_height_tiles // 2 - self.pos[1]\n",
        "        center_x = self.window_width // 2\n",
        "        center_y = self.window_height // 2\n",
        "        scale = self.pixels_per_tile * self.zoom\n",
        "        transform = pymunk.Transform.identity().translated(center_x, center_y).scaled(scale).translated(self.pos[0], self.pos[1])\n",
        "        #transform = pymunk.Transform.identity().scaled(scale).translated(center_x, center_y).scaled(self.zoom)\n",
        "        draw_options = DrawOptions(self.canvas)\n",
        "        draw_options.transform = transform\n",
        "\n",
        "        # Draw PyMunk objects\n",
        "        #self.space.debug_draw(draw_options)\n",
        "\n",
        "        #print(self.env.space)\n",
        "        for obj_name, obj in self.objects.items():\n",
        "            obj.render(self.canvas, self)\n",
        "\n",
        "        # Draw UI + Text\n",
        "        env.handle_ui(self.canvas)\n",
        "\n",
        "        self.ui_handler.render(self, env)\n",
        "\n",
        "        if hasattr(env, 'cur_action'):\n",
        "            self.key_panel_1.draw(self, env.cur_action[0])\n",
        "            self.key_panel_2.draw(self, env.cur_action[1])\n",
        "\n",
        "        img = np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "        if mode == RenderMode.PYGAME_WINDOW:\n",
        "            pygame.display.flip()\n",
        "            pygame.event.pump()\n",
        "            #pygame.display.update()\n",
        "            self.clock.tick(50)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def close(self) -> None:\n",
        "        pygame.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o43Fo3ssDPj"
      },
      "source": [
        "### Warehouse Brawl Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "Yl2-4QtPoDiW"
      },
      "outputs": [],
      "source": [
        "class Signal():\n",
        "    def __init__(self, env):\n",
        "        self._handlers: List[Callable] = []\n",
        "        self.env = env\n",
        "\n",
        "    def connect(self, handler: Callable):\n",
        "        self._handlers.append(handler)\n",
        "\n",
        "    def emit(self, *args, **kwargs):\n",
        "        for handler in self._handlers:\n",
        "            handler(self.env, *args, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "128rJJOGBtkn"
      },
      "outputs": [],
      "source": [
        "class Result(Enum):\n",
        "    WIN = \"win\"\n",
        "    LOSS = \"loss\"\n",
        "    DRAW = \"draw\"\n",
        "\n",
        "@dataclass\n",
        "class PlayerStats():\n",
        "    damage_taken: float\n",
        "    damage_done: float\n",
        "    lives_left: int\n",
        "\n",
        "@dataclass\n",
        "class MatchStats():\n",
        "    match_time: float  # Total match time in seconds\n",
        "    player1: PlayerStats\n",
        "    player2: PlayerStats\n",
        "    player1_result: Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "bFcO-JQsJE4n"
      },
      "outputs": [],
      "source": [
        "# Define an enumeration for the moves\n",
        "class MoveType(Enum):\n",
        "    NONE = auto()         # no move\n",
        "    NLIGHT = auto()       # grounded light neutral\n",
        "    DLIGHT = auto()       # grounded light down\n",
        "    SLIGHT = auto()       # grounded light side\n",
        "    NSIG = auto()         # grounded heavy neutral\n",
        "    DSIG = auto()         # grounded heavy down\n",
        "    SSIG = auto()         # grounded heavy side\n",
        "    NAIR = auto()         # aerial light neutral\n",
        "    DAIR = auto()         # aerial light down\n",
        "    SAIR = auto()         # aerial light side\n",
        "    RECOVERY = auto()     # aerial heavy neutral and aerial heavy side\n",
        "    GROUNDPOUND = auto()  # aerial heavy down\n",
        "\n",
        "    def __int__(self):\n",
        "        return self.value\n",
        "\n",
        "    def __float__(self):\n",
        "        return float(self.value)\n",
        "\n",
        "# Define a frozen dataclass for the key\n",
        "@dataclass(frozen=True)\n",
        "class CompactMoveState():\n",
        "    grounded: bool\n",
        "    heavy: bool\n",
        "    direction_type: int\n",
        "\n",
        "# Create the dictionary mapping CompactMoveState to a Move\n",
        "m_state_to_move = {\n",
        "    CompactMoveState(True, False, 0): MoveType.NLIGHT,      # grounded light neutral\n",
        "    CompactMoveState(True, False, 1): MoveType.DLIGHT,      # grounded light down\n",
        "    CompactMoveState(True, False, 2): MoveType.SLIGHT,      # grounded light side\n",
        "    CompactMoveState(True, True, 0): MoveType.NSIG,          # grounded heavy neutral\n",
        "    CompactMoveState(True, True, 1): MoveType.DSIG,          # grounded heavy down\n",
        "    CompactMoveState(True, True, 2): MoveType.SSIG,          # grounded heavy side\n",
        "    CompactMoveState(False, False, 0): MoveType.NAIR,        # aerial light neutral\n",
        "    CompactMoveState(False, False, 1): MoveType.DAIR,        # aerial light down\n",
        "    CompactMoveState(False, False, 2): MoveType.SAIR,        # aerial light side\n",
        "    CompactMoveState(False, True, 0): MoveType.RECOVERY,     # aerial heavy neutral\n",
        "    CompactMoveState(False, True, 1): MoveType.GROUNDPOUND,  # aerial heavy down\n",
        "    CompactMoveState(False, True, 2): MoveType.RECOVERY,     # aerial heavy side\n",
        "}\n",
        "\n",
        "class Facing(Enum):\n",
        "    RIGHT = 1\n",
        "    LEFT = -1\n",
        "\n",
        "    def __int__(self):\n",
        "        return self.value\n",
        "\n",
        "    @staticmethod\n",
        "    def flip(facing):\n",
        "        return Facing.LEFT if facing == Facing.RIGHT else Facing.RIGHT\n",
        "\n",
        "    @staticmethod\n",
        "    def from_direction(direction: float) -> \"Facing\":\n",
        "        return Facing.RIGHT if direction > 0 else Facing.LEFT\n",
        "\n",
        "    @staticmethod\n",
        "    def turn_check(facing, direction) -> bool:\n",
        "        if facing == Facing.RIGHT and direction < 0:\n",
        "            return True\n",
        "        if facing == Facing.LEFT and direction > 0:\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "JQ4OqVrSRP3m"
      },
      "outputs": [],
      "source": [
        "from pymunk.pygame_util import DrawOptions\n",
        "\n",
        "\"\"\"Coord system\n",
        "    +------ > x\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    |      . (3, 3)\n",
        "    v\n",
        "    y\n",
        "\"\"\"\n",
        "\n",
        "class WarehouseBrawl(MalachiteEnv[np.ndarray, np.ndarray, int]):\n",
        "\n",
        "    BRAWL_TO_UNITS = 1.024 / 320  # Conversion factor\n",
        "\n",
        "    def __init__(self, mode: RenderMode=RenderMode.RGB_ARRAY, resolution: CameraResolution=CameraResolution.LOW):\n",
        "        super(WarehouseBrawl, self).__init__()\n",
        "\n",
        "        self.stage_width_tiles: float = 29.8\n",
        "        self.stage_height_tiles: float = 16.8\n",
        "\n",
        "        self.mode = mode\n",
        "        self.resolution = resolution\n",
        "\n",
        "        self.agents = [0, 1] # Agent 0, agent 1\n",
        "        self.logger = ['', '']\n",
        "\n",
        "        # Params\n",
        "        self.fps = 30\n",
        "        self.dt = 1 / self.fps\n",
        "        self.max_timesteps = self.fps * 90\n",
        "\n",
        "        self.agent_1_name = 'Team 1'\n",
        "        self.agent_2_name = 'Team 2'\n",
        "\n",
        "        # Signals\n",
        "        self.knockout_signal = Signal(self)\n",
        "        self.win_signal = Signal(self)\n",
        "        self.hit_during_stun = Signal(self)\n",
        "\n",
        "        # Observation Space\n",
        "        self.observation_space = self.get_observation_space()\n",
        "\n",
        "        self.camera = Camera()\n",
        "\n",
        "        # Action Space\n",
        "        # WASD\n",
        "        self.action_space = self.get_action_space()\n",
        "        # spaces.Box(low=np.array([0] * 4), high=np.array([1] * 4), shape=(4,), dtype=np.float32)\n",
        "\n",
        "        self.action_spaces, self.observation_spaces = {}, {}\n",
        "        for agent_id in self.agents:\n",
        "            self.action_spaces[agent_id] = self.action_space\n",
        "            self.observation_spaces[agent_id] = self.observation_space\n",
        "\n",
        "        self.load_attacks()\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def get_observation_space(self):\n",
        "        # lowarray = np.array(\n",
        "        #     [0, -self.screen_width_tiles/2, -self.screen_width_tiles/2, 0, 0, 0, 0, 0] +\n",
        "        #     [0 for _ in range(len(Player.states))] +\n",
        "        #     [0] +\n",
        "        #     [(0, -self.screen_width_tiles, -self.screen_width_tiles, 0, 0)[i%5] for i in range(self.max_ammo*5)] +\n",
        "        #     [0, -self.screen_width_tiles/2, -self.screen_width_tiles/2, 0, -self.screen_width_tiles, -self.screen_width_tiles, -self.screen_width_tiles, -self.screen_width_tiles,\n",
        "        #     0, 0, 0, 0] +\n",
        "        #     [0 for _ in range(len(Player.states))] +\n",
        "        #     [(0, -self.screen_width_tiles, -self.screen_width_tiles, 0, 0)[i%5] for i in range(self.max_ammo*5)] +\n",
        "        #     [0]\n",
        "        # )\n",
        "        # higharray = np.array(\n",
        "        #     [1, self.screen_width_tiles/2, self.screen_width_tiles/2, self.screen_width_tiles/2, 2 * math.pi, 10, 20, 3] +\n",
        "        #     [1 for _ in range(len(Player.states))] +\n",
        "        #     [2*math.pi] +\n",
        "        #     [(1, self.screen_width_tiles, self.screen_width_tiles, 2*math.pi, 2*math.pi)[i%5] for i in range(self.max_ammo*5)] +\n",
        "        #     [1, self.screen_width_tiles/2, self.screen_width_tiles/2, self.screen_width_tiles/2, self.screen_width_tiles, self.screen_width_tiles, self.screen_width_tiles, self.screen_width_tiles,\n",
        "        #     2 * math.pi, 2 * math.pi, 20, 3] +\n",
        "        #     [1 for _ in range(len(Player.states))] +\n",
        "        #     [(1, self.screen_width_tiles, self.screen_width_tiles, 2*math.pi, 2*math.pi)[i%5] for i in range(self.max_ammo*5)] +\n",
        "        #     [self.time_limit]\n",
        "        # )\n",
        "\n",
        "        obs_helper = ObsHelper()\n",
        "        self.add_player_obs(obs_helper, 'player')\n",
        "        self.add_player_obs(obs_helper, 'opponent')\n",
        "\n",
        "        print('Obs space', obs_helper.low, obs_helper.high)\n",
        "\n",
        "        self.obs_helper = obs_helper\n",
        "\n",
        "        return self.obs_helper.get_as_box()\n",
        "\n",
        "    def add_player_obs(self, obs_helper, name: str='player') -> None:\n",
        "        obs_helper.add_section([-1, -1], [1, 1], f\"{name}_pos\")\n",
        "        obs_helper.add_section([-1, -1], [1, 1], f\"{name}_vel\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_facing\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_grounded\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_aerial\")\n",
        "        obs_helper.add_section([0], [2], f\"{name}_jumps_left\")\n",
        "        obs_helper.add_section([0], [12], f\"{name}_state\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_recoveries_left\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_dodge_timer\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_stun_frames\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_damage\")\n",
        "        obs_helper.add_section([0], [3], f\"{name}_stocks\")\n",
        "        obs_helper.add_section([0], [11], f\"{name}_move_type\")\n",
        "\n",
        "    def get_action_space(self):\n",
        "        act_helper = ActHelper()\n",
        "        act_helper.add_key(\"w\") # W (Aim up)\n",
        "        act_helper.add_key(\"a\") # A (Left)\n",
        "        act_helper.add_key(\"s\") # S (Aim down/fastfall)\n",
        "        act_helper.add_key(\"d\") # D (Right)\n",
        "        act_helper.add_key(\"space\") # Space (Jump)\n",
        "        act_helper.add_key(\"h\") # H (Pickup/Throw)\n",
        "        act_helper.add_key(\"l\") # L (Dash/Dodge)\n",
        "        act_helper.add_key(\"j\") # J (Light Attack)\n",
        "        act_helper.add_key(\"k\") # K (Heavy Attack)\n",
        "        act_helper.add_key(\"g\") # G (Taunt)\n",
        "\n",
        "        print('Action space', act_helper.low, act_helper.high)\n",
        "\n",
        "        self.act_helper = act_helper\n",
        "\n",
        "        return self.act_helper.get_as_box()\n",
        "\n",
        "    def square_floor_collision(arbiter, space, data):\n",
        "        \"\"\"\n",
        "        Collision handler callback that is called when a square collides with the platform.\n",
        "        It sets the square's collision flag so that is_on_floor() returns True.\n",
        "        \"\"\"\n",
        "        shape_a, shape_b = arbiter.shapes\n",
        "        # Check both shapes; one of them should be a square.\n",
        "        if hasattr(shape_a, \"owner\") and isinstance(shape_a.owner, Player):\n",
        "            shape_a.owner.collided_this_step = True\n",
        "        if hasattr(shape_b, \"owner\") and isinstance(shape_b.owner, Player):\n",
        "            shape_b.owner.collided_this_step = True\n",
        "        return True\n",
        "\n",
        "    def get_stats(self, agent_id: int) -> PlayerStats:\n",
        "        player = self.players[agent_id]\n",
        "        return PlayerStats(\n",
        "            damage_taken=player.damage_taken_total,\n",
        "            damage_done=player.damage_done,\n",
        "            lives_left=player.stocks)\n",
        "\n",
        "    def load_attacks(self):\n",
        "        # load all from /content/attacks\n",
        "        self.attacks = {}\n",
        "\n",
        "        self.keys = {\n",
        "            'Unarmed NLight': MoveType.NLIGHT,\n",
        "            'Unarmed DLight': MoveType.DLIGHT,\n",
        "            'Unarmed SLight': MoveType.SLIGHT,\n",
        "            'Unarmed NSig':   MoveType.NSIG,\n",
        "            'Unarmed DSig':   MoveType.DSIG,\n",
        "            'Unarmed SSig':   MoveType.SSIG,\n",
        "            'Unarmed NAir':   MoveType.NAIR,\n",
        "            'Unarmed DAir':   MoveType.DAIR,\n",
        "            'Unarmed SAir':   MoveType.SAIR,\n",
        "            'Unarmed Recovery': MoveType.RECOVERY,\n",
        "            'Unarmed Groundpound': MoveType.GROUNDPOUND,\n",
        "        }\n",
        "\n",
        "        for file in sorted(os.listdir('attacks')):\n",
        "            name = file.split('.')[0]\n",
        "            if name not in self.keys.keys(): continue\n",
        "            with open(os.path.join('attacks', file)) as f:\n",
        "                move_data = json.load(f)\n",
        "\n",
        "            self.attacks[self.keys[name]] = move_data\n",
        "\n",
        "\n",
        "    def step(self, action: dict[int, np.ndarray]):\n",
        "        # Create new rewards dict\n",
        "        self.cur_action = action\n",
        "        self.rewards = {agent: 0 for agent in self.agents}\n",
        "        self.terminated = False\n",
        "        self.logger = ['', '']\n",
        "\n",
        "        self.camera.process()\n",
        "\n",
        "        # Process all other steps\n",
        "        for obj_name, obj in self.objects.items():\n",
        "            # If player\n",
        "            if isinstance(obj, Player):\n",
        "                continue\n",
        "            else:\n",
        "                obj.process()\n",
        "\n",
        "        # Process player step\n",
        "        for agent in self.agents:\n",
        "            player = self.players[agent]\n",
        "            player.process(action[agent])\n",
        "            if player.stocks <= 0:\n",
        "                self.terminated = True\n",
        "                self.win_signal.emit(agent='player' if agent == 1 else 'opponent')\n",
        "\n",
        "\n",
        "        # Process physics info\n",
        "        for obj_name, obj in self.objects.items():\n",
        "            obj.physics_process(self.dt)\n",
        "\n",
        "        # PyMunk step\n",
        "        self.space.step(self.dt)\n",
        "        self.steps += 1\n",
        "\n",
        "        truncated = self.steps >= self.max_timesteps\n",
        "\n",
        "        # Collect observations\n",
        "        observations = {agent: self.observe(agent) for agent in self.agents}\n",
        "\n",
        "        return observations, self.rewards, self.terminated, truncated, {}\n",
        "\n",
        "    def add_reward(self, agent: int, reward: float) -> None:\n",
        "        # Not really in use\n",
        "        self.rewards[agent] += reward\n",
        "\n",
        "    def reset(self, seed=None) -> Tuple[dict[int, np.ndarray], dict[str, Any]]:\n",
        "        self.seed = seed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.space = pymunk.Space()\n",
        "        self.dt = 1 / 30.0\n",
        "        self.space.gravity = 0, 17.808\n",
        "\n",
        "        self.steps = 0\n",
        "\n",
        "        # Other params\n",
        "        self.rewards = {agent: 0 for agent in self.agents}\n",
        "\n",
        "        # Game Objects\n",
        "        self.objects: dict[str, GameObject] = {}\n",
        "\n",
        "        self.players: list[Player] = []\n",
        "        self.camera.reset(self)\n",
        "        self._setup()\n",
        "\n",
        "        return {agent: self.observe(agent) for agent in self.agents}, {}\n",
        "\n",
        "    def observe(self, agent: int) -> np.ndarray:\n",
        "        #  lh = LowHigh()\n",
        "        # lh += [-1, -1], [1, 1] # 2d vector to goal\n",
        "        # lh += [-1, -1], [1, 1] # 2d vector of global position\n",
        "        # lh += [-1, -1], [1, 1] # 2d vector of global velocity\n",
        "\n",
        "        obs = []\n",
        "        obs += self.players[agent].get_obs()\n",
        "        obs += self.players[1-agent].get_obs()\n",
        "        #obs += self.players[agent].body.position.x, self.players[agent].body.position.y\n",
        "        #obs += self.players[agent].body.position.x, self.players[agent].body.position.y\n",
        "        #obs += self.players[agent].body.velocity.x, self.players[agent].body.velocity.y\n",
        "\n",
        "        return np.array(obs)\n",
        "\n",
        "    def render(self) -> None | np.ndarray | str | list:\n",
        "        return self.camera.get_frame(self)\n",
        "\n",
        "    def handle_ui(self, canvas: pygame.Surface) -> None:\n",
        "        # Define UI\n",
        "        # player_stat = f\"P1: {self.players[0].stocks}, P2: {self.players[1].stocks}\"\n",
        "        # text_surface = self.camera.font.render(player_stat, True, (255, 255, 255))  # White text\n",
        "        # text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 50))  # Center the text\n",
        "        # canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # # Damage\n",
        "        # small_font = pygame.font.Font(None, 20)\n",
        "        # text_surface = small_font.render(f\"{self.players[0].damage}%, {self.players[1].damage}%\", True, (255, 255, 255))  # White text\n",
        "        # text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 70))  # Center the text\n",
        "        # canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # Smaller text\n",
        "        small_font = pygame.font.Font(None, 30)\n",
        "        text_surface = small_font.render(f\"Time: {self.steps}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 30))  # Center the text\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # Smaller text\n",
        "        small_font = pygame.font.Font(None, 20)\n",
        "        text_surface = small_font.render(f\"P1: {self.logger[0]['transition']}, P2: {self.logger[1]['transition']}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 50))  # Center the text\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # Smaller text\n",
        "        small_font = pygame.font.Font(None, 20)\n",
        "        text_surface = small_font.render(f\"P1: {self.logger[0].get('move_type', '')}, P2: {self.logger[1].get('move_type', '')}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 70))  # Center the text\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # Smaller text\n",
        "        text_surface = small_font.render(f\"P1 Total Reward: {self.logger[0].get('total_reward', '')}, Reward {self.logger[0].get('reward', '')}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(0, self.camera.window_height - 40))  # Center the text\n",
        "        # make it left\n",
        "        text_rect.left = 0\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        text_surface = small_font.render(f\"P2 Total Reward: {self.logger[1].get('total_reward', '')}, Reward {self.logger[1].get('reward', '')}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(0, self.camera.window_height - 20))  # Center the text\n",
        "        text_rect.left = 0\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "\n",
        "\n",
        "    def observation_space(self, agent: AgentID) -> gymnasium.spaces.Space:\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def action_space(self, agent: AgentID) -> gymnasium.spaces.Space:\n",
        "        return self.action_spaces[agent]\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.camera.close()\n",
        "\n",
        "    def _setup(self):\n",
        "        # Collsion fix\n",
        "        handler = self.space.add_collision_handler(3, 4)  # (Player1 collision_type, Player2 collision_type)\n",
        "        handler.begin = lambda *args, **kwargs: False\n",
        "\n",
        "        # Environment\n",
        "        ground = Ground(self.space, 0, 2.03, 10.67)\n",
        "        #platform1 = Ground(self.space, 5, 0, 2)  # First platform\n",
        "        #platform2 = Ground(self.space, 2, 3, 2)  # Second platform with a gap\n",
        "\n",
        "        self.objects['ground'] = ground\n",
        "        #self.objects['platform1'] = platform1\n",
        "        #self.objects['platform2'] = platform2\n",
        "\n",
        "        # Particle\n",
        "        #particle = Particle(self, [0, 0], 'test/unarmedgp.gif', scale=0.2)\n",
        "        #self.objects['particle'] = particle\n",
        "\n",
        "        # Target\n",
        "        #target = Target()\n",
        "        #self.objects['target'] = target\n",
        "\n",
        "        # Players\n",
        "        # randomize start pos, binary\n",
        "        p1_right = bool(random.getrandbits(1))\n",
        "        p1_start_pos = [5, 0] if p1_right else [-5, 0]\n",
        "        p2_start_pos = [-5, 0] if p1_right else [5, 0]\n",
        "\n",
        "        p1 = Player(self, 0, start_position=p1_start_pos, color=[0, 0, 255, 255])\n",
        "        p2 = Player(self, 1, start_position=p2_start_pos, color=[0, 255, 0, 255])\n",
        "\n",
        "        self.objects['player'] = p1\n",
        "        self.objects['opponent'] = p2\n",
        "\n",
        "        self.players += [p1, p2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZwI-S1X3P9L"
      },
      "source": [
        "### GameObject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "3osNsDxv3PrR"
      },
      "outputs": [],
      "source": [
        "class GameObject(ABC):\n",
        "\n",
        "    def render(self, canvas: pygame.Surface, camera: Camera) -> None:\n",
        "        pass\n",
        "\n",
        "    def process(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def physics_process(self, dt: float) -> None:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_image(canvas, img, pos, desired_width, camera, flipped: bool = False):\n",
        "        \"\"\"\n",
        "        Draws an image onto the canvas while correctly handling scaling and positioning.\n",
        "\n",
        "        Parameters:\n",
        "            canvas (pygame.Surface): The surface to draw onto.\n",
        "            img (pygame.Surface): The image to draw.\n",
        "            pos (tuple): The (x, y) position in game coordinates (center of the desired drawing).\n",
        "            desired_width (float): The width in game units.\n",
        "            camera (Camera): The camera object, which has a gtp() method for coordinate conversion.\n",
        "        \"\"\"\n",
        "        # Convert game coordinates to screen coordinates\n",
        "        screen_pos = camera.gtp(pos)\n",
        "\n",
        "        # Compute the new width in screen units\n",
        "        screen_width = int(desired_width * camera.scale_gtp())\n",
        "\n",
        "        # Maintain aspect ratio when scaling\n",
        "        aspect_ratio = img.get_height() / img.get_width()\n",
        "        screen_height = int(screen_width * aspect_ratio)\n",
        "\n",
        "        # Scale the image to the new size\n",
        "        scaled_img = pygame.transform.scale(img, (screen_width, screen_height))\n",
        "\n",
        "        if flipped:\n",
        "            scaled_img = pygame.transform.flip(scaled_img, True, False)\n",
        "\n",
        "        # Compute the top-left corner for blitting (since screen_pos is the center)\n",
        "        top_left = (screen_pos[0] - screen_width // 2, screen_pos[1] - screen_height // 2)\n",
        "\n",
        "        # Blit the scaled image onto the canvas\n",
        "        canvas.blit(scaled_img, top_left)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECELO0ke3LGO"
      },
      "source": [
        "### Other GameObjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "iLHQtBKr3Kq_"
      },
      "outputs": [],
      "source": [
        "class Ground(GameObject):\n",
        "    def __init__(self, space, x, y, width_ground, color=(150, 150, 150, 255)):\n",
        "        self.body = pymunk.Body(x, y, body_type=pymunk.Body.STATIC)\n",
        "        self.shape = pymunk.Poly.create_box(self.body, (width_ground, 0.1))\n",
        "        self.shape.collision_type = 2 # Ground\n",
        "        self.shape.owner = self\n",
        "        self.shape.body.position = (x, y)\n",
        "        self.shape.friction = 0.7\n",
        "        self.shape.color = color\n",
        "\n",
        "        self.width_ground = width_ground\n",
        "\n",
        "        space.add(self.shape, self.body)\n",
        "        self.loaded = False\n",
        "\n",
        "    def load_assets(self):\n",
        "        if self.loaded: return\n",
        "        self.loaded = True\n",
        "        self.bg_img = pygame.image.load('assets/map/bg.jpg')\n",
        "        self.stage_img = pygame.image.load('assets/map/stage.png')\n",
        "\n",
        "    def render(self, canvas, camera) -> None:\n",
        "        self.load_assets()\n",
        "\n",
        "        #self.draw_image(canvas, self.bg_img, (0, 0), 29.8, camera)\n",
        "        self.draw_image(canvas, self.stage_img, (0, 0.8), self.width_ground * 3.2, camera)\n",
        "\n",
        "class Stage(GameObject):\n",
        "    def __init__(self, space, x, y, width, height, color=(150, 150, 150, 255)):\n",
        "        self.body = pymunk.Body(x, y, body_type=pymunk.Body.STATIC)\n",
        "        self.shape = pymunk.Poly.create_box(self.body, (width, height))\n",
        "        self.shape.body.position = (x + width // 2, y)\n",
        "        self.shape.friction = 0.7\n",
        "        self.shape.color = color\n",
        "        space.add(self.shape, self.body)\n",
        "\n",
        "    def render(self, canvas, camera) -> None:\n",
        "        pass\n",
        "\n",
        "class Target(GameObject):\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    def render(self, canvas, camera) -> None:\n",
        "        pygame.draw.circle(canvas, (255,0,0), camera.gtp([5,0]), camera.scale_gtp() * 0.25)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpdrreEnsMVR"
      },
      "source": [
        "### Player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jww67QlGd4GL"
      },
      "source": [
        "#### PlayerInputInfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "WYd3nArud4PC"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class KeyStatus():\n",
        "    just_pressed: bool = False\n",
        "    held: bool = False\n",
        "    just_released: bool = False\n",
        "\n",
        "class PlayerInputHandler():\n",
        "    def __init__(self):\n",
        "        # Define the key order corresponding to the action vector:\n",
        "        # Index 0: W, 1: A, 2: S, 3: D, 4: space\n",
        "        self.key_names = [\"W\", \"A\", \"S\", \"D\", \"space\", 'h', 'l', 'j', 'k', 'g']\n",
        "        # Previous frame key state (all start as not pressed).\n",
        "        self.prev_state = {key: False for key in self.key_names}\n",
        "        # The current status for each key.\n",
        "        self.key_status = {key: KeyStatus() for key in self.key_names}\n",
        "        # Raw axes computed from key states.\n",
        "        self.raw_vertical = 0.0   # +1 if W is held, -1 if S is held.\n",
        "        self.raw_horizontal = 0.0 # +1 if D is held, -1 if A is held.\n",
        "\n",
        "    def update(self, action: np.ndarray):\n",
        "        \"\"\"\n",
        "        Given an action vector (floats representing 0 or 1),\n",
        "        update the internal state for each key, including:\n",
        "          - whether it was just pressed\n",
        "          - whether it is held\n",
        "          - whether it was just released\n",
        "        Also computes the raw input axes for WS and AD.\n",
        "\n",
        "        Parameters:\n",
        "            action (np.ndarray): 5-element vector representing the current key states.\n",
        "        \"\"\"\n",
        "\n",
        "        # Update each key's status.\n",
        "        for i, key in enumerate(self.key_names):\n",
        "            # Treat a value > 0.5 as pressed.\n",
        "            current = action[i] > 0.5\n",
        "            previous = self.prev_state[key]\n",
        "            self.key_status[key].just_pressed = (not previous and current)\n",
        "            self.key_status[key].just_released = (previous and not current)\n",
        "            self.key_status[key].held = current\n",
        "            # Save the current state for the next update.\n",
        "            self.prev_state[key] = current\n",
        "\n",
        "        # Compute the raw axes:\n",
        "        # Vertical axis: W (+1) and S (-1)\n",
        "        self.raw_vertical = (1.0 if self.key_status[\"W\"].held else 0.0) + (-1.0 if self.key_status[\"S\"].held else 0.0)\n",
        "        # Horizontal axis: D (+1) and A (-1)\n",
        "        self.raw_horizontal = (1.0 if self.key_status[\"D\"].held else 0.0) + (-1.0 if self.key_status[\"A\"].held else 0.0)\n",
        "\n",
        "    def __repr__(self):\n",
        "        # For debugging: provide a summary of the key statuses and axes.\n",
        "        statuses = \", \".join(\n",
        "            f\"{key}: (just_pressed={self.key_status[key].just_pressed}, held={self.key_status[key].held}, just_released={self.key_status[key].just_released})\"\n",
        "            for key in self.key_names\n",
        "        )\n",
        "        return (f\"PlayerInputHandler({statuses}, \"\n",
        "                f\"raw_horizontal={self.raw_horizontal}, raw_vertical={self.raw_vertical})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHx_GIg5a8x0"
      },
      "source": [
        "#### PlayerObjectState Abstract Base Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "id": "AtO2OKiBa8bS"
      },
      "outputs": [],
      "source": [
        "class PlayerObjectState(ABC):\n",
        "    def __init__(self, player: \"Player\"):\n",
        "        self.p: \"Player\" = player\n",
        "        self.invincible_timer = 0\n",
        "        self.dodge_cooldown = 0\n",
        "        self.stun_time_stored = 0\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def stunned(self, stun_time: int=0):\n",
        "        self.stun_time_stored = stun_time\n",
        "\n",
        "    def vulnerable(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def is_grounded(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    def is_aerial(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    def physics_process(self, dt: float) -> \"PlayerObjectState\":\n",
        "        # Killbox\n",
        "        sides = abs(self.p.body.position.x) > self.p.env.stage_width_tiles // 2\n",
        "        tops = abs(self.p.body.position.y) > self.p.env.stage_height_tiles // 2\n",
        "        if sides or tops:\n",
        "            return self.p.states['KO']\n",
        "\n",
        "        #self != self.p.states['stun'] and\n",
        "        if self.stun_time_stored > 0:\n",
        "            if self == self.p.states['stun']:\n",
        "                self.p.env.hit_during_stun.emit(agent='player' if self.p.agent_id == 0 else 'opponent')\n",
        "            stun_state = self.p.states['stun']\n",
        "            stun_state.set_stun(self.stun_time_stored)\n",
        "            self.stun_time_stored = 0\n",
        "            if hasattr(self, 'jumps_left'):\n",
        "                stun_state.jumps_left = self.jumps_left\n",
        "            return stun_state\n",
        "\n",
        "        # Tick timers\n",
        "        self.invincible_timer = max(0, self.invincible_timer-1)\n",
        "        self.dodge_cooldown = max(0, self.dodge_cooldown-1)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def exit(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "\n",
        "    def reset(self, old) -> \"PlayerObjectState\":\n",
        "        self.p = old.p\n",
        "        self.stun_time_stored = 0\n",
        "        self.invincible_timer = old.invincible_timer\n",
        "        self.dodge_cooldown = old.dodge_cooldown\n",
        "\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNweZj-Wc_zc"
      },
      "source": [
        "#### Basic States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "tTAYZyJ2dCIu"
      },
      "outputs": [],
      "source": [
        "class GroundState(PlayerObjectState):\n",
        "    def can_control(self):\n",
        "        return True\n",
        "\n",
        "    def is_grounded(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def reset(self, old) -> None:\n",
        "        if hasattr(old, 'dash_timer'):\n",
        "            self.dash_timer = old.dash_timer\n",
        "        else:\n",
        "            self.dash_timer = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_ground_state(p: \"Player\") -> PlayerObjectState:\n",
        "        if abs(p.input.raw_horizontal) > 1e-2:\n",
        "            return p.states['walking']\n",
        "        else:\n",
        "            return p.states['standing']\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        if not self.can_control(): return None\n",
        "\n",
        "        # Handle jump\n",
        "        direction = self.p.input.raw_horizontal\n",
        "        near_still = abs(direction) < 1e-2\n",
        "        if self.p.input.key_status[\"space\"].just_pressed and self.p.is_on_floor():\n",
        "            self.p.body.velocity = pymunk.Vec2d(self.p.body.velocity.x, -self.p.jump_speed)\n",
        "            self.p.facing = Facing.from_direction(direction)\n",
        "            in_air = self.p.states['in_air']\n",
        "            in_air.refresh()\n",
        "            return in_air\n",
        "\n",
        "        if not self.p.is_on_floor():\n",
        "            in_air = self.p.states['in_air']\n",
        "            in_air.refresh()\n",
        "            return in_air\n",
        "\n",
        "        # Handle dodge\n",
        "        if near_still and self.p.input.key_status['l'].just_pressed and self.dodge_cooldown <= 0:\n",
        "            self.dodge_cooldown = self.p.grounded_dodge_cooldown\n",
        "            dodge_state = self.p.states['dodge']\n",
        "            dodge_state.set_is_grounded(True)\n",
        "            return dodge_state\n",
        "\n",
        "        # Check for attack\n",
        "        move_type = self.p.get_move()\n",
        "        if move_type != MoveType.NONE:\n",
        "            attack_state = self.p.states['attack']\n",
        "            attack_state.give_move(move_type)\n",
        "            return attack_state\n",
        "\n",
        "        # Check for taunt\n",
        "        if self.p.input.key_status['g'].just_pressed:\n",
        "            taunt_state = self.p.states['taunt']\n",
        "            return taunt_state\n",
        "\n",
        "\n",
        "        return None\n",
        "\n",
        "class InAirState(PlayerObjectState):\n",
        "    def can_control(self):\n",
        "        return True\n",
        "\n",
        "    def is_aerial(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def refresh(self):\n",
        "        self.jump_timer = 0\n",
        "        self.jumps_left = 2\n",
        "        self.recoveries_left = 1\n",
        "\n",
        "    def set_jumps(self, jump_timer, jumps_left, recoveries_left):\n",
        "        self.jump_timer = jump_timer\n",
        "        self.jumps_left = jumps_left\n",
        "        self.recoveries_left = recoveries_left\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.is_base = True\n",
        "\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        if not self.can_control(): return None\n",
        "\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "        if self.is_base and Facing.turn_check(self.p.facing, direction):\n",
        "            air_turn = self.p.states['air_turnaround']\n",
        "            air_turn.send(self.jump_timer, self.jumps_left, self.recoveries_left)\n",
        "            return air_turn\n",
        "\n",
        "        vel_x = self.p.move_toward(self.p.body.velocity.x, direction * self.p.move_speed, self.p.in_air_ease)\n",
        "        #print(self.p.body.velocity.x, vel_x)\n",
        "        self.p.body.velocity = pymunk.Vec2d(vel_x, self.p.body.velocity.y)\n",
        "\n",
        "        #print(self.p.is_on_floor(), self.p.body.position)\n",
        "        if self.p.is_on_floor():\n",
        "            return GroundState.get_ground_state(self.p)\n",
        "\n",
        "        # Handle Jump\n",
        "        if self.p.input.key_status[\"space\"].just_pressed and self.can_jump():\n",
        "            self.p.body.velocity = pymunk.Vec2d(self.p.body.velocity.x, -self.p.jump_speed)\n",
        "            self.p.facing = Facing.from_direction(direction)\n",
        "            self.jump_timer = self.p.jump_cooldown\n",
        "            self.jumps_left -= 1\n",
        "\n",
        "        # Handle dodge\n",
        "        if self.p.input.key_status['l'].just_pressed and self.dodge_cooldown <= 0:\n",
        "            self.dodge_cooldown = self.p.air_dodge_cooldown\n",
        "            dodge_state = self.p.states['dodge']\n",
        "            dodge_state.jump_timer = self.jump_timer\n",
        "            dodge_state.jumps_left = self.jumps_left\n",
        "            dodge_state.recoveries_left = self.recoveries_left\n",
        "            dodge_state.set_is_grounded(False)\n",
        "            return dodge_state\n",
        "\n",
        "        # Check for attack\n",
        "        move_type = self.p.get_move()\n",
        "        if move_type != MoveType.NONE:\n",
        "            if move_type == MoveType.RECOVERY:\n",
        "                if self.recoveries_left > 0:\n",
        "                    self.recoveries_left -= 1\n",
        "                    attack_state = self.p.states['attack']\n",
        "                    attack_state.give_move(move_type)\n",
        "                    return attack_state\n",
        "            else:\n",
        "                attack_state = self.p.states['attack']\n",
        "                attack_state.give_move(move_type)\n",
        "                return attack_state\n",
        "\n",
        "        return None\n",
        "\n",
        "    def can_jump(self) -> bool:\n",
        "        return self.jump_timer <= 0 and self.jumps_left > 0\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        if self.p.body.velocity.y < 0:\n",
        "            self.p.animation_sprite_2d.play('alup')\n",
        "        else:\n",
        "            self.p.animation_sprite_2d.play('aldown')\n",
        "\n",
        "class TauntState(InAirState):\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.taunt_timer = self.p.taunt_time\n",
        "        self.seed = random.randint(0, 2)\n",
        "\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        self.taunt_timer = max(0, self.taunt_timer-1)\n",
        "        if self.taunt_timer <= 0:\n",
        "            if self.is_grounded:\n",
        "                return GroundState.get_ground_state(self.p)\n",
        "            else:\n",
        "                in_air = self.p.states['in_air']\n",
        "                if hasattr(self, 'jumps_left'):\n",
        "                    in_air.jumps_left = self.jumps_left\n",
        "                    in_air.jump_timer = 0\n",
        "                    in_air.recoveries_left = self.recoveries_left\n",
        "                return in_air\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        taunts = ['altroll', 'alhappy', 'alkai']\n",
        "        self.p.animation_sprite_2d.play(taunts[self.seed % 3])\n",
        "\n",
        "class WalkingState(GroundState):\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        # Leave walking if not moving\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "\n",
        "        # Check if turning\n",
        "        if Facing.turn_check(self.p.facing, direction):\n",
        "            if self.p.input.key_status[\"l\"].just_pressed:\n",
        "                return self.p.states['backdash']\n",
        "\n",
        "            return self.p.states['turnaround']\n",
        "        if abs(direction) < 1e-2:\n",
        "            return self.p.states['standing']\n",
        "\n",
        "        # Check for dash\n",
        "        if self.p.input.key_status[\"l\"].just_pressed:\n",
        "            return self.p.states['dash']\n",
        "\n",
        "        # Handle movement\n",
        "        self.p.body.velocity = pymunk.Vec2d(direction * self.p.move_speed, self.p.body.velocity.y)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('walk')\n",
        "\n",
        "class SprintingState(GroundState):\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        # Leave walking if not moving\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "        # Check if turning\n",
        "        if Facing.turn_check(self.p.facing, direction):\n",
        "            if self.p.input.key_status[\"l\"].just_pressed:\n",
        "                return self.p.states['backdash']\n",
        "            return self.p.states['turnaround']\n",
        "        if abs(direction) < 1e-2:\n",
        "            return self.p.states['standing']\n",
        "\n",
        "         # Check for dash\n",
        "        if self.p.input.key_status[\"l\"].just_pressed:\n",
        "            return self.p.states['dash']\n",
        "\n",
        "        # Handle movement\n",
        "        self.p.body.velocity = pymunk.Vec2d(direction * self.p.run_speed, self.p.body.velocity.y)\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('run')\n",
        "\n",
        "class StandingState(GroundState):\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        # Leave standing if starting to move\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "        if Facing.turn_check(self.p.facing, direction):\n",
        "            if self.p.input.key_status[\"l\"].just_pressed:\n",
        "                return self.p.states['backdash']\n",
        "            return self.p.states['turnaround']\n",
        "        if abs(direction) > 1e-2:\n",
        "            self.p.facing = Facing.from_direction(direction)\n",
        "            return self.p.states['walking']\n",
        "\n",
        "\n",
        "        # gradual ease\n",
        "        vel_x = self.p.move_toward(self.p.body.velocity.x, 0, self.p.move_speed)\n",
        "        self.p.body.velocity = pymunk.Vec2d(vel_x, self.p.body.velocity.y)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('idle')\n",
        "\n",
        "class TurnaroundState(GroundState):\n",
        "    def enter(self) -> None:\n",
        "        self.turnaround_timer = self.p.turnaround_time\n",
        "\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        if self.turnaround_timer <= 0:\n",
        "            # After the turnaround period, update the facing direction.\n",
        "            self.p.facing = Facing.flip(self.p.facing)\n",
        "            return GroundState.get_ground_state(self.p)\n",
        "\n",
        "\n",
        "        # Allow breaking out of turnaround by jumping.\n",
        "        if self.p.input.key_status[\"space\"].just_pressed and self.p.is_on_floor():\n",
        "            self.p.body.velocity = pymunk.Vec2d(self.p.body.velocity.x, -self.p.jump_speed)\n",
        "            return self.p.states['in_air']\n",
        "\n",
        "        if self.p.input.key_status[\"l\"].just_pressed:\n",
        "                return self.p.states['backdash']\n",
        "\n",
        "\n",
        "        self.turnaround_timer = max(0, self.turnaround_timer-1)\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('turn')\n",
        "\n",
        "class AirTurnaroundState(InAirState):\n",
        "\n",
        "    def send(self, jump_timer, jumps_left, recoveries_left):\n",
        "        self.jump_timer = jump_timer\n",
        "        self.jumps_left = jumps_left\n",
        "        self.recoveries_left = recoveries_left\n",
        "\n",
        "    def is_base(self):\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.turnaround_timer = self.p.turnaround_time\n",
        "        self.is_base = False\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        if self.turnaround_timer <= 0:\n",
        "            # After the turnaround period, update the facing direction.\n",
        "            self.p.facing = Facing.flip(self.p.facing)\n",
        "            in_air = self.p.states['in_air']\n",
        "            in_air.set_jumps(self.jump_timer, self.jumps_left, self.recoveries_left)\n",
        "            return in_air\n",
        "\n",
        "\n",
        "        self.turnaround_timer = max(0, self.turnaround_timer-1)\n",
        "        return None\n",
        "\n",
        "    def can_jump(self) -> bool:\n",
        "        return self.jump_timer <= 0 and self.jumps_left > 0\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('turn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwnWPBG_nQeg"
      },
      "source": [
        "#### Hurt States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "cug9v9L1nKaP"
      },
      "outputs": [],
      "source": [
        "class StunState(InAirState):\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def set_stun(self, stun_frames):\n",
        "        self.stun_frames = stun_frames\n",
        "        #print('stun', self.stun_frames)\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "\n",
        "        self.stun_frames = max(0, self.stun_frames-1)\n",
        "\n",
        "        vel_x = self.p.move_toward(self.p.body.velocity.x, 0, self.p.in_air_ease / 1.5)\n",
        "        #print(self.p.body.velocity.x, vel_x)\n",
        "        self.p.body.velocity = pymunk.Vec2d(vel_x, self.p.body.velocity.y)\n",
        "\n",
        "        if self.stun_frames > 0: return None\n",
        "\n",
        "        if self.p.is_on_floor():\n",
        "            return GroundState.get_ground_state(self.p)\n",
        "        else:\n",
        "            in_air = self.p.states['in_air']\n",
        "            if hasattr(self, 'jumps_left'):\n",
        "                in_air.jumps_left = max(1, self.jumps_left)\n",
        "            else:\n",
        "                in_air.jumps_left = 1\n",
        "            return in_air\n",
        "\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('hurt_up')\n",
        "\n",
        "class KOState(GroundState):\n",
        "\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.p.env.knockout_signal.emit(agent='player' if self.p.agent_id == 0 else 'opponent')\n",
        "        self.timer = 30 * 3\n",
        "        self.p.stocks -= 1\n",
        "        self.p.body.velocity_func = DodgeState.no_gravity_velocity_func\n",
        "        self.p.body.velocity = pymunk.Vec2d(0, 0)\n",
        "\n",
        "    def exit(self) -> None:\n",
        "        self.invincible_timer = self.p.invincible_time\n",
        "        self.p.body.body_type = pymunk.Body.DYNAMIC\n",
        "        self.p.body.velocity_func = pymunk.Body.update_velocity\n",
        "        self.p.body.velocity = pymunk.Vec2d(0, 0)\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "\n",
        "        self.timer -= 1\n",
        "\n",
        "        if self.timer <= 0:\n",
        "            self.p.respawn()\n",
        "            in_air = self.p.states['in_air']\n",
        "            in_air.jumps_left = 0\n",
        "            in_air.recoveries_left = 0\n",
        "            return in_air\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('hurt_up')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7lJmFQncGw7"
      },
      "source": [
        "#### Dash States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "0bp2tc-ncHzm"
      },
      "outputs": [],
      "source": [
        "class DashState(GroundState):\n",
        "    def enter(self) -> None:\n",
        "        self.dash_timer = self.p.dash_time\n",
        "        # Optionally, play a dash sound or animation here.\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "\n",
        "        # Apply a strong forward velocity in the facing direction.\n",
        "        self.p.body.velocity = pymunk.Vec2d(int(self.p.facing) * self.p.dash_speed, self.p.body.velocity.y)\n",
        "        self.dash_timer = max(0, self.dash_timer-1)\n",
        "        if self.dash_timer <= 0:\n",
        "            return self.p.states['sprinting']\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('run')\n",
        "\n",
        "\n",
        "class BackDashState(GroundState):\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.backdash_timer = self.p.backdash_time\n",
        "        # Backdash is usually slower than a forward dash.\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        # Apply velocity opposite to the facing direction.\n",
        "        # Note: Backdash does not change facing_direction.\n",
        "        self.p.body.velocity = pymunk.Vec2d(-int(self.p.facing) * self.p.backdash_speed, self.p.body.velocity.y)\n",
        "        self.backdash_timer = max(0, self.backdash_timer-1)\n",
        "        if self.backdash_timer <= 0:\n",
        "            return GroundState.get_ground_state(self.p)\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('backdash')\n",
        "\n",
        "class DodgeState(InAirState):\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def no_gravity_velocity_func(body, gravity, damping, dt):\n",
        "        # Call the default velocity updater with gravity set to zero.\n",
        "        pymunk.Body.update_velocity(body, pymunk.Vec2d(0, 0), damping, dt)\n",
        "\n",
        "    def set_is_grounded(self, is_grounded: bool) -> None:\n",
        "        self.is_grounded = is_grounded\n",
        "\n",
        "    def is_aerial(self) -> bool:\n",
        "        return not self.is_grounded\n",
        "\n",
        "    def is_grounded(self) -> bool:\n",
        "        return self.is_grounded\n",
        "\n",
        "    def vulnerable(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.dodge_timer = self.p.dodge_time\n",
        "        # disable player gravity\n",
        "        # Override the body's velocity function to ignore gravity.\n",
        "        self.p.body.velocity_func = DodgeState.no_gravity_velocity_func\n",
        "        self.p.body.velocity = pymunk.Vec2d(0, 0)\n",
        "\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        self.dodge_timer = max(0, self.dodge_timer-1)\n",
        "        if self.dodge_timer <= 0:\n",
        "            if self.is_grounded:\n",
        "                return GroundState.get_ground_state(self.p)\n",
        "            else:\n",
        "                in_air = self.p.states['in_air']\n",
        "                if hasattr(self, 'jumps_left'):\n",
        "                    in_air.jumps_left = self.jumps_left\n",
        "                    in_air.jump_timer = 0\n",
        "                    in_air.recoveries_left = self.recoveries_left\n",
        "                return in_air\n",
        "        return None\n",
        "\n",
        "    def exit(self) -> None:\n",
        "        self.p.body.body_type = pymunk.Body.DYNAMIC\n",
        "        self.p.body.velocity_func = pymunk.Body.update_velocity\n",
        "        self.p.body.velocity = pymunk.Vec2d(0, 0)\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('dodge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzQW3ZFWjYdP"
      },
      "source": [
        "#### Move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "xFB5GgOqjcHA"
      },
      "outputs": [],
      "source": [
        "class MoveManager():\n",
        "    def __init__(self, player: \"Player\", move_data):\n",
        "        self.p = player\n",
        "        self.move_data = move_data\n",
        "        self.all_hit_agents: List = []             # List of LegendAgent instances (to be defined elsewhere)\n",
        "        initial_power = move_data['powers'][move_data['move']['initialPowerIndex']]\n",
        "        self.current_power = Power.get_power(initial_power)\n",
        "        self.current_power.p = self.p\n",
        "        self.frame = 0\n",
        "        self.move_facing_direction = self.p.facing\n",
        "        self.hit_agent = None\n",
        "        self.keys = {\n",
        "            'LIGHT': 'j',\n",
        "            'HEAVY': 'k',\n",
        "            'THROW': 'l'\n",
        "        }\n",
        "\n",
        "    def do_move(self, is_holding_move_type: bool) -> bool:\n",
        "        \"\"\"\n",
        "        action: list of ints (e.g. 0 or 1) representing input keys.\n",
        "        is_holding_move_type: whether the move key is held.\n",
        "        \"\"\"\n",
        "        self.move_facing_direction = self.p.facing\n",
        "        key = self.keys[self.move_data['move']['actionKey']]\n",
        "        holding_move_key = self.p.input.key_status[key].held\n",
        "        done, next_power = self.current_power.do_power(holding_move_key, is_holding_move_type, self)\n",
        "        if next_power is not None:\n",
        "            self.current_power = next_power\n",
        "        self.frame += 1\n",
        "        return done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_joKiMJlVSC"
      },
      "source": [
        "#### Frame Change Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "k3-oNTUflUf1"
      },
      "outputs": [],
      "source": [
        "class HurtboxPositionChange():\n",
        "    def __init__(self, xOffset=0, yOffset=0, width=0, height=0, active=False):\n",
        "        self.xOffset = xOffset\n",
        "        self.yOffset = yOffset\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.active = active\n",
        "\n",
        "class CasterPositionChange():\n",
        "    def __init__(self, x=0, y=0, active=False):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.active = active\n",
        "\n",
        "class DealtPositionTarget():\n",
        "    def __init__(self, xOffset=0, yOffset=0, active=False):\n",
        "        self.xOffset = xOffset\n",
        "        self.yOffset = yOffset\n",
        "        self.active = active\n",
        "\n",
        "class CasterVelocitySet():\n",
        "    def __init__(self, magnitude=0.0, directionDeg=0.0, active=False):\n",
        "        self.magnitude = magnitude\n",
        "        self.directionDeg = directionDeg\n",
        "        self.active = active\n",
        "\n",
        "class CasterVelocitySetXY():\n",
        "    def __init__(self, magnitudeX=0.0, magnitudeY=0.0, activeX=False, activeY=False):\n",
        "        self.magnitudeX = magnitudeX\n",
        "        self.magnitudeY = -magnitudeY\n",
        "        self.activeX = activeX\n",
        "        self.activeY = activeY\n",
        "\n",
        "class CasterVelocityDampXY():\n",
        "    def __init__(self, dampX=1.0, dampY=1.0, activeX=False, activeY=False):\n",
        "        self.dampX = dampX\n",
        "        self.dampY = dampY\n",
        "        self.activeX = activeX\n",
        "        self.activeY = activeY\n",
        "\n",
        "class CastFrameChangeHolder():\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        data: a dictionary representing a single frame change from the cast data.\n",
        "        For each element, if its data is present in the dictionary, instantiate the corresponding class;\n",
        "        otherwise, use a default instance.\n",
        "        \"\"\"\n",
        "        self.frame = data.get(\"frame\", 0)\n",
        "\n",
        "        # For each change, if its key is present, create an instance with the provided data.\n",
        "        # Otherwise, instantiate with default values.\n",
        "        if \"casterPositionChange\" in data:\n",
        "            cp_data = data[\"casterPositionChange\"]\n",
        "            self.caster_position_change = CasterPositionChange(\n",
        "                x=cp_data.get(\"x\", 0),\n",
        "                y=cp_data.get(\"y\", 0),\n",
        "                active=cp_data.get(\"active\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.caster_position_change = CasterPositionChange()\n",
        "\n",
        "        if \"dealtPositionTarget\" in data:\n",
        "            dpt_data = data[\"dealtPositionTarget\"]\n",
        "            self.dealt_position_target = DealtPositionTarget(\n",
        "                xOffset=dpt_data.get(\"xOffset\", 0),\n",
        "                yOffset=dpt_data.get(\"yOffset\", 0),\n",
        "                active=dpt_data.get(\"active\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.dealt_position_target = DealtPositionTarget()\n",
        "\n",
        "        if \"casterVelocitySet\" in data:\n",
        "            cvs_data = data[\"casterVelocitySet\"]\n",
        "            self.caster_velocity_set = CasterVelocitySet(\n",
        "                magnitude=cvs_data.get(\"magnitude\", 0.0),\n",
        "                directionDeg=cvs_data.get(\"directionDeg\", 0.0),\n",
        "                active=cvs_data.get(\"active\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.caster_velocity_set = None\n",
        "\n",
        "        if \"casterVelocitySetXY\" in data:\n",
        "            cvsxy_data = data[\"casterVelocitySetXY\"]\n",
        "            self.caster_velocity_set_xy = CasterVelocitySetXY(\n",
        "                magnitudeX=cvsxy_data.get(\"magnitudeX\", 0.0),\n",
        "                magnitudeY=cvsxy_data.get(\"magnitudeY\", 0.0),\n",
        "                activeX=cvsxy_data.get(\"activeX\", False),\n",
        "                activeY=cvsxy_data.get(\"activeY\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.caster_velocity_set_xy = None\n",
        "\n",
        "        if \"casterVelocityDampXY\" in data:\n",
        "            cvdxy_data = data[\"casterVelocityDampXY\"]\n",
        "            self.caster_velocity_damp_xy = CasterVelocityDampXY(\n",
        "                dampX=cvdxy_data.get(\"dampX\", 1.0),\n",
        "                dampY=cvdxy_data.get(\"dampY\", 1.0),\n",
        "                activeX=cvdxy_data.get(\"activeX\", False),\n",
        "                activeY=cvdxy_data.get(\"activeY\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.caster_velocity_damp_xy = None\n",
        "\n",
        "        if \"hurtboxPositionChange\" in data:\n",
        "            hpc_data = data[\"hurtboxPositionChange\"]\n",
        "            self.hurtbox_position_change = HurtboxPositionChange(\n",
        "                xOffset=hpc_data.get(\"xOffset\", 0),\n",
        "                yOffset=hpc_data.get(\"yOffset\", 0),\n",
        "                width=hpc_data.get(\"width\", 0),\n",
        "                height=hpc_data.get(\"height\", 0),\n",
        "                active=hpc_data.get(\"active\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.hurtbox_position_change = HurtboxPositionChange()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<CastFrameChangeHolder frame={self.frame}>\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YTVrhr0dRvv"
      },
      "source": [
        "#### Cast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "oVlMkEWPdSWm"
      },
      "outputs": [],
      "source": [
        "class Cast():\n",
        "    def __init__(self, cast_data):\n",
        "        self.frame_idx = 0\n",
        "        self.cast_data = cast_data\n",
        "        self.startup_frames = cast_data.get(\"startupFrames\", 0) // 2\n",
        "        self.attack_frames = cast_data.get(\"attackFrames\", 0) // 2\n",
        "        self.base_damage = cast_data.get(\"baseDamage\", 0)\n",
        "        self.variable_force = cast_data.get(\"variableForce\", 0.0)\n",
        "        self.fixed_force = cast_data.get(\"fixedForce\", 0.0)\n",
        "        self.hit_angle_deg = cast_data.get(\"hitAngleDeg\", 0.0)\n",
        "        self.must_be_held = cast_data.get(\"mustBeHeld\", False)\n",
        "        self.collision_check_points = cast_data.get(\"collisionCheckPoints\", [])\n",
        "        self.hitboxes = cast_data.get(\"hitboxes\", [])\n",
        "\n",
        "    @staticmethod\n",
        "    def get_cast(cast_data) -> \"Cast\":\n",
        "        return Cast(cast_data)\n",
        "\n",
        "    def get_frame_data(self, idx):\n",
        "        \"\"\"\n",
        "        Iterate through the cast_data's 'frameChanges' list (if present) and return a\n",
        "        CastFrameChangeHolder built from the dictionary whose 'frame' equals idx.\n",
        "        If none is found, return None.\n",
        "        \"\"\"\n",
        "        frame_changes = self.cast_data.get(\"frameChanges\", [])\n",
        "        for change_data in frame_changes:\n",
        "            # Only use the data that is present; don't create a new change if not provided.\n",
        "            if change_data.get(\"frame\") == idx:\n",
        "                return CastFrameChangeHolder(change_data)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPOX5xjac5XC"
      },
      "source": [
        "#### Power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "yWsjNY8pc64M"
      },
      "outputs": [],
      "source": [
        "class Power():\n",
        "\n",
        "    def __init__(self, power_data, casts):\n",
        "        \"\"\"\n",
        "        power_data: an object (or dict) representing the PowerScriptableObject.\n",
        "                    Expected to have attributes like recovery, fixedRecovery,\n",
        "                    onHitNextPower, onMissNextPower, hitAngleDeg, minCharge, isCharge, etc.\n",
        "        \"\"\"\n",
        "        self.power_data = power_data\n",
        "        self.casts = casts\n",
        "        self.cast_idx = 0\n",
        "        self.total_frame_count = 0\n",
        "        self.frames_into_recovery = 0\n",
        "        self.recovery_frames = 0\n",
        "        self.hit_anyone = False\n",
        "        self.dealt_position_target_exists = False\n",
        "        self.current_dealt_position_target = (0.0, 0.0)\n",
        "        self.agents_in_move = []\n",
        "        self.is_switching_casts = True\n",
        "        self.past_point_positions = []\n",
        "\n",
        "        # deal with the power data\n",
        "        self.power_id = power_data.get('powerID', -1)\n",
        "        self.fixed_recovery = power_data.get('fixedRecovery', 0) // 2\n",
        "        self.recovery = power_data.get('recovery', 0) // 2\n",
        "        self.cooldown = power_data.get('cooldown', 0) // 2\n",
        "        self.min_charge = power_data.get('minCharge', 0) // 2\n",
        "        self.stun_time = power_data.get('stunTime', 0) // 2\n",
        "        self.hit_angle_deg = power_data.get('hitAngleDeg', 0.0)\n",
        "        self.is_charge = power_data.get('isCharge', False)\n",
        "        self.damage_over_life_of_hitbox = power_data.get('damageOverLifeOfHitbox', False)\n",
        "        self.disable_caster_gravity = power_data.get('disableCasterGravity', False)\n",
        "        self.disable_hit_gravity = power_data.get('disableHitGravity', False)\n",
        "        self.target_all_hit_agents = power_data.get('targetAllHitAgents', False)\n",
        "        self.transition_on_instant_hit = power_data.get('transitionOnInstantHit', False)\n",
        "        self.on_hit_velocity_set_active = power_data.get('onHitVelocitySetActive', False)\n",
        "        self.on_hit_velocity_set_magnitude = power_data.get('onHitVelocitySetMagnitude', 0.0)\n",
        "        self.on_hit_velocity_set_direction_deg = power_data.get('onHitVelocitySetDirectionDeg', 0.0)\n",
        "        self.enable_floor_drag = power_data.get('enableFloorDrag', False)\n",
        "\n",
        "        # Next-power indices (set to -1 if not provided)\n",
        "        self.on_hit_next_power_index = power_data.get('onHitNextPowerIndex', -1)\n",
        "        self.on_miss_next_power_index = power_data.get('onMissNextPowerIndex', -1)\n",
        "        self.on_ground_next_power_index = power_data.get('onGroundNextPowerIndex', -1)\n",
        "\n",
        "        # last_power is True if both onHitNextPower and onMissNextPower are None.\n",
        "        self.last_power = (self.on_hit_next_power_index == -1 and self.on_miss_next_power_index == -1)\n",
        "\n",
        "        if casts and len(casts) > 0:\n",
        "            # Use the last cast to determine recoveryFrames.\n",
        "            self.recovery_frames = self.recovery + self.fixed_recovery\n",
        "\n",
        "    @staticmethod\n",
        "    def get_power(power_data) -> \"Power\":\n",
        "        casts = [Cast.get_cast(cast) for cast in power_data['casts']]\n",
        "        return Power(power_data, casts)\n",
        "\n",
        "    def do_power(self, holding_key, is_holding_move_type, move_manager):\n",
        "        \"\"\"\n",
        "        Execute one frame of the power.\n",
        "\n",
        "        Parameters:\n",
        "          holding_key (bool): whether the move key is held.\n",
        "          is_holding_move_type (bool): e.g. whether a charge modifier is held.\n",
        "          move_manager: the MoveManager (with attributes such as moveFacingDirection, hit_agent, all_hit_agents, etc.)\n",
        "\n",
        "        Returns a tuple (done, next_power):\n",
        "          - done (bool): whether this power (and move) is finished.\n",
        "          - next_power: the next Power instance to transition to (or None if finished).\n",
        "        \"\"\"\n",
        "        done = False\n",
        "        transitioning_to_next_power = False\n",
        "        next_power = None\n",
        "\n",
        "        # For recovery-block checks; initialize defaults in case not set later.\n",
        "        in_startup = False\n",
        "        in_attack = False\n",
        "\n",
        "        # Disable caster gravity.\n",
        "        self.p.set_gravity_disabled(self.disable_caster_gravity)\n",
        "\n",
        "        is_past_min_charge = self.total_frame_count > self.min_charge\n",
        "        last_cast = self.casts[-1]\n",
        "        is_past_max_charge = self.total_frame_count > last_cast.startup_frames\n",
        "\n",
        "        # If this power is a charge and either (a) not holding key and past min charge, or (b) past max charge, then switch.\n",
        "        if self.is_charge and ((not holding_key and is_past_min_charge) or is_past_max_charge):\n",
        "            if self.on_miss_next_power_index != -1:\n",
        "                miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                next_power = Power.get_power(miss_power)\n",
        "            else:\n",
        "                print(\"...how?\")\n",
        "        else:\n",
        "            current_cast: Cast = self.casts[self.cast_idx]\n",
        "            cfch = current_cast.get_frame_data(current_cast.frame_idx)\n",
        "            # Calculate hit vector\n",
        "\n",
        "            hit_vector = (0.0, 0.0, 0.0)\n",
        "            if cfch is not None and cfch.dealt_position_target is not None and cfch.dealt_position_target.active:\n",
        "                self.dealt_position_target_exists = True\n",
        "                self.current_dealt_position_target = (cfch.dealt_position_target.xOffset, cfch.dealt_position_target.yOffset)\n",
        "            else:\n",
        "                self.dealt_position_target_exists = False\n",
        "                self.current_dealt_position_target = (0.0, 0.0)\n",
        "            if not self.dealt_position_target_exists:\n",
        "                # No target: calculate force from angle.\n",
        "                # Assume hitAngleDeg may be a wrapped value with a 'Value' attribute; otherwise, use power_data.hitAngleDeg.\n",
        "                if current_cast.hit_angle_deg != 0.0:\n",
        "                    hit_angle_deg = current_cast.hit_angle_deg\n",
        "                else:\n",
        "                    hit_angle_deg = self.hit_angle_deg\n",
        "                hit_vector = (\n",
        "                    math.cos(math.radians(hit_angle_deg)),\n",
        "                    -math.sin(math.radians(hit_angle_deg)),\n",
        "                    0.0\n",
        "                )\n",
        "                # Multiply x by moveFacingDirection.\n",
        "                hit_vector = (hit_vector[0] * int(move_manager.move_facing_direction), hit_vector[1], hit_vector[2])\n",
        "\n",
        "            in_startup = current_cast.frame_idx < current_cast.startup_frames\n",
        "            is_in_attack_frames = current_cast.frame_idx < (current_cast.startup_frames + current_cast.attack_frames)\n",
        "            in_attack = (not in_startup) and (is_in_attack_frames or current_cast.must_be_held)\n",
        "\n",
        "            if in_startup:\n",
        "                self.p.do_cast_frame_changes_with_changes(cfch, self.enable_floor_drag, move_manager)\n",
        "                self.p.set_hitboxes_to_draw()\n",
        "            elif in_attack:\n",
        "                self.p.do_cast_frame_changes_with_changes(cfch, self.enable_floor_drag, move_manager)\n",
        "                self.p.set_hitboxes_to_draw(current_cast.hitboxes,\n",
        "                                                  current_cast.collision_check_points,\n",
        "                                                  move_manager.move_facing_direction)\n",
        "\n",
        "                cast_damage = current_cast.base_damage\n",
        "                if self.damage_over_life_of_hitbox:\n",
        "                    damage_to_deal = cast_damage / current_cast.attack_frames\n",
        "                else:\n",
        "                    damage_to_deal = cast_damage\n",
        "\n",
        "                # Check collision.\n",
        "                collided = False\n",
        "                if self.is_switching_casts:\n",
        "                    self.is_switching_casts = False\n",
        "                else:\n",
        "                    for i in range(len(current_cast.collision_check_points)):\n",
        "                        point = current_cast.collision_check_points[i]\n",
        "                        point_offset = Capsule.get_hitbox_offset(point['xOffset'], point['yOffset'])\n",
        "                        # Multiply x offset by moveFacingDirection.\n",
        "                        point_offset = (point_offset[0] * int(move_manager.move_facing_direction), point_offset[1])\n",
        "                        # Assume agent.position is a tuple (x, y)\n",
        "                        point_pos = (self.p.body.position[0] + point_offset[0], self.p.body.position[1] + point_offset[1])\n",
        "                        collided = point_pos[1] > 1.54\n",
        "\n",
        "                # Initialize past point positions for the next frame.\n",
        "                self.past_point_positions = []\n",
        "                for point in current_cast.collision_check_points:\n",
        "                    point_offset = Capsule.get_hitbox_offset(point['xOffset'], point['yOffset'])\n",
        "                    point_offset = (point_offset[0] * int(move_manager.move_facing_direction), point_offset[1])\n",
        "                    point_pos = (self.p.body.position[0] + point_offset[0], self.p.body.position[1] + point_offset[1])\n",
        "                    self.past_point_positions.append(point_pos)\n",
        "\n",
        "                if current_cast.must_be_held and (not is_holding_move_type):\n",
        "                    transitioning_to_next_power = True\n",
        "                    if self.on_miss_next_power_index != -1:\n",
        "                        miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                        next_power = Power.get_power(miss_power)\n",
        "                        next_power = move_manager.move_data.onMissNextPower.get_power()\n",
        "                if collided:\n",
        "                    transitioning_to_next_power = True\n",
        "                    if self.on_ground_next_power_index != -1:\n",
        "                        ground_power = move_manager.move_data['powers'][self.on_ground_next_power_index ]\n",
        "                        next_power = Power.get_power(ground_power)\n",
        "                    elif self.on_miss_next_power_index != -1:\n",
        "                        miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                        next_power = Power.get_power(miss_power)\n",
        "\n",
        "                # Check hitboxes.\n",
        "                hitbox_hit = False\n",
        "                hit_agents = []\n",
        "                for hitbox in current_cast.hitboxes:\n",
        "                    hitbox_offset = Capsule.get_hitbox_offset(hitbox['xOffset'], hitbox['yOffset'])\n",
        "                    hitbox_offset = (hitbox_offset[0] * int(move_manager.move_facing_direction), hitbox_offset[1])\n",
        "                    hitbox_pos = (self.p.body.position[0] + hitbox_offset[0], self.p.body.position[1] + hitbox_offset[1])\n",
        "                    hitbox_size = Capsule.get_hitbox_size(hitbox['width'], hitbox['height'])\n",
        "                    capsule1 = CapsuleCollider(center=hitbox_pos, width=hitbox_size[0], height=hitbox_size[1])\n",
        "                    intersects = self.p.opponent.hurtbox_collider.intersects(capsule1)\n",
        "                    hit_agent = self.p.opponent\n",
        "                    #print(self.p.opponent)\n",
        "                    #print(hitbox_pos, hitbox_size)\n",
        "                    #print(self.p.opponent.hurtbox_collider.center, self.p.opponent.hurtbox_collider.width, self.p.opponent.hurtbox_collider.height)\n",
        "                    if intersects and hit_agent.state.vulnerable():\n",
        "                        #print(self.p.opponent.hurtbox_collider, capsule1)\n",
        "                        hitbox_hit = True\n",
        "                        #print(f'Player {self.p.agent_id} HIT!')\n",
        "                        if not self.hit_anyone:\n",
        "                            if self.on_hit_velocity_set_active:\n",
        "                                on_hit_vel = (math.cos(math.radians(self.on_hit_velocity_set_direction_deg)),\n",
        "                                                math.sin(math.radians(self.on_hit_velocity_set_direction_deg)))\n",
        "                                on_hit_vel = (on_hit_vel[0] * self.on_hit_velocity_set_magnitude, on_hit_vel[1])\n",
        "\n",
        "                                self.p.body.velocity = pymunk.Vec2d(on_hit_vel[0], on_hit_vel[1])\n",
        "                        self.hit_anyone = True\n",
        "                        force_magnitude = (current_cast.fixed_force +\n",
        "                                            current_cast.variable_force * hit_agent.damage * 0.02622)\n",
        "                        if hit_agent not in hit_agents:\n",
        "                            if self.damage_over_life_of_hitbox:\n",
        "                                hit_agent.apply_damage(damage_to_deal, self.stun_time,\n",
        "                                                    (hit_vector[0] * (force_magnitude / current_cast.cast_data.attackFrames),\n",
        "                                                    hit_vector[1] * (force_magnitude / current_cast.cast_data.attackFrames)))\n",
        "                            else:\n",
        "                                hit_agent.apply_damage(damage_to_deal, self.stun_time,\n",
        "                                                    (hit_vector[0] * force_magnitude, hit_vector[1] * force_magnitude))\n",
        "                            hit_agents.append(hit_agent)\n",
        "                        if hit_agent not in self.agents_in_move:\n",
        "                            if move_manager.hit_agent is None:\n",
        "                                move_manager.hit_agent = hit_agent\n",
        "                            if not self.damage_over_life_of_hitbox:\n",
        "                                hit_agent.apply_damage(damage_to_deal, self.stun_time,\n",
        "                                                    (hit_vector[0] * force_magnitude, hit_vector[1] * force_magnitude))\n",
        "                            hit_agent.set_gravity_disabled(self.disable_hit_gravity)\n",
        "                            self.agents_in_move.append(hit_agent)\n",
        "                        if hit_agent not in move_manager.all_hit_agents:\n",
        "                            hit_agent.just_got_hit = True\n",
        "                            move_manager.all_hit_agents.append(hit_agent)\n",
        "                if hitbox_hit and self.transition_on_instant_hit:\n",
        "                    if self.on_hit_next_power_index != -1:\n",
        "                        hit_power = move_manager.move_data['powers'][self.on_hit_next_power_index]\n",
        "                        next_power = Power.get_power(hit_power)\n",
        "                    elif self.on_miss_next_power_index != -1:\n",
        "                        miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                        next_power = Power.get_power(miss_power)\n",
        "                if self.cast_idx == len(self.casts) - 1 and self.last_power:\n",
        "                    self.frames_into_recovery += 1\n",
        "\n",
        "            # Increment the current cast's frame index.\n",
        "            current_cast.frame_idx += 1\n",
        "\n",
        "            # Recovery handling: if not transitioning and not in startup or attack.\n",
        "            if (not transitioning_to_next_power) and (not in_attack) and (not in_startup):\n",
        "                self.p.set_hitboxes_to_draw()\n",
        "                if self.cast_idx == len(self.casts) - 1:\n",
        "                    if self.frames_into_recovery >= self.recovery_frames:\n",
        "                        if self.last_power:\n",
        "                            done = True\n",
        "                        else:\n",
        "                            if self.hit_anyone:\n",
        "                                if self.on_hit_next_power_index != -1:\n",
        "                                    hit_power = move_manager.move_data['powers'][self.on_hit_next_power_index]\n",
        "                                    next_power = Power.get_power(hit_power)\n",
        "                                elif self.on_miss_next_power_index != -1:\n",
        "                                    miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                                    next_power = Power.get_power(miss_power)\n",
        "                            else:\n",
        "                                if self.on_miss_next_power_index != -1:\n",
        "                                    miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                                    next_power = Power.get_power(miss_power)\n",
        "                    else:\n",
        "                        self.frames_into_recovery += 1\n",
        "                else:\n",
        "                    self.cast_idx += 1\n",
        "                    self.is_switching_casts = True\n",
        "\n",
        "        self.total_frame_count += 1\n",
        "        if next_power is not None:\n",
        "            next_power.p = self.p\n",
        "        return done, next_power"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6fV-UuVEsGz"
      },
      "source": [
        "#### Attacking State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "bWqy_FUnEvsp"
      },
      "outputs": [],
      "source": [
        "class AttackState(PlayerObjectState):\n",
        "\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def give_move(self, move_type: \"MoveType\") -> None:\n",
        "        self.move_type = move_type\n",
        "        # load json Unarmed SLight.json\n",
        "        #with open('Unarmed SLight.json') as f:\n",
        "        #    move_data = json.load(f)\n",
        "        move_data = self.p.env.attacks[move_type]\n",
        "        self.move_manager = MoveManager(self.p, move_data)\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.dash_timer = self.p.dash_time\n",
        "        # get random number from 1 to 12\n",
        "        self.seed = random.randint(1, 12)\n",
        "        # Optionally, play a dash sound or animation here.\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        is_holding_move_type = self.move_type == self.p.get_move()\n",
        "\n",
        "        done = self.move_manager.do_move(is_holding_move_type)\n",
        "\n",
        "        if done:\n",
        "            self.p.set_hitboxes_to_draw()\n",
        "\n",
        "            if self.p.is_on_floor():\n",
        "                return GroundState.get_ground_state(self.p)\n",
        "            else:\n",
        "                in_air = self.p.states['in_air']\n",
        "                if hasattr(self, 'jumps_left'):\n",
        "                    in_air.jumps_left = self.jumps_left\n",
        "                    in_air.recoveries_left = self.recoveries_left\n",
        "                    in_air.jump_timer = 0\n",
        "                return in_air\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        player_anim, attack_anim = self.p.attack_anims[self.move_type]\n",
        "        current_power = self.move_manager.current_power\n",
        "        if isinstance(player_anim, str):\n",
        "            self.p.animation_sprite_2d.play(player_anim)\n",
        "        elif isinstance(player_anim, dict):\n",
        "\n",
        "            player_anim = player_anim[current_power.power_id]\n",
        "            if isinstance(player_anim, list):\n",
        "                current_cast = current_power.casts[current_power.cast_idx]\n",
        "                in_startup = current_cast.frame_idx < current_cast.startup_frames\n",
        "                self.p.animation_sprite_2d.play(player_anim[0 if in_startup else 1])\n",
        "            else:\n",
        "                self.p.animation_sprite_2d.play(player_anim[current_power.power_id])\n",
        "        else:\n",
        "            self.p.animation_sprite_2d.play(player_anim[self.seed % len(player_anim)])\n",
        "        #self.p.animation_sprite_2d.play('run')\n",
        "        if isinstance(attack_anim, str):\n",
        "            self.p.attack_sprite.play(attack_anim)\n",
        "        elif isinstance(attack_anim, dict):\n",
        "            attack_anim = attack_anim[current_power.power_id]\n",
        "            if isinstance(attack_anim, list):\n",
        "                current_cast = current_power.casts[current_power.cast_idx]\n",
        "                in_startup = current_cast.frame_idx < current_cast.startup_frames\n",
        "                self.p.attack_sprite.play(attack_anim[0 if in_startup else 1])\n",
        "            elif isinstance(attack_anim, tuple):\n",
        "                self.p.attack_sprite.play(attack_anim[self.seed % len(attack_anim)])\n",
        "            else:\n",
        "                self.p.attack_sprite.play(attack_anim)\n",
        "        else:\n",
        "            self.p.attack_sprite.play(attack_anim[self.seed % len(attack_anim)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRxHMA07ZKE0"
      },
      "source": [
        "#### AnimatedSprite 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "HlGhclgrZLW4"
      },
      "outputs": [],
      "source": [
        "def hex_to_rgb(hex_color):\n",
        "    \"\"\"Convert a hex string (e.g., '#FE9000') to an RGB tuple.\"\"\"\n",
        "    hex_color = hex_color.lstrip('#')\n",
        "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Animation():\n",
        "    frames: list[np.ndarray]\n",
        "    frame_durations: list[float]\n",
        "    frames_per_step: list[float]\n",
        "\n",
        "class AnimationSprite2D(GameObject):\n",
        "    ENV_FPS = 30  # Environment FPS\n",
        "    albert_palette = {\n",
        "        \"base\": hex_to_rgb(\"#FE9000\"),\n",
        "        \"sides\": hex_to_rgb(\"#A64A00\"),\n",
        "        \"top_bottom\": hex_to_rgb(\"#FFB55A\"),\n",
        "        \"outline\": hex_to_rgb(\"#A02800\")\n",
        "    }\n",
        "\n",
        "    kai_palette = {\n",
        "        \"base\": hex_to_rgb(\"#00A1FE\"),\n",
        "        \"sides\": hex_to_rgb(\"#006080\"),\n",
        "        \"top_bottom\": hex_to_rgb(\"#74CEFF\"),\n",
        "        \"outline\": hex_to_rgb(\"#0069BA\")\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, camera, scale, animation_folder, agent_id):\n",
        "        super().__init__()\n",
        "        self.finished = False\n",
        "        self.scale = scale\n",
        "        self.agent_id = agent_id\n",
        "        self.current_frame_index = 0\n",
        "        self.frame_timer = 0\n",
        "        self.animation_folder = animation_folder\n",
        "\n",
        "        self.animations: dict[str, Animation] = {}\n",
        "        self.current_animation = None\n",
        "        self.frames = []\n",
        "        self.current_frame_index = 0\n",
        "\n",
        "        self.anim_data = {\n",
        "            #'altroll': [1.0],\n",
        "            #'alhappy': [1.0],\n",
        "            'default': [1.4],\n",
        "            'unarmednsig_paper': [1.6],\n",
        "            'unarmednsig_rock': [1.6],\n",
        "            'unarmednsig_scissors': [1.6],\n",
        "            'unarmedrecovery': [1.0],\n",
        "            'unarmeddlight': [1.2],\n",
        "        }\n",
        "\n",
        "        self.color_mapping = {self.albert_palette[key]: self.kai_palette[key] for key in self.albert_palette}\n",
        "\n",
        "\n",
        "        self.loaded = False\n",
        "\n",
        "    def load_animations(self, animation_folder):\n",
        "        \"\"\"\n",
        "        Loads animations from the specified folder.\n",
        "        \"\"\"\n",
        "        self.loaded = True\n",
        "        if not os.path.exists(animation_folder):\n",
        "            print(f\"Assets folder {animation_folder} not found!\")\n",
        "            return\n",
        "\n",
        "        for category in os.listdir(animation_folder):\n",
        "            category_path = os.path.join(animation_folder, category)\n",
        "            if os.path.isdir(category_path):\n",
        "                frames = []\n",
        "                for file in sorted(os.listdir(category_path)):\n",
        "                    file_name = os.path.splitext(file)[0]\n",
        "                    self.animations[file_name] = self.load_animation(os.path.join(category_path, file))\n",
        "            else:\n",
        "                file_name = os.path.splitext(category)[0]\n",
        "                self.animations[file_name] = self.load_animation(category_path)\n",
        "\n",
        "\n",
        "    def remap_colors(self, image, mapping):\n",
        "        \"\"\"\n",
        "        Given an image as a numpy ndarray (H x W x 3 or 4) and a mapping dictionary\n",
        "        mapping RGB tuples to new RGB tuples, return a new image with the colors replaced.\n",
        "        \"\"\"\n",
        "        # Make a copy so as not to modify the original.\n",
        "        out = image.copy()\n",
        "\n",
        "        # Determine whether the image has an alpha channel.\n",
        "        has_alpha = out.shape[2] == 4\n",
        "\n",
        "        # For each mapping entry, create a mask and replace the RGB channels.\n",
        "        for old_color, new_color in mapping.items():\n",
        "            # Create a boolean mask for pixels that match old_color.\n",
        "            # Compare only the first 3 channels.\n",
        "            mask = (out[..., :3] == old_color).all(axis=-1)\n",
        "\n",
        "            # Replace the pixel's R, G, B values with the new_color.\n",
        "            out[mask, 0] = new_color[0]\n",
        "            out[mask, 1] = new_color[1]\n",
        "            out[mask, 2] = new_color[2]\n",
        "            # The alpha channel (if present) remains unchanged.\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def load_animation(self, file_path):\n",
        "        # Load GIF and extract frames\n",
        "        gif = Image.open(file_path)\n",
        "        frames = []\n",
        "        frame_durations = []  # Store frame durations in milliseconds\n",
        "        total_duration = 0\n",
        "\n",
        "        # get file name without extension\n",
        "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "\n",
        "        for frame in ImageSequence.Iterator(gif):\n",
        "            # Convert and scale frame\n",
        "\n",
        "            pygame_frame = pygame.image.fromstring(frame.convert(\"RGBA\").tobytes(), frame.size, \"RGBA\")\n",
        "\n",
        "            # if self.agent_id == 1:\n",
        "            #     # Convert the pygame surface to a numpy array.\n",
        "            #     frame_array = pygame.surfarray.array3d(pygame_frame).transpose(1, 0, 2)  # shape (H, W, 3)\n",
        "\n",
        "            #     # Remap colors using our mapping.\n",
        "            #     new_frame_array = self.remap_colors(frame_array, self.color_mapping)\n",
        "\n",
        "            #     # Optionally, create a new pygame surface from the new_frame_array.\n",
        "            #     # (If you need to convert back to a surface, note that pygame expects (width, height).)\n",
        "            #     pygame_frame = pygame.surfarray.make_surface(new_frame_array.transpose(1, 0, 2))\n",
        "            #scaled_frame = pygame.transform.scale(pygame_frame, (int(frame.width * scale), int(frame.height * scale)))\n",
        "            frames.append(pygame_frame)\n",
        "\n",
        "            # Extract frame duration\n",
        "            duration = frame.info.get('duration', 100)  # Default 100ms if missing\n",
        "            frame_durations.append(duration)\n",
        "            total_duration += duration\n",
        "\n",
        "        gif.close()\n",
        "\n",
        "        # Compute how many game steps each GIF frame should last\n",
        "        frames_per_step = [max(1, round((duration / 1000) * self.ENV_FPS)) for duration in frame_durations]\n",
        "\n",
        "        return Animation(frames, frame_durations, frames_per_step)\n",
        "\n",
        "    def play(self, animation_name):\n",
        "        \"\"\"\n",
        "        Plays the given animation.\n",
        "        \"\"\"\n",
        "        if animation_name == None:\n",
        "            self.current_animation = None\n",
        "            return\n",
        "        if animation_name in self.animations and self.current_animation != animation_name:\n",
        "            #print(animation_name, 'from', self.current_animation)\n",
        "            self.current_animation = animation_name\n",
        "            self.frames = self.animations[animation_name].frames\n",
        "            self.current_data = self.anim_data.get(animation_name, self.anim_data['default'])\n",
        "            self.frame_durations = self.animations[animation_name].frame_durations\n",
        "            self.frames_per_step = self.animations[animation_name].frames_per_step\n",
        "            self.frame_timer = 0\n",
        "            self.current_frame_index = 0\n",
        "\n",
        "    def process(self, position):\n",
        "        \"\"\"\n",
        "        Advances the animation, ensuring it syncs properly with a 30 FPS game loop.\n",
        "        \"\"\"\n",
        "\n",
        "        self.position = position\n",
        "        if self.current_animation is None: return\n",
        "        if not self.finished:\n",
        "            self.frame_timer += 1  # Increment frame timer (game steps)\n",
        "\n",
        "            # Move to the next frame only when enough game steps have passed\n",
        "            if self.frame_timer >= self.frames_per_step[self.current_frame_index]:\n",
        "                self.frame_timer = 0\n",
        "                self.current_frame_index += 1\n",
        "                if self.current_frame_index >= len(self.frames):\n",
        "                    self.current_frame_index = 0\n",
        "                    #self.finished = True  # Mark for deletion\n",
        "\n",
        "    def render(self, camera: Camera, flipped: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Draws the current animation frame on the screen at a fixed position.\n",
        "        \"\"\"\n",
        "        if not self.loaded:\n",
        "            self.load_animations(self.animation_folder)\n",
        "        if self.current_animation is None or self.current_animation == '': return\n",
        "        if not self.finished:\n",
        "            #camera.canvas.blit(self.frames[self.current_frame_index], (0,0))\n",
        "            width = self.current_data[0]\n",
        "            self.draw_image(camera.canvas, self.frames[self.current_frame_index], self.position, self.scale * width, camera, flipped=flipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_-olBqx4-J7"
      },
      "source": [
        "#### Player GameObject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Tt6DNLXqsMaM"
      },
      "outputs": [],
      "source": [
        "class Player(GameObject):\n",
        "    PLAYER_RADIUS = 10\n",
        "\n",
        "    def __init__(self, env, agent_id: int, start_position=[0,0], color=[200, 200, 0, 255]):\n",
        "        self.env = env\n",
        "\n",
        "        self.delta = env.dt\n",
        "        self.agent_id = agent_id\n",
        "        self.space = self.env.space\n",
        "\n",
        "        hitbox_size = Capsule.get_hitbox_size(290//2, 320//2)\n",
        "        self.hurtbox_collider = CapsuleCollider(center=(0, 0), width=hitbox_size[0], height=hitbox_size[1])\n",
        "\n",
        "        self.start_position = start_position\n",
        "\n",
        "        # Create input handlers\n",
        "        self.input = PlayerInputHandler()\n",
        "\n",
        "        # Attack anim stuff\n",
        "\n",
        "        self.attack_anims = {\n",
        "            MoveType.NLIGHT : ('idle', 'unarmednlightfinisher'),\n",
        "            MoveType.DLIGHT : ('idle', 'unarmeddlight'),\n",
        "            MoveType.SLIGHT : ('alpunch', 'unarmedslight'),\n",
        "            MoveType.NSIG   : ('alup', {28: 'unarmednsig_held', 29: ('unarmednsig_paper', 'unarmednsig_rock', 'unarmednsig_scissors')}),\n",
        "            MoveType.DSIG   : ('idle', {26: 'unarmeddsig_held', 27: 'unarmeddsig_end'}),\n",
        "            MoveType.SSIG   : ('alssig', {21: 'unarmedssig_held', 22: 'unarmedssig_end'}),\n",
        "            MoveType.NAIR   : ('alup', 'unarmednlightnofinisher'),\n",
        "            MoveType.DAIR   : ('alpunch', 'unarmeddair'),\n",
        "            MoveType.SAIR   : ('alpunch', 'unarmedsair'),\n",
        "            MoveType.RECOVERY : ('alup', 'unarmedrecovery'),\n",
        "            MoveType.GROUNDPOUND : ('algroundpound', {16: ['unarmedgp', 'unarmedgp_held'], 17: 'unarmedgp_end', 18: 'unarmedgp_end', 19: 'unarmedgp_end'}),\n",
        "        }\n",
        "\n",
        "        # Create player states\n",
        "        self.states_types: dict[str, PlayerObjectState] = {\n",
        "            'walking': WalkingState,\n",
        "            'standing': StandingState,\n",
        "            'turnaround': TurnaroundState,\n",
        "            'air_turnaround': AirTurnaroundState,\n",
        "            'sprinting': SprintingState,\n",
        "            'stun': StunState,\n",
        "            'in_air': InAirState,\n",
        "            'dodge': DodgeState,\n",
        "            'attack': AttackState,\n",
        "            'dash': DashState,\n",
        "            'backdash': BackDashState,\n",
        "            'KO': KOState,\n",
        "            'taunt': TauntState,\n",
        "        }\n",
        "        self.state_mapping = {\n",
        "            'WalkingState': 0,\n",
        "            'StandingState': 1,\n",
        "            'TurnaroundState': 2,\n",
        "            'AirTurnaroundState': 3,\n",
        "            'SprintingState': 4,\n",
        "            'StunState': 5,\n",
        "            'InAirState': 6,\n",
        "            'DodgeState': 7,\n",
        "            'AttackState': 8,\n",
        "            'DashState': 9,\n",
        "            'BackDashState': 10,\n",
        "            'KOState': 11,\n",
        "            'TauntState': 12,\n",
        "        }\n",
        "\n",
        "        self.states: dict[str, PlayerObjectState] = {\n",
        "            state_name: state_type(self) for state_name, state_type in self.states_types.items()\n",
        "        }\n",
        "        self.state = self.states['in_air']\n",
        "        self.state.jumps_left = 0\n",
        "        self.state.jump_timer = 0\n",
        "        self.state.recoveries_left = 0\n",
        "        self.state.is_base = True\n",
        "\n",
        "        # Other living stats\n",
        "        self.facing = Facing.RIGHT if start_position[0] < 0 else Facing.LEFT\n",
        "        self.damage = 0\n",
        "        self.smoothXVel = 0\n",
        "        self.damage_taken_this_stock = 0\n",
        "        self.damage_taken_total = 0\n",
        "        self.damage_done = 0\n",
        "        self.stocks = 3\n",
        "\n",
        "        self.prev_x = start_position[0]\n",
        "        self.prev_y = start_position[1]\n",
        "        self.damage_velocity = (0, 0)\n",
        "        self.target_vel = (0, 0)\n",
        "\n",
        "        self.hitboxes_to_draw = []\n",
        "        self.points_to_draw = []\n",
        "\n",
        "        # PyMunk Params\n",
        "        x, y = self.start_position\n",
        "        width, height = 0.87, 1.0\n",
        "        self.mass = 1\n",
        "\n",
        "        # Create PyMunk Object\n",
        "        self.shape = pymunk.Poly.create_box(None, size=(width, height))\n",
        "        self.shape.collision_type = 3 if agent_id == 0 else 4\n",
        "        self.shape.owner = self\n",
        "        #self.moment = pymunk.moment_for_poly(self.mass, self.shape.get_vertices())\n",
        "        self.moment = 1e9\n",
        "        self.body = pymunk.Body(self.mass, self.moment)\n",
        "        self.shape.body = self.body\n",
        "        self.shape.body.position = (x, y)\n",
        "        self.shape.friction = 0.7\n",
        "        self.shape.color = color\n",
        "\n",
        "        # Parameters\n",
        "        self.move_speed = 6.75\n",
        "        self.jump_speed = 8.9\n",
        "        self.in_air_ease = 6.75 / self.env.fps\n",
        "        self.run_speed = 8\n",
        "        self.dash_speed = 10\n",
        "        self.backdash_speed = 4\n",
        "        self.turnaround_time = 4\n",
        "        self.taunt_time = 30\n",
        "        self.backdash_time = 7\n",
        "        self.dodge_time = 10\n",
        "        self.grounded_dodge_cooldown = 30\n",
        "        self.smoothTimeX = 0.33 * self.env.fps\n",
        "        self.air_dodge_cooldown = 82\n",
        "        self.invincible_time = self.env.fps * 3\n",
        "        self.jump_cooldown = self.env.fps * 0.5\n",
        "        self.dash_time = self.env.fps * 0.3\n",
        "        self.dash_cooldown = 8\n",
        "\n",
        "        # Signals\n",
        "        self.just_got_hit = False\n",
        "\n",
        "        self.state_str = 'InAirState'\n",
        "\n",
        "        self.space.add(self.shape, self.body)\n",
        "\n",
        "        # Assets\n",
        "        self.assets_loaded = False\n",
        "        animation_folder = 'assets'\n",
        "        if not os.path.exists(animation_folder):\n",
        "            self.load_assets()\n",
        "        self.animation_sprite_2d = AnimationSprite2D(self.env.camera, 1.0, 'assets/player', agent_id)\n",
        "        self.attack_sprite = AnimationSprite2D(self.env.camera, 2.0, 'assets/attacks', agent_id)\n",
        "\n",
        "    def get_obs(self) -> list[float]:\n",
        "\n",
        "        obs = []\n",
        "        pos = self.body.position\n",
        "        # Clamp values to [-1, 1] (or replace with proper normalization if needed)\n",
        "        x_norm = max(-18, min(18, pos.x))\n",
        "        y_norm = max(-7, min(7, pos.y))\n",
        "        obs.extend([x_norm, y_norm])\n",
        "\n",
        "        vel = self.body.velocity\n",
        "        vx_norm = max(-10.0, min(10.0, vel.x))\n",
        "        vy_norm = max(-10.0, min(10.0, vel.y))\n",
        "        obs.extend([vx_norm, vy_norm])\n",
        "\n",
        "        obs.append(1.0 if self.facing == Facing.RIGHT else 0.0)\n",
        "\n",
        "        grounded = 1.0 if self.is_on_floor() else 0.0\n",
        "        obs.append(grounded)\n",
        "\n",
        "        obs.append(0.0 if grounded == 1.0 else 1.0)\n",
        "\n",
        "        obs.append(float(self.state.jumps_left) if hasattr(self.state, 'jumps_left') else 0.0)\n",
        "\n",
        "        current_state_name = type(self.state).__name__\n",
        "        state_index = self.state_mapping.get(current_state_name, 0)\n",
        "        obs.append(float(state_index))\n",
        "\n",
        "        obs.append(float(self.state.jumps_left) if hasattr(self.state, 'recoveries_left') else 0.0)\n",
        "\n",
        "        obs.append(float(self.state.dodge_timer) if hasattr(self.state, 'dodge_timer') else 0.0)\n",
        "\n",
        "        obs.append(float(self.state.stun_frames) if hasattr(self.state, 'stun_frames') else 0.0)\n",
        "\n",
        "        obs.append(float(self.damage) / 700.0)\n",
        "\n",
        "        # 12. Stocks – expected to be between 0 and 3.\n",
        "        obs.append(float(self.stocks))\n",
        "\n",
        "        # 13. Move type – if the state has a move_type attribute, otherwise 0.\n",
        "        obs.append(float(self.state.move_type) if hasattr(self.state, 'move_type') else 0.0)\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def respawn(self) -> None:\n",
        "        self.body.position = self.start_position\n",
        "        self.body.velocity = pymunk.Vec2d(0, 0)\n",
        "        self.damage = 0\n",
        "        self.damage_taken_this_stock = 0\n",
        "        self.smoothXVel = 0\n",
        "        self.target_vel = (0, 0)\n",
        "\n",
        "    def apply_damage(self, damage_default: float, stun_dealt: int=0, velocity_dealt: Tuple[float, float]=(0,0)):\n",
        "        self.damage = min(700, self.damage + damage_default)\n",
        "        self.damage_taken_this_stock += damage_default\n",
        "        self.damage_taken_total += damage_default\n",
        "        self.damage_taken_this_frame += damage_default\n",
        "        self.state.stunned(stun_dealt)\n",
        "        scale = (1.024 / 320.0) * 12 # 0.165\n",
        "        self.damage_velocity = (velocity_dealt[0] * scale, velocity_dealt[1] * scale)\n",
        "\n",
        "        self.opponent.damage_done += damage_default\n",
        "\n",
        "    def load_assets(self):\n",
        "        if self.assets_loaded: return\n",
        "        if os.path.isdir('assets'): return\n",
        "\n",
        "        data_path = \"assets.zip\"\n",
        "        if not os.path.isfile(data_path):\n",
        "            print(\"Downloading assets.zip...\")\n",
        "            url = \"https://drive.google.com/file/d/1F2MJQ5enUPVtyi3s410PUuv8LiWr8qCz/view?usp=sharing\"\n",
        "            gdown.download(url, output=data_path, fuzzy=True)\n",
        "\n",
        "\n",
        "        # check if directory\n",
        "        !unzip -q \"/content/$data_path\"\n",
        "        print(\"Downloaded!\")\n",
        "\n",
        "        self.assets_loaded = True\n",
        "\n",
        "    def is_on_floor(self) -> bool:\n",
        "        return self.shape.cache_bb().intersects(self.env.objects['ground'].shape.cache_bb())\n",
        "        #return abs(self.body.position.y - 1.540) < 0.03 and abs(self.body.position.x) < 5.77\n",
        "\n",
        "    def set_gravity_disabled(self, disabled:bool) -> None:\n",
        "        self.body.gravity_scale = 0 if disabled else 1\n",
        "\n",
        "    def render(self, screen, camera) -> None:\n",
        "        self.state.animate_player(camera)\n",
        "\n",
        "        position = self.body.position\n",
        "        self.animation_sprite_2d.process(position)\n",
        "        self.attack_sprite.process(position)\n",
        "        flipped = self.facing == Facing.LEFT\n",
        "        self.animation_sprite_2d.render(camera, flipped=flipped)\n",
        "        self.attack_sprite.render(camera, flipped=flipped)\n",
        "\n",
        "\n",
        "        hurtbox_offset = Capsule.get_hitbox_offset(0, 0)\n",
        "        hurtbox_offset = (hurtbox_offset[0] * int(self.facing), hurtbox_offset[1])\n",
        "        hurtbox_pos = (self.body.position[0] + hurtbox_offset[0], self.body.position[1] + hurtbox_offset[1])\n",
        "        hurtbox_data = np.array([\n",
        "            self.hurtbox_collider.center[0],\n",
        "            self.hurtbox_collider.center[1],\n",
        "            self.hurtbox_collider.width / (2 * WarehouseBrawl.BRAWL_TO_UNITS),\n",
        "            self.hurtbox_collider.height / (2 * WarehouseBrawl.BRAWL_TO_UNITS)\n",
        "        ])\n",
        "        Capsule.draw_hurtbox(camera, hurtbox_data, hurtbox_pos)\n",
        "\n",
        "        # Draw hitboxes\n",
        "        for hitbox in self.hitboxes_to_draw:\n",
        "            hitbox_offset = list(Capsule.get_hitbox_offset(hitbox['xOffset'], hitbox['yOffset']))\n",
        "            hitbox_offset[0] = hitbox_offset[0] * int(self.facing)\n",
        "            hitbox_pos = (self.body.position[0] + hitbox_offset[0], self.body.position[1] + hitbox_offset[1])\n",
        "            hitbox_data = np.array([\n",
        "                0,\n",
        "                0,\n",
        "                hitbox['width'],\n",
        "                hitbox['height']\n",
        "            ])\n",
        "            Capsule.draw_hitbox(camera, hitbox_data, hitbox_pos)\n",
        "\n",
        "        # draw circle\n",
        "        cc = (227, 138, 14) if self.agent_id == 0 else (18, 131, 201)\n",
        "        screen_pos = camera.gtp((int(position[0]), int(position[1])-1))\n",
        "        pygame.draw.circle(camera.canvas, cc, screen_pos, camera.scale_gtp() * 0.25)\n",
        "\n",
        "\n",
        "    def set_hitboxes_to_draw(self, hitboxes: Optional[List[Any]]=None,\n",
        "                             points: Optional[List[Any]]=None,\n",
        "                             move_facing: Optional[Facing]=None):\n",
        "        if hitboxes is None:\n",
        "            self.hitboxes_to_draw = []\n",
        "        else:\n",
        "            self.facing = move_facing\n",
        "            self.hitboxes_to_draw = hitboxes\n",
        "            self.points_to_draw = points\n",
        "\n",
        "    def smooth_damp(current, target, current_velocity, smooth_time, dt=0.016):\n",
        "        # This is a very rough approximation.\n",
        "        # In a real implementation, you'd compute the damped value properly.\n",
        "        diff = target - current\n",
        "        change = diff * dt / smooth_time if smooth_time != 0 else diff\n",
        "        new_value = current + change\n",
        "        new_velocity = change / dt\n",
        "        return new_value, new_velocity\n",
        "\n",
        "    def do_cast_frame_changes(self):\n",
        "        # Create a new CastFrameChangeHolder and force hurtbox change.\n",
        "        reset_holder = CastFrameChangeHolder()\n",
        "        # Activate the hurtbox change.\n",
        "        reset_holder.hurtbox_position_change.active = True\n",
        "\n",
        "        hpc = reset_holder.hurtbox_position_change\n",
        "        # Get the hurtbox offset from the utility.\n",
        "        hurtbox_offset = Capsule.get_hitbox_offset(hpc.xOffset, hpc.yOffset)\n",
        "        # Multiply the x component by the agent's facing direction.\n",
        "        hurtbox_offset = (hurtbox_offset[0] * int(self.facing), hurtbox_offset[1])\n",
        "        # Apply to the hurtbox collider.\n",
        "        self.hurtbox_collider.offset = hurtbox_offset\n",
        "        size = Capsule.get_hitbox_size(hpc.width, hpc.height)\n",
        "        self.hurtbox_collider.size = (2.0 * size[0], 2.0 * size[1])\n",
        "\n",
        "    # --- Second version: with changes, floor drag, and move manager ---\n",
        "    def do_cast_frame_changes_with_changes(self, changes, enable_floor_drag, mm):\n",
        "        # If floor drag is enabled, smooth-damp the x velocity toward 0.\n",
        "        if enable_floor_drag:\n",
        "            vel_x = self.move_toward(self.body.velocity.x, 0, self.in_air_ease)\n",
        "            self.body.velocity = pymunk.Vec2d(vel_x, self.body.velocity.y)\n",
        "\n",
        "        if changes is None:\n",
        "            return\n",
        "\n",
        "        # Process hurtbox position change.\n",
        "        hpc = changes.hurtbox_position_change\n",
        "        if hpc is not None and hpc.active:\n",
        "            hurtbox_offset = Capsule.get_hitbox_offset(hpc.xOffset, hpc.yOffset)\n",
        "            hurtbox_offset = (hurtbox_offset[0] * int(mm.move_facing_direction), hurtbox_offset[1])\n",
        "            # Set collider direction based on dimensions.\n",
        "\n",
        "            self.hurtbox_collider.offset = hurtbox_offset\n",
        "            size = Capsule.get_hitbox_size(hpc.width, hpc.height)\n",
        "            self.hurtbox_collider.size = (2.0 * size[0], 2.0 * size[1])\n",
        "\n",
        "        # Process caster position change (if any; currently no action).\n",
        "        cpc = changes.caster_position_change\n",
        "        if cpc is not None and cpc.active:\n",
        "            # Implement caster position change if needed.\n",
        "            pass\n",
        "\n",
        "        # Process dealt position target changes.\n",
        "        # (The original code has a commented-out block; here we check if the current power has a target.)\n",
        "        if hasattr(self.state, 'move_manager') and self.state.move_manager.current_power.dealt_position_target_exists:\n",
        "            mm = self.state.move_manager\n",
        "\n",
        "            target_pos = Capsule.get_hitbox_offset(mm.current_power.current_dealt_position_target[0],\n",
        "                                                               mm.current_power.current_dealt_position_target[1])\n",
        "            target_pos = (target_pos[0] * int(mm.move_facing_direction), target_pos[1])\n",
        "            # Assume self.position is available as self.position.\n",
        "            current_pos = self.body.position  # (x, y, z)\n",
        "            if mm.current_power.power_data.get(\"targetAllHitAgents\", False):\n",
        "                for agent in mm.all_hit_agents:\n",
        "                    # Compute a new velocity vector.\n",
        "                    vel = tuple(0.5 * ((current_pos[i] + target_pos[i] - agent.body.position[i])) for i in range(2))\n",
        "                    agent.set_position_target_vel(vel)\n",
        "            elif mm.hit_agent is not None:\n",
        "                vel = tuple(0.5 * ((current_pos[i] + target_pos[i] - mm.hit_agent.body.position[i])) for i in range(2))\n",
        "                mm.hit_agent.set_position_target_vel(vel)\n",
        "\n",
        "        # Process caster velocity set.\n",
        "        cvs = changes.caster_velocity_set\n",
        "        if cvs is not None and cvs.active:\n",
        "            angle_rad = math.radians(cvs.directionDeg)\n",
        "            vel = (math.cos(angle_rad) * cvs.magnitude, -math.sin(angle_rad) * cvs.magnitude)\n",
        "            vel = (vel[0] * int(mm.move_facing_direction), vel[1])\n",
        "            self.body.velocity = pymunk.Vec2d(vel[0], vel[1])\n",
        "\n",
        "        # Process caster velocity set XY.\n",
        "        cvsxy = changes.caster_velocity_set_xy\n",
        "        if cvsxy is not None:\n",
        "            vx, vy = self.body.velocity\n",
        "            if getattr(cvsxy, 'activeX', False):\n",
        "                vx = cvsxy.magnitudeX * int(mm.move_facing_direction)\n",
        "            if getattr(cvsxy, 'activeY', False):\n",
        "                vy = cvsxy.magnitudeY\n",
        "            self.body.velocity = pymunk.Vec2d(vx, vy)\n",
        "\n",
        "        # Process caster velocity damp XY.\n",
        "        cvdxy = changes.caster_velocity_damp_xy\n",
        "        if cvdxy is not None:\n",
        "            vx, vy = self.body.velocity\n",
        "            if getattr(cvdxy, 'activeX', False):\n",
        "                vx *= cvdxy.dampX\n",
        "            if getattr(cvdxy, 'activeY', False):\n",
        "                vy *= cvdxy.dampY\n",
        "            self.body.velocity = pymunk.Vec2d(vx, vy)\n",
        "\n",
        "    def get_move(self) -> MoveType:\n",
        "        # Assuming that 'p' is a Player instance and that p.input is an instance of PlayerInputHandler.\n",
        "        # Also assume that p.input.update(action) has already been called.\n",
        "\n",
        "        # Determine move types:\n",
        "        heavy_move = self.input.key_status['k'].held         # heavy move if key 'k' is held\n",
        "        light_move = (not heavy_move) and self.input.key_status['j'].held  # light move if not heavy and key 'j' is held\n",
        "        throw_move = (not heavy_move) and (not light_move) and self.input.key_status['h'].held  # throw if pickup key 'h' is held\n",
        "\n",
        "        # Determine directional keys:\n",
        "        left_key = self.input.key_status[\"A\"].held            # left key (A)\n",
        "        right_key = self.input.key_status[\"D\"].held           # right key (D)\n",
        "        up_key = self.input.key_status[\"W\"].held              # aim up (W)\n",
        "        down_key = self.input.key_status[\"S\"].held            # aim down (S)\n",
        "\n",
        "        # Calculate combined directions:\n",
        "        side_key = left_key or right_key\n",
        "\n",
        "        # Calculate move direction:\n",
        "        neutral_move = ((not side_key) and (not down_key)) or up_key\n",
        "        down_move = (not neutral_move) and down_key\n",
        "        side_move = (not neutral_move) and (not down_key) and side_key\n",
        "\n",
        "        # Check if any move key (light, heavy, or throw) is pressed:\n",
        "        hitting_any_move_key = light_move or heavy_move or throw_move\n",
        "        if not hitting_any_move_key:\n",
        "            move_type = MoveType.NONE\n",
        "        else:\n",
        "            # (Optional) Print the results:\n",
        "            # print(\"heavy_move:\", heavy_move)\n",
        "            # print(\"light_move:\", light_move)\n",
        "            # print(\"throw_move:\", throw_move)\n",
        "            # print(\"neutral_move:\", neutral_move)\n",
        "            # print(\"down_move:\", down_move)\n",
        "            # print(\"side_move:\", side_move)\n",
        "            # print(\"hitting_any_move_key:\", hitting_any_move_key)\n",
        "            cms = CompactMoveState(self.is_on_floor(), heavy_move, 0 if neutral_move else (1 if down_move else 2))\n",
        "            move_type = m_state_to_move[cms]\n",
        "            #print(move_type)\n",
        "        return move_type\n",
        "\n",
        "    def process(self, action: np.ndarray) -> None:\n",
        "        self.damage_taken_this_frame = 0\n",
        "        if not hasattr(self, 'opponent'):\n",
        "            self.opponent = self.env.players[1-self.agent_id]\n",
        "        #if self.env.steps == 2: self.animation_sprite_2d.play('altroll')\n",
        "        # Process inputs\n",
        "        self.input.update(action)\n",
        "        #self.direction = [action[0] - action[1], action[2] - action[3]]\n",
        "\n",
        "        # Reward: TO DELETE\n",
        "        multiple = 1 if self.body.position.x < 0 else -1\n",
        "        self.env.add_reward(self.agent_id, multiple * (self.body.position.x - self.prev_x))\n",
        "\n",
        "    def physics_process(self, delta: float) -> None:\n",
        "        new_state: PlayerObjectState = self.state.physics_process(delta)\n",
        "        self.hurtbox_collider.center = self.body.position\n",
        "        self.body.velocity = (self.body.velocity.x + self.damage_velocity[0] + self.target_vel[0],\n",
        "                              self.body.velocity.y + self.damage_velocity[1] + self.target_vel[1])\n",
        "\n",
        "\n",
        "        if new_state is not None:\n",
        "            new_state.reset(self.state)\n",
        "            self.state.exit()\n",
        "            self.state_str = f'{type(self.state).__name__} -> {type(new_state).__name__}'\n",
        "\n",
        "            #print()\n",
        "            self.state = new_state\n",
        "            self.state.enter()\n",
        "        log = {\n",
        "            'transition': self.state_str\n",
        "        }\n",
        "\n",
        "        if hasattr(self.state, 'move_type'):\n",
        "            log['move_type'] = self.state.move_type\n",
        "        self.env.logger[self.agent_id] = log\n",
        "\n",
        "        #self.body.velocity = pymunk.Vec2d(self.direction[0] * self.move_speed, self.body.velocity.y)\n",
        "        #self.body.velocity = pymunk.Vec2d(self.direction[0] * self.move_speed, self.direction[1] * self.move_speed)\n",
        "\n",
        "        self.prev_x = self.body.position.x\n",
        "        self.prev_y = self.body.position.y\n",
        "        self.damage_velocity = (0, 0)\n",
        "        self.target_vel = (0, 0)\n",
        "\n",
        "    def set_position_target_vel(self, vel: Tuple[float, float]) -> None:\n",
        "        self.target_vel = vel\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def move_toward(current: float, target: float, delta: float) -> float:\n",
        "        \"\"\"\n",
        "        Moves 'current' toward 'target' by 'delta' amount, but will not overshoot 'target'.\n",
        "        If delta is negative, it moves away from 'target'.\n",
        "\n",
        "        Examples:\n",
        "        move_toward(5, 10, 4)    -> 9\n",
        "        move_toward(10, 5, 4)    -> 6\n",
        "        move_toward(5, 10, 9)    -> 10\n",
        "        move_toward(10, 5, -1.5) -> 11.5\n",
        "        \"\"\"\n",
        "        # If current already equals target, return target immediately.\n",
        "        if current == target:\n",
        "            return target\n",
        "\n",
        "        # Calculate the difference and determine the movement direction.\n",
        "        diff = target - current\n",
        "        direction = diff / abs(diff)  # +1 if target > current, -1 if target < current\n",
        "\n",
        "        if delta >= 0:\n",
        "            # Move toward target: add (delta * direction)\n",
        "            candidate = current + delta * direction\n",
        "            # Clamp so we do not overshoot target.\n",
        "            if direction > 0:\n",
        "                return min(candidate, target)\n",
        "            else:\n",
        "                return max(candidate, target)\n",
        "        else:\n",
        "            # Move away from target: subtract (|delta| * direction)\n",
        "            # (This reverses the movement direction relative to the vector toward target.)\n",
        "            return current - abs(delta) * direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYrlmB7ymkBY"
      },
      "source": [
        "### Hitbox and Hurtbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "fJBceI7V8fJO"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "import math\n",
        "\n",
        "class Capsule():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def drawArc(surface, center, r, th, start, stop, color):\n",
        "        x, y = center\n",
        "        points_outer = []\n",
        "        points_inner = []\n",
        "        n = round(r*abs(stop-start))\n",
        "        if n<2:\n",
        "            n = 2\n",
        "        if n>30: n = 30\n",
        "        for i in range(n):\n",
        "            delta = i/(n-1)\n",
        "            phi0 = start + (stop-start)*delta\n",
        "            x0 = round(x+r*math.cos(phi0))\n",
        "            y0 = round(y+r*math.sin(phi0))\n",
        "            points_outer.append([x0,y0])\n",
        "            phi1 = stop + (start-stop)*delta\n",
        "            x1 = round(x+(r-th)*math.cos(phi1))\n",
        "            y1 = round(y+(r-th)*math.sin(phi1))\n",
        "            points_inner.append([x1,y1])\n",
        "        points = points_outer + points_inner\n",
        "        pygame.gfxdraw.aapolygon(surface, points, color)\n",
        "        pygame.gfxdraw.filled_polygon(surface, points, color)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_hitbox_offset(x_offset, y_offset):\n",
        "        \"\"\"\n",
        "        Converts offset values into world coordinates.\n",
        "        \"\"\"\n",
        "        return (x_offset * 2 * WarehouseBrawl.BRAWL_TO_UNITS,\n",
        "                y_offset * 2 * WarehouseBrawl.BRAWL_TO_UNITS)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_hitbox_size(width, height):\n",
        "        \"\"\"\n",
        "        Converts hitbox width and height into world coordinates.\n",
        "        \"\"\"\n",
        "        return (width * 2 * WarehouseBrawl.BRAWL_TO_UNITS,\n",
        "                height * 2 * WarehouseBrawl.BRAWL_TO_UNITS)\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_hitbox(camera: Camera, hitbox: np.ndarray, pos):\n",
        "        \"\"\"\n",
        "        Draws a rounded rectangle (capsule) on the screen using PyGame.\n",
        "        \"\"\"\n",
        "        Capsule.draw_hithurtbox(camera, hitbox, pos, color=(255, 0, 0))\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_hurtbox(camera: Camera, hitbox: np.ndarray, pos):\n",
        "        \"\"\"\n",
        "        Draws a rounded rectangle (capsule) on the screen using PyGame.\n",
        "        \"\"\"\n",
        "        Capsule.draw_hithurtbox(camera, hitbox, pos, color=(247, 215, 5))\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_hithurtbox(camera: Camera, hitbox: np.ndarray, pos: bool, color=(255, 0, 0)):\n",
        "        \"\"\"\n",
        "        Draws a rounded rectangle (capsule) on the screen using PyGame.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get canvas\n",
        "        canvas = camera.canvas\n",
        "\n",
        "        # Hitbox: [x_offset, y_offset, width, height]\n",
        "        x_offset, y_offset, width, height = hitbox\n",
        "\n",
        "        # Convert from brawl units to game units\n",
        "        size = Capsule.get_hitbox_size(width, height)\n",
        "        x_offset, y_offset = Capsule.get_hitbox_offset(x_offset, y_offset)\n",
        "\n",
        "        # Combine offset and position\n",
        "        pos = np.array(pos) + np.array([x_offset, y_offset])\n",
        "\n",
        "        # Convert to pixels using camera intrinsics\n",
        "        scale_cst = camera.scale_gtp()\n",
        "        size = (size[0] * scale_cst, size[1] * scale_cst)\n",
        "        pos = camera.gtp(pos)\n",
        "\n",
        "        rect = pygame.Rect(pos[0] - size[0] // 2,\n",
        "                           pos[1] - size[1] // 2,\n",
        "                           size[0], size[1])\n",
        "\n",
        "        if width < height:\n",
        "            # Vertical Capsule\n",
        "            radius = size[0] // 2\n",
        "            half_height = size[1] // 2\n",
        "            circle_height = half_height - radius\n",
        "\n",
        "            Capsule.drawArc(canvas, (pos[0], pos[1] - circle_height), radius, 2, math.pi, 2 * math.pi, color)\n",
        "            Capsule.drawArc(canvas, (pos[0], pos[1] + circle_height), radius, 2, 0, math.pi, color)\n",
        "            pygame.draw.line(canvas, color, (rect.left, rect.top + radius), (rect.left, rect.bottom - radius), 2)\n",
        "            pygame.draw.line(canvas, color, (rect.right-2, rect.top + radius), (rect.right-2, rect.bottom - radius), 2)\n",
        "\n",
        "        elif width == height:\n",
        "            # Circular Capsule\n",
        "            pygame.draw.circle(canvas, color, (rect.centerx, rect.centery), size[0] // 2, 2)\n",
        "\n",
        "        else:\n",
        "            # Horizontal Capsule\n",
        "            radius = size[1] // 2\n",
        "            half_width = size[0] // 2\n",
        "            circle_width = half_width - radius\n",
        "\n",
        "            Capsule.drawArc(canvas, (pos[0] + circle_width, pos[1]), radius, 2, 1.5 * math.pi, 2.5 * math.pi, color)\n",
        "            Capsule.drawArc(canvas, (pos[0] - circle_width, pos[1]), radius, 2, 0.5 * math.pi, 1.5 * math.pi, color)\n",
        "            pygame.draw.line(canvas, color, (rect.left + radius, rect.top), (rect.right - radius, rect.top), 2)\n",
        "            pygame.draw.line(canvas, color, (rect.left + radius, rect.bottom-2), (rect.right - radius, rect.bottom-2), 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def check_collision(hitbox_pos, width, height, collidables):\n",
        "        \"\"\"\n",
        "        Checks for collision between the hitbox and a list of collidable objects.\n",
        "\n",
        "        :param hitbox_pos: (x, y) position of the hitbox center.\n",
        "        :param width: Width of the hitbox.\n",
        "        :param height: Height of the hitbox.\n",
        "        :param collidables: A list of PyGame Rect objects representing collidable objects.\n",
        "        :return: List of colliding objects.\n",
        "        \"\"\"\n",
        "        size = Capsule.get_hitbox_size(width, height)\n",
        "        hitbox_rect = pygame.Rect(hitbox_pos[0] - size[0] // 2,\n",
        "                                  hitbox_pos[1] - size[1] // 2,\n",
        "                                  size[0], size[1])\n",
        "\n",
        "        collisions = [obj for obj in collidables if hitbox_rect.colliderect(obj)]\n",
        "        return collisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "4-HHm2qHwDm0"
      },
      "outputs": [],
      "source": [
        "class CapsuleCollider():\n",
        "    def __init__(self, center, width, height, is_hurtbox=False):\n",
        "        \"\"\"\n",
        "        :param center: (x, y) position of the capsule's center.\n",
        "        :param width: Width of the capsule.\n",
        "        :param height: Height of the capsule.\n",
        "        \"\"\"\n",
        "        self.center = pygame.Vector2(center)\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.radius = min(width, height) / 2  # Radius of cap circles\n",
        "        self.is_circle = width == height  # If it's a perfect circle\n",
        "\n",
        "    def draw(self, camera) -> None:\n",
        "        # use Capsule to draw this\n",
        "        Capsule.draw_hitbox(camera, [0, 0, self.width, self.height], self.center, facing_right=True)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CapsuleCollider(center={self.center}, width={self.width}, height={self.height})\"\n",
        "\n",
        "    def update(self):\n",
        "        # Define the main body rectangle\n",
        "        center, width, height = self.center, self.width, self.height\n",
        "        if not self.is_circle:\n",
        "            if width < height:\n",
        "                self.rect = pygame.Rect(center[0] - width / 2, center[1] - (height / 2 - self.radius),\n",
        "                                        width, height - 2 * self.radius)\n",
        "                self.cap1 = pygame.Vector2(center[0], center[1] - (height / 2 - self.radius))  # Top circle\n",
        "                self.cap2 = pygame.Vector2(center[0], center[1] + (height / 2 - self.radius))  # Bottom circle\n",
        "            else:\n",
        "                self.rect = pygame.Rect(center[0] - (width / 2 - self.radius), center[1] - height / 2,\n",
        "                                        width - 2 * self.radius, height)\n",
        "                self.cap1 = pygame.Vector2(center[0] - (width / 2 - self.radius), center[1])  # Left circle\n",
        "                self.cap2 = pygame.Vector2(center[0] + (width / 2 - self.radius), center[1])  # Right circle\n",
        "        else:\n",
        "            self.rect = None\n",
        "            self.cap1 = self.center  # Single circle\n",
        "\n",
        "    def intersects(self, other):\n",
        "        \"\"\"\n",
        "        Checks if this capsule collider intersects with another.\n",
        "\n",
        "        :param other: Another CapsuleCollider object.\n",
        "        :return: True if colliding, False otherwise.\n",
        "        \"\"\"\n",
        "        self.update()\n",
        "        other.update()\n",
        "\n",
        "\n",
        "        # Case 1: If both are circles (width == height)\n",
        "        if self.is_circle and other.is_circle:\n",
        "            collided = self._circle_circle_collision(self.cap1, self.radius, other.cap1, other.radius)\n",
        "\n",
        "        # Case 2: If this is a circle but the other is a capsule\n",
        "        elif self.is_circle:\n",
        "            collided = (self._circle_circle_collision(self.cap1, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap1, self.radius, other.cap2, other.radius) or\n",
        "                    self._circle_rectangle_collision(self.cap1, self.radius, other.rect))\n",
        "\n",
        "        # Case 3: If the other is a circle but this is a capsule\n",
        "        elif other.is_circle:\n",
        "            collided = (self._circle_circle_collision(self.cap1, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap2, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_rectangle_collision(other.cap1, other.radius, self.rect))\n",
        "\n",
        "        # Case 4: Both are capsules\n",
        "        else:\n",
        "            collided = (self._circle_circle_collision(self.cap1, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap1, self.radius, other.cap2, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap2, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap2, self.radius, other.cap2, other.radius) or\n",
        "                    self._rectangle_rectangle_collision(self.rect, other.rect) or\n",
        "                    self._circle_rectangle_collision(self.cap1, self.radius, other.rect) or\n",
        "                    self._circle_rectangle_collision(self.cap2, self.radius, other.rect) or\n",
        "                    self._circle_rectangle_collision(other.cap1, other.radius, self.rect) or\n",
        "                    self._circle_rectangle_collision(other.cap2, other.radius, self.rect))\n",
        "        #if collided:\n",
        "            #print(self, other)\n",
        "        return collided\n",
        "\n",
        "    def _circle_circle_collision(self, center1, radius1, center2, radius2):\n",
        "        \"\"\"Check if two circles intersect.\"\"\"\n",
        "        return center1.distance_to(center2) < (radius1 + radius2)\n",
        "\n",
        "    def _rectangle_rectangle_collision(self, rect1, rect2):\n",
        "        \"\"\"Check if two rectangles overlap.\"\"\"\n",
        "        return rect1.colliderect(rect2)\n",
        "\n",
        "    def _circle_rectangle_collision(self, circle_center, circle_radius, rect):\n",
        "        \"\"\"Check if a circle and a rectangle overlap.\"\"\"\n",
        "        if rect is None:\n",
        "            return False  # If one of them is a pure circle, no need to check rectangle\n",
        "\n",
        "        # Find the closest point on the rectangle to the circle center\n",
        "        closest_x = max(rect.left, min(circle_center.x, rect.right))\n",
        "        closest_y = max(rect.top, min(circle_center.y, rect.bottom))\n",
        "\n",
        "        # Calculate the distance from this closest point to the circle center\n",
        "        return circle_center.distance_to(pygame.Vector2(closest_x, closest_y)) < circle_radius"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGcNyfuaZWZL"
      },
      "source": [
        "### Animation Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "ZaH1TRFxmmY7"
      },
      "outputs": [],
      "source": [
        "class Particle(GameObject):\n",
        "    ENV_FPS = 30  # Environment FPS\n",
        "\n",
        "    def __init__(self, env, position, gif_path: str, scale: float = 1.0):\n",
        "        \"\"\"\n",
        "        A temporary particle that plays an animation once and deletes itself.\n",
        "\n",
        "        - `position`: The world position where the animation should be played.\n",
        "        - `gif_path`: Path to the GIF animation.\n",
        "        - `scale`: Scale factor for resizing frames.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.position = position\n",
        "        self.finished = False\n",
        "        self.scale = scale\n",
        "        self.current_frame_index = 0\n",
        "        self.frame_timer = 0\n",
        "\n",
        "        # Load GIF and extract frames\n",
        "        gif = Image.open(gif_path)\n",
        "        self.frames = []\n",
        "        self.frame_durations = []  # Store frame durations in milliseconds\n",
        "        total_duration = 0\n",
        "\n",
        "        for frame in ImageSequence.Iterator(gif):\n",
        "            # Convert and scale frame\n",
        "            pygame_frame = pygame.image.fromstring(frame.convert(\"RGBA\").tobytes(), frame.size, \"RGBA\")\n",
        "            scaled_frame = pygame.transform.scale(pygame_frame, (int(frame.width * scale), int(frame.height * scale)))\n",
        "            self.frames.append(scaled_frame)\n",
        "\n",
        "            # Extract frame duration\n",
        "            duration = frame.info.get('duration', 100)  # Default 100ms if missing\n",
        "            self.frame_durations.append(duration)\n",
        "            total_duration += duration\n",
        "\n",
        "        # Compute how many game steps each GIF frame should last\n",
        "        self.frames_per_step = [max(1, round((duration / 1000) * self.ENV_FPS)) for duration in self.frame_durations]\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"\n",
        "        Advances the animation, ensuring it syncs properly with a 30 FPS game loop.\n",
        "        \"\"\"\n",
        "        self.position = self.env.objects['opponent'].body.position\n",
        "        if not self.finished:\n",
        "            self.frame_timer += 1  # Increment frame timer (game steps)\n",
        "\n",
        "            # Move to the next frame only when enough game steps have passed\n",
        "            if self.frame_timer >= self.frames_per_step[self.current_frame_index]:\n",
        "                self.frame_timer = 0\n",
        "                self.current_frame_index += 1\n",
        "                if self.current_frame_index >= len(self.frames):\n",
        "                    self.current_frame_index = 0\n",
        "                    #self.finished = True  # Mark for deletion\n",
        "\n",
        "    def render(self, canvas: pygame.Surface, camera: Camera) -> None:\n",
        "        \"\"\"\n",
        "        Draws the current animation frame on the screen at a fixed position.\n",
        "        \"\"\"\n",
        "\n",
        "        # Define collidable objects (e.g., players)\n",
        "        player_rect = pygame.Rect(300, 400, 50, 50)  # A player hitbox\n",
        "        collidables = [player_rect]\n",
        "\n",
        "        # Define a hitbox\n",
        "        hitbox_pos = (0, 3)\n",
        "        hitbox_pos = self.position\n",
        "        hitbox = np.array([0, 0, 32, 480])\n",
        "\n",
        "        # Draw the hitbox\n",
        "        #Capsule.draw_hitbox(camera, hitbox, hitbox_pos)\n",
        "\n",
        "        # Check for collisions\n",
        "        #colliding_objects = BrawlHitboxUtility.check_collision(hitbox_pos, hitbox_width, hitbox_height, collidables)\n",
        "        #if colliding_objects:\n",
        "        #    print(\"Collision detected!\")\n",
        "\n",
        "        if not self.finished:\n",
        "            screen_pos = camera.gtp(self.position)\n",
        "            screen_pos = (0,0)\n",
        "            #canvas.blit(self.frames[self.current_frame_index], screen_pos)\n",
        "            self.draw_image(canvas, self.frames[self.current_frame_index], self.position, 2, camera)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2khY6i4Iq297"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlEWjV-ere9z"
      },
      "source": [
        "### Agent Abstract Base Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "Q4c8fEMfreUF"
      },
      "outputs": [],
      "source": [
        "SelfAgent = TypeVar(\"SelfAgent\", bound=\"Agent\")\n",
        "\n",
        "class Agent(ABC):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            file_path: Optional[str] = None\n",
        "        ):\n",
        "\n",
        "        # If no supplied file_path, load from gdown (optional file_path returned)\n",
        "        if file_path is None:\n",
        "            file_path = self._gdown()\n",
        "\n",
        "        self.file_path: Optional[str] = file_path\n",
        "        self.initialized = False\n",
        "\n",
        "    def get_env_info(self, env):\n",
        "        if isinstance(env, Monitor):\n",
        "            self_env = env.env\n",
        "        else:\n",
        "            self_env = env\n",
        "        self.observation_space = self_env.observation_space\n",
        "        self.obs_helper = self_env.obs_helper\n",
        "        self.action_space = self_env.action_space\n",
        "        self.act_helper = self_env.act_helper\n",
        "        self.env = env\n",
        "        self._initialize()\n",
        "        self.initialized = True\n",
        "\n",
        "    def get_num_timesteps(self) -> int:\n",
        "        if hasattr(self, 'model'):\n",
        "            return self.model.num_timesteps\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def update_num_timesteps(self, num_timesteps: int) -> None:\n",
        "        if hasattr(self, 'model'):\n",
        "            self.model.num_timesteps = num_timesteps\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, obs) -> spaces.Space:\n",
        "        pass\n",
        "\n",
        "    def save(self, file_path: str) -> None:\n",
        "        return\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        return\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def _gdown(self) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Loads the necessary file from Google Drive, returning a file path.\n",
        "        Or, returns None, if the agent does not require loaded files.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83MTxzRvh58N"
      },
      "source": [
        "### Agent Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "2XLXjSF-h4EQ"
      },
      "outputs": [],
      "source": [
        "class ConstantAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = np.zeros_like(self.action_space.sample())\n",
        "        return action\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = self.action_space.sample()\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpulsG6R6eGG"
      },
      "source": [
        "## StableBaselines3 Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRa5uPgibFAm"
      },
      "source": [
        "### Reward Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "id": "ysWnnzGza-P7"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RewTerm():\n",
        "    \"\"\"Configuration for a reward term.\"\"\"\n",
        "\n",
        "    func: Callable[..., torch.Tensor] = MISSING\n",
        "    \"\"\"The name of the function to be called.\n",
        "\n",
        "    This function should take the environment object and any other parameters\n",
        "    as input and return the reward signals as torch float tensors of\n",
        "    shape (num_envs,).\n",
        "    \"\"\"\n",
        "\n",
        "    weight: float = MISSING\n",
        "    \"\"\"The weight of the reward term.\n",
        "\n",
        "    This is multiplied with the reward term's value to compute the final\n",
        "    reward.\n",
        "\n",
        "    Note:\n",
        "        If the weight is zero, the reward term is ignored.\n",
        "    \"\"\"\n",
        "\n",
        "    params: dict[str, Any] = field(default_factory=dict)\n",
        "    \"\"\"The parameters to be passed to the function as keyword arguments. Defaults to an empty dict.\n",
        "\n",
        "    .. note::\n",
        "        If the value is a :class:`SceneEntityCfg` object, the manager will query the scene entity\n",
        "        from the :class:`InteractiveScene` and process the entity's joints and bodies as specified\n",
        "        in the :class:`SceneEntityCfg` object.\n",
        "    \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "w35uogzHqVVL"
      },
      "outputs": [],
      "source": [
        "class RewardManager():\n",
        "    \"\"\"Reward terms for the MDP.\"\"\"\n",
        "\n",
        "    # (1) Constant running reward\n",
        "    def __init__(self,\n",
        "                 reward_functions: Optional[Dict[str, RewTerm]]=None,\n",
        "                 signal_subscriptions: Optional[Dict[str, Tuple[str, RewTerm]]]=None) -> None:\n",
        "        self.reward_functions = reward_functions\n",
        "        self.signal_subscriptions = signal_subscriptions\n",
        "        self.total_reward = 0.0\n",
        "        self.collected_signal_rewards = 0.0\n",
        "\n",
        "    def subscribe_signals(self, env) -> None:\n",
        "        if self.signal_subscriptions is None:\n",
        "            return\n",
        "        for _, (name, term_cfg) in self.signal_subscriptions.items():\n",
        "            getattr(env, name).connect(partial(self._signal_func, term_cfg))\n",
        "\n",
        "    def _signal_func(self, term_cfg: RewTerm, *args, **kwargs):\n",
        "        term_partial = partial(term_cfg.func, **term_cfg.params)\n",
        "        self.collected_signal_rewards += term_partial(*args, **kwargs) * term_cfg.weight\n",
        "\n",
        "\n",
        "    def process(self, env, dt) -> float:\n",
        "        # reset computation\n",
        "        reward_buffer = 0.0\n",
        "        # iterate over all the reward terms\n",
        "        if self.reward_functions is not None:\n",
        "            for name, term_cfg in self.reward_functions.items():\n",
        "                # skip if weight is zero (kind of a micro-optimization)\n",
        "                if term_cfg.weight == 0.0:\n",
        "                    continue\n",
        "                # compute term's value\n",
        "                value = term_cfg.func(env, **term_cfg.params) * term_cfg.weight\n",
        "                # update total reward\n",
        "                reward_buffer += value\n",
        "\n",
        "        reward = reward_buffer + self.collected_signal_rewards\n",
        "        self.collected_signal_rewards = 0.0\n",
        "\n",
        "        self.total_reward += reward\n",
        "\n",
        "        log = env.logger[0]\n",
        "        log['reward'] = f'{reward_buffer:.3f}'\n",
        "        log['total_reward'] = f'{self.total_reward:.3f}'\n",
        "        env.logger[0] = log\n",
        "        return reward\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_reward = 0\n",
        "        self.collected_signal_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxD2EcJGrjvj"
      },
      "source": [
        "### Save, Self-play, and Opponents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "5U4IT7f4rj8a"
      },
      "outputs": [],
      "source": [
        "class SaveHandlerMode(Enum):\n",
        "    FORCE = 0\n",
        "    RESUME = 1\n",
        "\n",
        "class SaveHandler():\n",
        "    \"\"\"Handles saving.\n",
        "\n",
        "    Args:\n",
        "        agent (Agent): Agent to save.\n",
        "        save_freq (int): Number of steps between saving.\n",
        "        max_saved (int): Maximum number of saved models.\n",
        "        save_dir (str): Directory to save models.\n",
        "        name_prefix (str): Prefix for saved models.\n",
        "    \"\"\"\n",
        "\n",
        "    # System for saving to internet\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            agent: Agent,\n",
        "            save_freq: int=10_000,\n",
        "            max_saved: int=20,\n",
        "            run_name: str='experiment_1',\n",
        "            save_path: str='checkpoints',\n",
        "            name_prefix: str = \"rl_model\",\n",
        "            mode: SaveHandlerMode=SaveHandlerMode.FORCE\n",
        "        ):\n",
        "        self.agent = agent\n",
        "        self.save_freq = save_freq\n",
        "        self.run_name = run_name\n",
        "        self.max_saved = max_saved\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "        self.mode = mode\n",
        "\n",
        "        self.steps_until_save = save_freq\n",
        "        # Get model paths from exp_path, if it exists\n",
        "        exp_path = self._experiment_path()\n",
        "        self.history: List[str] = []\n",
        "        if self.mode == SaveHandlerMode.FORCE:\n",
        "            # Clear old dir\n",
        "            if os.path.exists(exp_path) and len(os.listdir(exp_path)) != 0:\n",
        "                while True:\n",
        "                    answer = input(f\"Would you like to clear the folder {exp_path} (SaveHandlerMode.FORCE): yes (y) or no (n): \").strip().lower()\n",
        "                    if answer in ('y', 'n'):\n",
        "                        break\n",
        "                    else:\n",
        "                        print(\"Invalid input, please enter 'y' or 'n'.\")\n",
        "\n",
        "                if answer == 'n':\n",
        "                    raise ValueError('Please switch to SaveHandlerMode.FORCE or use a new run_name.')\n",
        "                print(f'Clearing {exp_path}...')\n",
        "                if os.path.exists(exp_path):\n",
        "                    shutil.rmtree(exp_path)\n",
        "            else:\n",
        "                print(f'{exp_path} empty or does not exist. Creating...')\n",
        "\n",
        "            if not os.path.exists(exp_path):\n",
        "                os.makedirs(exp_path)\n",
        "        elif self.mode == SaveHandlerMode.RESUME:\n",
        "            if os.path.exists(exp_path):\n",
        "                # Get all model paths\n",
        "                self.history = [os.path.join(exp_path, f) for f in os.listdir(exp_path) if os.path.isfile(os.path.join(exp_path, f))]\n",
        "                # Filter any non .csv\n",
        "                self.history = [f for f in self.history if f.endswith('.zip')]\n",
        "                if len(self.history) != 0:\n",
        "                    self.history.sort(key=lambda x: int(os.path.basename(x).split('_')[-2].split('.')[0]))\n",
        "                    self.history = self.history[-max_saved:]\n",
        "                    print(f'Best model is {self.history[-1]}')\n",
        "                else:\n",
        "                    print(f'No models found in {exp_path}.')\n",
        "                    raise FileNotFoundError\n",
        "            else:\n",
        "                print(f'No file found at {exp_path}')\n",
        "\n",
        "\n",
        "    def update_info(self) -> None:\n",
        "        self.num_timesteps = self.agent.get_num_timesteps()\n",
        "\n",
        "    def _experiment_path(self) -> str:\n",
        "        \"\"\"\n",
        "        Helper to get experiment path for each type of checkpoint.\n",
        "\n",
        "        :param extension: Checkpoint file extension (zip for model, pkl for others)\n",
        "        :return: Path to the checkpoint\n",
        "        \"\"\"\n",
        "        return os.path.join(self.save_path, self.run_name)\n",
        "\n",
        "    def _checkpoint_path(self, extension: str = '') -> str:\n",
        "        \"\"\"\n",
        "        Helper to get checkpoint path for each type of checkpoint.\n",
        "\n",
        "        :param extension: Checkpoint file extension (zip for model, pkl for others)\n",
        "        :return: Path to the checkpoint\n",
        "        \"\"\"\n",
        "        return os.path.join(self._experiment_path(), f\"{self.name_prefix}_{self.num_timesteps}_steps.{extension}\")\n",
        "\n",
        "    def save_agent(self) -> None:\n",
        "        print(f\"Saving agent to {self._checkpoint_path()}\")\n",
        "        model_path = self._checkpoint_path('zip')\n",
        "        self.agent.save(model_path)\n",
        "        self.history.append(model_path)\n",
        "        if len(self.history) > self.max_saved:\n",
        "            os.remove(self.history.pop(0))\n",
        "\n",
        "    def process(self) -> bool:\n",
        "        self.num_timesteps += 1\n",
        "\n",
        "        if self.steps_until_save <= 0:\n",
        "            # Save agent\n",
        "            self.steps_until_save = self.save_freq\n",
        "            self.save_agent()\n",
        "            return True\n",
        "        self.steps_until_save -= 1\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_latest_model_path(self) -> str:\n",
        "        if len(self.history) == 0:\n",
        "            return None\n",
        "        return self.history[-1]\n",
        "\n",
        "class SelfPlaySelectionMode(Enum):\n",
        "    LATEST = 0\n",
        "    ELO = 1\n",
        "\n",
        "class SelfPlayHandler():\n",
        "    \"\"\"Handles self-play.\"\"\"\n",
        "\n",
        "    def __init__(self, agent_partial: partial, mode: SelfPlaySelectionMode=SelfPlaySelectionMode.LATEST):\n",
        "        self.agent_partial = agent_partial\n",
        "        self.mode = mode\n",
        "\n",
        "    def get_opponent(self) -> Agent:\n",
        "        assert self.save_handler is not None, \"Save handler must be specified for self-play\"\n",
        "\n",
        "        if self.mode == SelfPlaySelectionMode.LATEST:\n",
        "            # Get the best model from the save handler\n",
        "            self.best_model = self.save_handler.get_latest_model_path()\n",
        "            if self.best_model:\n",
        "                try:\n",
        "                    opponent = self.agent_partial(file_path=self.best_model)\n",
        "                    opponent.get_env_info(self.env)\n",
        "                    return opponent\n",
        "                except FileNotFoundError:\n",
        "                    print(f\"Warning: Self-play file {self.best_model} not found. Defaulting to constant agent.\")\n",
        "                    opponent = ConstantAgent()\n",
        "                    opponent.get_env_info(self.env)\n",
        "            else:\n",
        "                print(\"Warning: No self-play model saved. Defaulting to constant agent.\")\n",
        "                opponent = ConstantAgent()\n",
        "                opponent.get_env_info(self.env)\n",
        "\n",
        "        elif self.mode == SelfPlaySelectionMode.ELO:\n",
        "            raise NotImplementedError\n",
        "\n",
        "        return opponent\n",
        "\n",
        "@dataclass\n",
        "class OpponentsCfg():\n",
        "    \"\"\"Configuration for opponents.\n",
        "\n",
        "    Args:\n",
        "        swap_steps (int): Number of steps between swapping opponents.\n",
        "        opponents (dict): Dictionary specifying available opponents and their selection probabilities.\n",
        "    \"\"\"\n",
        "    swap_steps: int = 10_000\n",
        "    opponents: dict[str, Any] = field(default_factory=lambda: {\n",
        "                'random_agent': (0.8, partial(RandomAgent)),\n",
        "                'constant_agent': (0.2, partial(ConstantAgent)),\n",
        "                #'recurrent_agent': (0.1, partial(RecurrentPPOAgent, file_path='skibidi')),\n",
        "            })\n",
        "\n",
        "    def validate_probabilities(self) -> None:\n",
        "        total_prob = sum(prob if isinstance(prob, float) else prob[0] for prob in self.opponents.values())\n",
        "\n",
        "        if abs(total_prob - 1.0) > 1e-5:\n",
        "            print(f\"Warning: Probabilities do not sum to 1 (current sum = {total_prob}). Normalizing...\")\n",
        "            self.opponents = {\n",
        "                key: (value / total_prob if isinstance(value, float) else (value[0] / total_prob, value[1]))\n",
        "                for key, value in self.opponents.items()\n",
        "            }\n",
        "\n",
        "    def process(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def on_env_reset(self) -> Agent:\n",
        "\n",
        "        agent_name = random.choices(\n",
        "            list(self.opponents.keys()),\n",
        "            weights=[prob if isinstance(prob, float) else prob[0] for prob in self.opponents.values()]\n",
        "        )[0]\n",
        "\n",
        "        # If self-play is selected, return the trained model\n",
        "        print(f'Selected {agent_name}')\n",
        "        if agent_name == \"self_play\":\n",
        "            selfplay_handler: SelfPlayHandler = self.opponents[agent_name][1]\n",
        "            return selfplay_handler.get_opponent()\n",
        "        else:\n",
        "            # Otherwise, return an instance of the selected agent class\n",
        "            opponent = self.opponents[agent_name][1]()\n",
        "\n",
        "        opponent.get_env_info(self.env)\n",
        "        return opponent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "404LO62Oqsqz"
      },
      "source": [
        "### Self-Play Warehouse Brawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "gr1VxOWM5pBp"
      },
      "outputs": [],
      "source": [
        "class SelfPlayWarehouseBrawl(gymnasium.Env):\n",
        "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self,\n",
        "                 reward_manager: Optional[RewardManager]=None,\n",
        "                 opponent_cfg: OpponentsCfg=OpponentsCfg(),\n",
        "                 save_handler: Optional[SaveHandler]=None,\n",
        "                 render_every: int | None = None,\n",
        "                 resolution: CameraResolution=CameraResolution.LOW):\n",
        "        \"\"\"\n",
        "        Initializes the environment.\n",
        "\n",
        "        Args:\n",
        "            reward_manager (Optional[RewardManager]): Reward manager.\n",
        "            opponent_cfg (OpponentCfg): Configuration for opponents.\n",
        "            save_handler (SaveHandler): Configuration for self-play.\n",
        "            render_every (int | None): Number of steps between a demo render (None if no rendering).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.reward_manager = reward_manager\n",
        "        self.save_handler = save_handler\n",
        "        self.opponent_cfg = opponent_cfg\n",
        "        self.render_every = render_every\n",
        "        self.resolution = resolution\n",
        "\n",
        "        self.games_done = 0\n",
        "\n",
        "\n",
        "        # Give OpponentCfg references, and normalize probabilities\n",
        "        self.opponent_cfg.env = self\n",
        "        self.opponent_cfg.validate_probabilities()\n",
        "\n",
        "        # Check if using self-play\n",
        "        if (selfplay_data := self.opponent_cfg.opponents.get('self_play')) is not None:\n",
        "            assert self.save_handler is not None, \"Save handler must be specified for self-play\"\n",
        "\n",
        "            # Give SelfPlayHandler references\n",
        "            selfplay_handler: SelfPlayHandler = selfplay_data[1]\n",
        "            selfplay_handler.save_handler = self.save_handler\n",
        "            selfplay_handler.env = self\n",
        "\n",
        "        self.best_model = None\n",
        "\n",
        "        self.raw_env = WarehouseBrawl(resolution=resolution)\n",
        "        self.action_space = self.raw_env.action_space\n",
        "        self.act_helper = self.raw_env.act_helper\n",
        "        self.observation_space = self.raw_env.observation_space\n",
        "        self.obs_helper = self.raw_env.obs_helper\n",
        "\n",
        "    def on_training_start(self):\n",
        "        # Update SaveHandler\n",
        "        if self.save_handler is not None:\n",
        "            self.save_handler.update_info()\n",
        "\n",
        "    def on_training_end(self):\n",
        "        if self.save_handler is not None:\n",
        "            self.save_handler.agent.update_num_timesteps(self.save_handler.num_timesteps)\n",
        "            self.save_handler.save_agent()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        full_action = {\n",
        "            0: action,\n",
        "            1: self.opponent_agent.predict(self.opponent_obs),\n",
        "        }\n",
        "\n",
        "        observations, rewards, terminated, truncated, info = self.raw_env.step(full_action)\n",
        "\n",
        "        if self.save_handler is not None:\n",
        "            self.save_handler.process()\n",
        "\n",
        "        if self.reward_manager is None:\n",
        "            reward = rewards[0]\n",
        "        else:\n",
        "            reward = self.reward_manager.process(self.raw_env, 1 / 30.0)\n",
        "\n",
        "        return observations[0], reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # Reset MalachiteEnv\n",
        "        observations, info = self.raw_env.reset()\n",
        "\n",
        "        self.reward_manager.reset()\n",
        "\n",
        "        # Select agent\n",
        "        new_agent: Agent = self.opponent_cfg.on_env_reset()\n",
        "        if new_agent is not None:\n",
        "            self.opponent_agent: Agent = new_agent\n",
        "        self.opponent_obs = observations[1]\n",
        "\n",
        "\n",
        "        self.games_done += 1\n",
        "        #if self.games_done % self.render_every == 0:\n",
        "            #self.render_out_video()\n",
        "\n",
        "        return observations[0], info\n",
        "\n",
        "    def render(self):\n",
        "        img = self.raw_env.render()\n",
        "        return img\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyCz9XRL0tLW"
      },
      "source": [
        "## Run Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ed2wOQ-S0zUv"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_match(agent_1: Agent | partial,\n",
        "              agent_2: Agent | partial,\n",
        "              max_timesteps=30*90,\n",
        "              video_path: Optional[str]=None,\n",
        "              agent_1_name: Optional[str]=None,\n",
        "              agent_2_name: Optional[str]=None,\n",
        "              resolution = CameraResolution.LOW,\n",
        "              reward_manager: Optional[RewardManager]=None\n",
        "              ) -> MatchStats:\n",
        "    # Initialize env\n",
        "    env = WarehouseBrawl(resolution=resolution)\n",
        "    observations, infos = env.reset()\n",
        "    obs_1 = observations[0]\n",
        "    obs_2 = observations[1]\n",
        "\n",
        "    if reward_manager is not None:\n",
        "        reward_manager.reset()\n",
        "        reward_manager.subscribe_signals(env)\n",
        "\n",
        "    if agent_1_name is None:\n",
        "        agent_1_name = 'agent_1'\n",
        "    if agent_2_name is None:\n",
        "        agent_2_name = 'agent_2'\n",
        "\n",
        "    env.agent_1_name = agent_1_name\n",
        "    env.agent_2_name = agent_2_name\n",
        "\n",
        "\n",
        "    writer = None\n",
        "    if video_path is None:\n",
        "        print(\"video_path=None -> Not rendering\")\n",
        "    else:\n",
        "        print(f\"video_path={video_path} -> Rendering\")\n",
        "        # Initialize video writer\n",
        "        writer = skvideo.io.FFmpegWriter(video_path, outputdict={\n",
        "            '-vcodec': 'libx264',  # Use H.264 for Windows Media Player\n",
        "            '-pix_fmt': 'yuv420p',  # Compatible with both WMP & Colab\n",
        "            '-preset': 'fast',  # Faster encoding\n",
        "            '-crf': '20',  # Quality-based encoding (lower = better quality)\n",
        "            '-r': '30'  # Frame rate\n",
        "        })\n",
        "\n",
        "    # If partial\n",
        "    if callable(agent_1):\n",
        "        agent_1 = agent_1()\n",
        "    if callable(agent_2):\n",
        "        agent_2 = agent_2()\n",
        "\n",
        "    # Initialize agents\n",
        "    if not agent_1.initialized: agent_1.get_env_info(env)\n",
        "    if not agent_2.initialized: agent_2.get_env_info(env)\n",
        "    # 596, 336\n",
        "\n",
        "    for _ in tqdm(range(max_timesteps), total=max_timesteps):\n",
        "        # actions = {agent: agents[agent].predict(None) for agent in range(2)}\n",
        "\n",
        "        # observations, rewards, terminations, truncations, infos\n",
        "\n",
        "        full_action = {\n",
        "            0: agent_1.predict(obs_1),\n",
        "            1: agent_2.predict(obs_2)\n",
        "        }\n",
        "\n",
        "        observations, rewards, terminated, truncated, info = env.step(full_action)\n",
        "        obs_1 = observations[0]\n",
        "        obs_2 = observations[1]\n",
        "\n",
        "        if reward_manager is not None:\n",
        "            reward_manager.process(env, 1 / env.fps)\n",
        "\n",
        "        if video_path is not None:\n",
        "            img = env.render()\n",
        "            writer.writeFrame(img)\n",
        "            del img\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "        #env.show_image(img)\n",
        "\n",
        "    if video_path is not None:\n",
        "        writer.close()\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "    # visualize\n",
        "    # Video(video_path, embed=True, width=800) if video_path is not None else None\n",
        "    player_1_stats = env.get_stats(0)\n",
        "    player_2_stats = env.get_stats(1)\n",
        "    match_stats = MatchStats(\n",
        "        match_time=env.steps / env.fps,\n",
        "        player1=player_1_stats,\n",
        "        player2=player_2_stats,\n",
        "        player1_result=Result.WIN if player_1_stats.lives_left > player_2_stats.lives_left else Result.LOSS\n",
        "    )\n",
        "\n",
        "    del env\n",
        "\n",
        "    return match_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdErHuFqNSFl"
      },
      "source": [
        "# SUBMISSION: Additional Imports\n",
        "Note that all the imports up to this point (for the Malachite Env, WarehouseBrawl, etc...) will be automatically included in the submission, so you need not write them.\n",
        "\n",
        "Requirements:\n",
        "- **DO NOT** import any modules beyond the following code block. They will not be parsed and may cause your submission to fail validation.\n",
        "- Only write imports that have not been used above this code block\n",
        "- Only write imports that are from libraries listed here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "K_Ln88TlNTxX"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC  # Sample RL Algo imports\n",
        "from sb3_contrib import RecurrentPPO  # Importing an LSTM\n",
        "from stable_baselines3.common.callbacks import CheckpointCallback\n",
        "import numpy as np\n",
        "from stable_baselines3.common.vec_env import SubprocVecEnv"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8HOfcO1p-TK"
      },
      "source": [
        "# SUBMISSION: Agent\n",
        "This will be the Agent class we run in the 1v1. We've started you off with a functioning RL agent (`SB3Agent(Agent)`) and if-statement agent (`BasedAgent(Agent)`). Feel free to copy either to `SubmittedAgent(Agent)` then begin modifying.\n",
        "\n",
        "Requirements:\n",
        "- Your submission **MUST** be of type `SubmittedAgent(Agent)`\n",
        "- Any instantiated classes **MUST** be defined within and below this code block.\n",
        "\n",
        "Remember, your agent can be either machine learning, OR if-statement based. I've seen many successful agents arising purely from if-statements - give them a shot as well, if ML is too complicated at first!!\n",
        "\n",
        "Also PLEASE ask us questions in the Discord server if any of the API is confusing. We'd be more than happy to clarify and get the team on the right track.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "X9wauAD_p93C"
      },
      "outputs": [],
      "source": [
        "# # We're using PPO by default, but feel free to experiment with other Stable-Baselines 3 algorithms!\n",
        "# class SubmittedAgent(Agent):\n",
        "\n",
        "#     def __init__(\n",
        "#             self,\n",
        "#             file_path: Optional[str] = None,\n",
        "#             # example_argument = 0,\n",
        "#     ):\n",
        "#         # Your code here\n",
        "#         super().__init__(file_path)\n",
        "\n",
        "#     def _initialize(self) -> None:\n",
        "#         if self.file_path is None:\n",
        "#             print('hii')\n",
        "#             self.model = PPO(\"MlpPolicy\", self.env, verbose=0)\n",
        "#             del self.env\n",
        "#         else:\n",
        "#             self.model = PPO.load(self.file_path)\n",
        "#             # self.model = A2C.load(self.file_path)\n",
        "#             # self.model = SAC.load(self.file_path)\n",
        "\n",
        "#     def _gdown(self) -> str:\n",
        "#         data_path = \"rl-model.zip\"\n",
        "#         if not os.path.isfile(data_path):\n",
        "#             print(f\"Downloading {data_path}...\")\n",
        "#             # Place a link to your PUBLIC model data here. This is where we will download it from on the tournament server.\n",
        "#             url = \"https://drive.google.com/file/d/1G60ilYtohdmXsYyjBtwdzC1PRBerqpfJ/view?usp=sharing\"\n",
        "#             gdown.download(url, output=data_path, fuzzy=True)\n",
        "#         return data_path\n",
        "\n",
        "#     def predict(self, obs):\n",
        "#         action, _ = self.model.predict(obs)\n",
        "#         return action\n",
        "\n",
        "#     def save(self, file_path: str) -> None:\n",
        "#         self.model.save(file_path)\n",
        "\n",
        "#     # If modifying the number of models (or training in general), modify this\n",
        "#     def learn(self, env, total_timesteps, log_interval: int = 4):\n",
        "#         self.model.set_env(env)\n",
        "#         self.model.learn(total_timesteps=total_timesteps, log_interval=log_interval)\n",
        "\n",
        "# # Create my agent\n",
        "# # Use SubmittedAgent(file_path='data.zip')\n",
        "# my_agent = SubmittedAgent()\n",
        "# my_agent2 = SubmittedAgent()\n",
        "# #my_agent = RecurrentPPOAgent('recurrent')\n",
        "# run_match(my_agent, my_agent2, video_path='vis.mp4', resolution=CameraResolution.LOW)\n",
        "# Video('vis.mp4', embed=True, width=800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SubmittedAgent(Agent):\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_path: Optional[str] = None,\n",
        "        learning_rate: float = 3e-4,\n",
        "        n_steps: int = 2048,\n",
        "        batch_size: int = 128,\n",
        "        n_epochs: int = 10,\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        ent_coef: float = 0.01,\n",
        "        clip_range: float = 0.2,\n",
        "        verbose: int = 1,\n",
        "        max_grad_norm: float = 0.5,\n",
        "        device: str = \"auto\",\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_steps = n_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.ent_coef = ent_coef\n",
        "        self.clip_range = clip_range\n",
        "        self.verbose = verbose\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.device = device\n",
        "        super().__init__(file_path)\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        if self.file_path is None:\n",
        "            # Deep MLP architecture optimized for fighting games\n",
        "            policy_kwargs = dict(\n",
        "                net_arch=dict(\n",
        "                    pi=[64, 128, 256, 128, 64],  # Deep policy network\n",
        "                    vf=[64, 128, 256, 128, 64],  # Deep value network\n",
        "                ),\n",
        "                activation_fn=torch.nn.ReLU,\n",
        "                ortho_init=True,\n",
        "            )\n",
        "\n",
        "            self.model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                self.env,\n",
        "                learning_rate=self.learning_rate,\n",
        "                n_steps=self.n_steps,\n",
        "                batch_size=self.batch_size,\n",
        "                n_epochs=self.n_epochs,\n",
        "                gamma=self.gamma,\n",
        "                gae_lambda=self.gae_lambda,\n",
        "                ent_coef=self.ent_coef,\n",
        "                clip_range=self.clip_range,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=self.verbose,\n",
        "                normalize_advantage=True,\n",
        "                max_grad_norm=self.max_grad_norm,\n",
        "                device=self.device,\n",
        "            )\n",
        "            del self.env\n",
        "        else:\n",
        "            self.model = PPO.load(\n",
        "                self.file_path, n_steps=30 * 90 * 3, batch_size=128, device=self.device\n",
        "            )\n",
        "\n",
        "    def predict(self, obs):\n",
        "        \"\"\"Get action with deterministic prediction\"\"\"\n",
        "        # Convert observation to tensor on correct device if needed\n",
        "        if isinstance(obs, np.ndarray):\n",
        "            obs = torch.from_numpy(obs).to(self.model.device)\n",
        "        action, _ = self.model.predict(obs, deterministic=True)\n",
        "        return action\n",
        "\n",
        "    def save(self, file_path: str) -> None:\n",
        "        \"\"\"Save model with timesteps\"\"\"\n",
        "        self.model.save(file_path, include=[\"num_timesteps\"])\n",
        "\n",
        "    def learn(self, env, total_timesteps, log_interval: int = 1, verbose: int = 1):\n",
        "        \"\"\"Train model with checkpoint callback\"\"\"\n",
        "        self.model.set_env(env)\n",
        "        self.model.verbose = verbose\n",
        "\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            log_interval=log_interval,\n",
        "        )\n",
        "\n",
        "    # def _gdown(self) -> str:\n",
        "    #     data_path = \"rl-model.zip\"\n",
        "    #     if not os.path.isfile(data_path):\n",
        "    #         print(f\"Downloading {data_path}...\")\n",
        "    #         url = \"https://drive.google.com/file/d/1mYJ07TYJ3Qo_wYLjYT6pQIlaRDepuAav/view?usp=sharing\"\n",
        "    #         gdown.download(url, output=data_path, fuzzy=True)\n",
        "    #     return data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfKVniFCp1ux"
      },
      "source": [
        "# Training\n",
        "\n",
        "Here, you can set the reward functions and train your agent. If you'd like to write a heuristic (if-statement) agent, you can also reference the Example Agents here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MMYCnbOrXs6"
      },
      "source": [
        "## Example Agent Classes\n",
        "Reference these to design a gamut of opponents for your model to face off against!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "QCST5qk0wnf8"
      },
      "outputs": [],
      "source": [
        "# Recall the possible observations\n",
        "# Set name='player', or name='opponent'\n",
        "# obs_helper.get_section(obs, f\"{name}_pos\") # low=[-1, -1], high=[1, 1]\n",
        "# obs_helper.get_section(obs, f\"{name}_facing\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_vel\") # low=[-1, -1], high=[1, 1]\n",
        "# obs_helper.get_section(obs, f\"{name}_grounded\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_aerial\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_jumps_left\") # low=[0], high=[2]\n",
        "# obs_helper.get_section(obs, f\"{name}_state\") # low=[0], high=[12]\n",
        "# obs_helper.get_section(obs, f\"{name}_recoveries_left\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_dodge_timer\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_stun_frames\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_damage\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_stocks\") # low=[0], high=[3]\n",
        "# obs_helper.get_section(obs, f\"{name}_move_type\") # low=[0], high=[11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "DBpCptggHEdK"
      },
      "outputs": [],
      "source": [
        "# player_state and opponent_state map to this\n",
        "# state_mapping = {\n",
        "#             'WalkingState': 0,\n",
        "#             'StandingState': 1,\n",
        "#             'TurnaroundState': 2,\n",
        "#             'AirTurnaroundState': 3,\n",
        "#             'SprintingState': 4,\n",
        "#             'StunState': 5,\n",
        "#             'InAirState': 6,\n",
        "#             'DodgeState': 7,\n",
        "#             'AttackState': 8,\n",
        "#             'DashState': 9,\n",
        "#             'BackDashState': 10,\n",
        "#             'KOState': 11,\n",
        "#             'TauntState': 12,\n",
        "#         }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "zNS45A00n82h"
      },
      "outputs": [],
      "source": [
        "class ConstantAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = np.zeros_like(self.action_space.sample())\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "xKEpMi3Qo81L"
      },
      "outputs": [],
      "source": [
        "class RandomAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = self.action_space.sample()\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "ivsxuwzXwfU5"
      },
      "outputs": [],
      "source": [
        "class BasedAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.time = 0\n",
        "\n",
        "    def predict(self, obs):\n",
        "        self.time += 1\n",
        "        pos = self.obs_helper.get_section(obs, 'player_pos')\n",
        "        opp_pos = self.obs_helper.get_section(obs, 'opponent_pos')\n",
        "        opp_KO = self.obs_helper.get_section(obs, 'opponent_state') in [5, 11]\n",
        "        action = self.act_helper.zeros()\n",
        "\n",
        "        # If off the edge, come back\n",
        "        if pos[0] > 10.67/2:\n",
        "            action = self.act_helper.press_keys(['a'])\n",
        "        elif pos[0] < -10.67/2:\n",
        "            action = self.act_helper.press_keys(['d'])\n",
        "        elif not opp_KO:\n",
        "            # Head toward opponent\n",
        "            if (opp_pos[0] > pos[0]):\n",
        "                action = self.act_helper.press_keys(['d'])\n",
        "            else:\n",
        "                action = self.act_helper.press_keys(['a'])\n",
        "\n",
        "        # Note: Passing in partial action\n",
        "        # Jump if below map or opponent is above you\n",
        "        if (pos[1] > 1.6 or pos[1] > opp_pos[1]) and self.time % 2 == 0:\n",
        "            action = self.act_helper.press_keys(['space'], action)\n",
        "\n",
        "        # Attack if near\n",
        "        if (pos[0] - opp_pos[0])**2 + (pos[1] - opp_pos[1])**2 < 4.0:\n",
        "            action = self.act_helper.press_keys(['j'], action)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "mKpJyPUenUk6"
      },
      "outputs": [],
      "source": [
        "class UserInputAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = self.act_helper.zeros()\n",
        "\n",
        "        keys = pygame.key.get_pressed()\n",
        "        if keys[pygame.K_w]:\n",
        "            action = self.act_helper.press_keys(['w'], action)\n",
        "        if keys[pygame.K_a]:\n",
        "            action = self.act_helper.press_keys(['a'], action)\n",
        "        if keys[pygame.K_s]:\n",
        "            action = self.act_helper.press_keys(['s'], action)\n",
        "        if keys[pygame.K_d]:\n",
        "            action = self.act_helper.press_keys(['d'], action)\n",
        "        if keys[pygame.K_SPACE]:\n",
        "            action = self.act_helper.press_keys(['space'], action)\n",
        "        # h j k l\n",
        "        if keys[pygame.K_h]:\n",
        "            action = self.act_helper.press_keys(['h'], action)\n",
        "        if keys[pygame.K_j]:\n",
        "            action = self.act_helper.press_keys(['j'], action)\n",
        "        if keys[pygame.K_k]:\n",
        "            action = self.act_helper.press_keys(['k'], action)\n",
        "        if keys[pygame.K_l]:\n",
        "            action = self.act_helper.press_keys(['l'], action)\n",
        "        if keys[pygame.K_g]:\n",
        "            action = self.act_helper.press_keys(['g'], action)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "eBHjq00zkD0b"
      },
      "outputs": [],
      "source": [
        "class ClockworkAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            action_sheet: Optional[List[Tuple[int, List[str]]]] = None,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.steps = 0\n",
        "        self.current_action_end = 0  # Tracks when the current action should stop\n",
        "        self.current_action_data = None  # Stores the active action\n",
        "        self.action_index = 0  # Index in the action sheet\n",
        "\n",
        "        if action_sheet is None:\n",
        "            self.action_sheet = [\n",
        "                (10, ['a']),\n",
        "                (1, ['l']),\n",
        "                (20, ['a']),\n",
        "                (3, ['a', 'j']),\n",
        "                (30, []),\n",
        "                (7, ['d']),\n",
        "                (1, ['a']),\n",
        "                (4, ['a','l']),\n",
        "                (1, ['a']),\n",
        "                (4, ['a','l']),\n",
        "                (1, ['a']),\n",
        "                (4, ['a','l']),\n",
        "                (1, ['a']),\n",
        "                (4, ['a','l']),\n",
        "                (20, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "            ]\n",
        "        else:\n",
        "            self.action_sheet = action_sheet\n",
        "\n",
        "\n",
        "    def predict(self, obs):\n",
        "        \"\"\"\n",
        "        Returns an action vector based on the predefined action sheet.\n",
        "        \"\"\"\n",
        "        # Check if the current action has expired\n",
        "        if self.steps >= self.current_action_end and self.action_index < len(self.action_sheet):\n",
        "            hold_time, action_data = self.action_sheet[self.action_index]\n",
        "            self.current_action_data = action_data  # Store the action\n",
        "            self.current_action_end = self.steps + hold_time  # Set duration\n",
        "            self.action_index += 1  # Move to the next action\n",
        "\n",
        "        # Apply the currently active action\n",
        "        action = self.act_helper.press_keys(self.current_action_data)\n",
        "\n",
        "\n",
        "        self.steps += 1  # Increment step counter\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "y4JPHficefKk"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "\n",
        "class SB3Agent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            sb3_class: Optional[Type[BaseAlgorithm]] = PPO,\n",
        "            file_path: Optional[str] = None\n",
        "    ):\n",
        "        self.sb3_class = sb3_class\n",
        "        super().__init__(file_path)\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        if self.file_path is None:\n",
        "            self.model = self.sb3_class(\"MlpPolicy\", self.env, verbose=0)\n",
        "            del self.env\n",
        "        else:\n",
        "            self.model = self.sb3_class.load(self.file_path, n_steps=30*90*3, batch_size=128)\n",
        "\n",
        "    def _gdown(self) -> str:\n",
        "        # Call gdown to your link\n",
        "        return\n",
        "\n",
        "    #def set_ignore_grad(self) -> None:\n",
        "        #self.model.set_ignore_act_grad(True)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action, _ = self.model.predict(obs)\n",
        "        return action\n",
        "\n",
        "    def save(self, file_path: str) -> None:\n",
        "        self.model.save(file_path, include=['num_timesteps'])\n",
        "\n",
        "    def learn(self, env, total_timesteps, log_interval: int = 1, verbose=0):\n",
        "        self.model.set_env(env)\n",
        "        self.model.verbose = verbose\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            log_interval=log_interval,\n",
        "\n",
        "\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "eiXaIv8rcyds"
      },
      "outputs": [],
      "source": [
        "from sb3_contrib import RecurrentPPO\n",
        "\n",
        "class RecurrentPPOAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            file_path: Optional[str] = None\n",
        "    ):\n",
        "        super().__init__(file_path)\n",
        "        self.lstm_states = None\n",
        "        self.episode_starts = np.ones((1,), dtype=bool)\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        if self.file_path is None:\n",
        "            self.model = RecurrentPPO(\"MlpLstmPolicy\", self.env, verbose=0)\n",
        "            del self.env\n",
        "        else:\n",
        "            self.model = RecurrentPPO.load(self.file_path, n_steps=30*90*3, batch_size=128)\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.episode_starts = True\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action, self.lstm_states = self.model.predict(obs, state=self.lstm_states, episode_start=self.episode_starts, deterministic=True)\n",
        "        if self.episode_starts: self.episode_starts = False\n",
        "        return action\n",
        "\n",
        "    def save(self, file_path: str) -> None:\n",
        "        self.model.save(file_path)\n",
        "\n",
        "    def learn(self, env, total_timesteps, log_interval: int = 16, verbose=0):\n",
        "        self.model.set_env(env)\n",
        "        self.model.verbose = verbose\n",
        "        self.model.learn(total_timesteps=total_timesteps, log_interval=log_interval)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b_3pL3gk9gI"
      },
      "source": [
        "## Training Function\n",
        "A helper function for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "id": "j3sYq4l_k9Bp"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "\n",
        "\n",
        "class TrainLogging(Enum):\n",
        "    NONE = 0\n",
        "    TO_FILE = 1\n",
        "    PLOT = 2\n",
        "\n",
        "\n",
        "def plot_results(log_folder, title=\"Learning Curve\"):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
        "\n",
        "    weights = np.repeat(1.0, 50) / 50\n",
        "    print(weights, y)\n",
        "    y = np.convolve(y, weights, \"valid\")\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y) :]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel(\"Number of Timesteps\")\n",
        "    plt.ylabel(\"Rewards\")\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def train(\n",
        "    agent: Agent,\n",
        "    reward_manager: RewardManager,\n",
        "    save_handler: Optional[SaveHandler] = None,\n",
        "    opponent_cfg: OpponentsCfg = OpponentsCfg(),\n",
        "    resolution: CameraResolution = CameraResolution.LOW,\n",
        "    train_timesteps: int = 400_000,\n",
        "    train_logging: TrainLogging = TrainLogging.PLOT,\n",
        "):\n",
        "    # Create environment\n",
        "    env = SelfPlayWarehouseBrawl(\n",
        "        reward_manager=reward_manager,\n",
        "        opponent_cfg=opponent_cfg,\n",
        "        save_handler=save_handler,\n",
        "        resolution=resolution,\n",
        "    )\n",
        "    reward_manager.subscribe_signals(env.raw_env)\n",
        "    if train_logging != TrainLogging.NONE:\n",
        "        # Create log dir\n",
        "        log_dir = (\n",
        "            f\"{save_handler._experiment_path()}/\"\n",
        "            if save_handler is not None\n",
        "            else \"/tmp/gym/\"\n",
        "        )\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Logs will be saved in log_dir/monitor.csv\n",
        "        env = Monitor(env, log_dir)\n",
        "\n",
        "    # Get the base WarehouseBrawl environment\n",
        "    base_env = env.unwrapped if hasattr(env, \"unwrapped\") else env\n",
        "\n",
        "    try:\n",
        "        agent.get_env_info(env)\n",
        "        base_env.on_training_start()  # Call on_training_start on the base env\n",
        "        agent.learn(env, total_timesteps=train_timesteps, verbose=1)\n",
        "        base_env.on_training_end()  # Call on_training_end on the base env\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    if save_handler is not None:\n",
        "        save_handler.save_agent()\n",
        "\n",
        "    if train_logging == TrainLogging.PLOT:\n",
        "        plot_results(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7wH-mhyfDpZ"
      },
      "source": [
        "## Example Reward Functions\n",
        "Find more [here](https://colab.research.google.com/drive/1qMs336DclBwdn6JBASa5ioDIfvenW8Ha?usp=sharing#scrollTo=-XAOXXMPTiHJ)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "nsKtGxYRq6t2"
      },
      "outputs": [],
      "source": [
        "def base_height_l2(\n",
        "    env: WarehouseBrawl,\n",
        "    target_height: float,\n",
        "    obj_name: str = 'player'\n",
        ") -> float:\n",
        "    \"\"\"Penalize asset height from its target using L2 squared kernel.\n",
        "\n",
        "    Note:\n",
        "        For flat terrain, target height is in the world frame. For rough terrain,\n",
        "        sensor readings can adjust the target height to account for the terrain.\n",
        "    \"\"\"\n",
        "    # Extract the used quantities (to enable type-hinting)\n",
        "    obj: GameObject = env.objects[obj_name]\n",
        "\n",
        "    # Compute the L2 squared penalty\n",
        "    return (obj.body.position.y - target_height)**2\n",
        "\n",
        "class RewardMode(Enum):\n",
        "    ASYMMETRIC_OFFENSIVE = 0\n",
        "    SYMMETRIC = 1\n",
        "    ASYMMETRIC_DEFENSIVE = 2\n",
        "\n",
        "def damage_interaction_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    mode: RewardMode = RewardMode.SYMMETRIC,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward based on damage interactions between players.\n",
        "\n",
        "    Modes:\n",
        "    - ASYMMETRIC_OFFENSIVE (0): Reward is based only on damage dealt to the opponent\n",
        "    - SYMMETRIC (1): Reward is based on both dealing damage to the opponent and avoiding damage\n",
        "    - ASYMMETRIC_DEFENSIVE (2): Reward is based only on avoiding damage\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment\n",
        "        mode (DamageRewardMode): Reward mode, one of DamageRewardMode\n",
        "\n",
        "    Returns:\n",
        "        float: The computed reward.\n",
        "    \"\"\"\n",
        "    # Getting player and opponent from the enviornment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Reward dependent on the mode\n",
        "    damage_taken = player.damage_taken_this_frame\n",
        "    damage_dealt = opponent.damage_taken_this_frame\n",
        "\n",
        "    if mode == RewardMode.ASYMMETRIC_OFFENSIVE:\n",
        "        reward = damage_dealt\n",
        "    elif mode == RewardMode.SYMMETRIC:\n",
        "        reward = damage_dealt - damage_taken\n",
        "    elif mode == RewardMode.ASYMMETRIC_DEFENSIVE:\n",
        "        reward = -damage_taken\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "    return reward / 140"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 95,
      "metadata": {
        "id": "LwvU62TGmqpX"
      },
      "outputs": [],
      "source": [
        "def danger_zone_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    zone_penalty: int = 1,\n",
        "    zone_height: float = 4.2\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Applies a penalty for every time frame player surpases a certain height threshold in the environment.\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment.\n",
        "        zone_penalty (int): The penalty applied when the player is in the danger zone.\n",
        "        zone_height (float): The height threshold defining the danger zone.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed penalty as a tensor.\n",
        "    \"\"\"\n",
        "    # Get player object from the environment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "\n",
        "    # Apply penalty if the player is in the danger zone\n",
        "    reward = -zone_penalty if player.body.position.y >= zone_height else 0.0\n",
        "\n",
        "    return reward * env.dt\n",
        "\n",
        "def in_state_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    desired_state: Type[PlayerObjectState]=BackDashState,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Applies a penalty for every time frame player surpases a certain height threshold in the environment.\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment.\n",
        "        zone_penalty (int): The penalty applied when the player is in the danger zone.\n",
        "        zone_height (float): The height threshold defining the danger zone.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed penalty as a tensor.\n",
        "    \"\"\"\n",
        "    # Get player object from the environment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "\n",
        "    # Apply penalty if the player is in the danger zone\n",
        "    reward = 1 if isinstance(player.state, desired_state) else 0.0\n",
        "\n",
        "    return reward * env.dt\n",
        "\n",
        "def head_to_middle_reward(\n",
        "    env: WarehouseBrawl,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Applies a penalty for every time frame player surpases a certain height threshold in the environment.\n",
        "\n",
        "    Args:\n",
        "        env (WarehouseBrawl): The game environment.\n",
        "        zone_penalty (int): The penalty applied when the player is in the danger zone.\n",
        "        zone_height (float): The height threshold defining the danger zone.\n",
        "\n",
        "    Returns:\n",
        "        float: The computed penalty as a tensor.\n",
        "    \"\"\"\n",
        "    # Get player object from the environment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "\n",
        "    # Apply penalty if the player is in the danger zone\n",
        "    multiplier = -1 if player.body.position.x > 0 else 1\n",
        "    reward = multiplier * (player.body.position.x - player.prev_x)\n",
        "\n",
        "    return reward\n",
        "\n",
        "def head_to_opponent(\n",
        "    env: WarehouseBrawl,\n",
        ") -> float:\n",
        "\n",
        "    # Get player object from the environment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Apply penalty if the player is in the danger zone\n",
        "    multiplier = -1 if player.body.position.x > opponent.body.position.x else 1\n",
        "    reward = multiplier * (player.body.position.x - player.prev_x)\n",
        "\n",
        "    return reward\n",
        "\n",
        "def on_win_reward(env: WarehouseBrawl, agent: str) -> float:\n",
        "    if agent == 'player':\n",
        "        return 1.0\n",
        "    else:\n",
        "        return -1.0\n",
        "\n",
        "def on_knockout_reward(env: WarehouseBrawl, agent: str) -> float:\n",
        "    if agent == 'player':\n",
        "        return -1.0\n",
        "    else:\n",
        "        return 1.0\n",
        "\n",
        "def on_combo_reward(env: WarehouseBrawl, agent: str) -> float:\n",
        "    if agent == 'player':\n",
        "        return -1.0\n",
        "    else:\n",
        "        return 1.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {},
      "outputs": [],
      "source": [
        "class RewardFunctions:\n",
        "    @staticmethod\n",
        "    def lives_advantage(env: WarehouseBrawl) -> float:\n",
        "        \"\"\"Reward for having more stocks than opponent\"\"\"\n",
        "        player: Player = env.objects[\"player\"]\n",
        "        opponent: Player = env.objects[\"opponent\"]\n",
        "        stocks_diff = player.stocks - opponent.stocks\n",
        "        return stocks_diff - 0.5\n",
        "\n",
        "    @staticmethod\n",
        "    def damage_advantage(env: WarehouseBrawl) -> float:\n",
        "        \"\"\"Reward based on damage interactions between players\"\"\"\n",
        "        return damage_interaction_reward(env, mode=RewardMode.SYMMETRIC)\n",
        "\n",
        "    @staticmethod\n",
        "    def movement_reward(env: WarehouseBrawl) -> float:\n",
        "        \"\"\"Reward for moving towards opponent with acceleration bonus\"\"\"\n",
        "        player: Player = env.objects[\"player\"]\n",
        "        opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "        # Calculate movement direction and speed\n",
        "        movement = player.body.position.x - player.prev_x\n",
        "        distance_to_opponent = abs(opponent.body.position.x - player.body.position.x)\n",
        "        moving_towards = (\n",
        "            movement * (opponent.body.position.x - player.body.position.x) > 0\n",
        "        )\n",
        "\n",
        "        # Base reward for moving towards opponent\n",
        "        base_reward = 0.1 if moving_towards else -0.05\n",
        "\n",
        "        # Add speed bonus (higher reward for faster movement)\n",
        "        speed_bonus = abs(movement) * 0.5\n",
        "\n",
        "        # Add proximity bonus (higher reward when closer)\n",
        "        proximity_bonus = 0.2 * (1 - min(distance_to_opponent / 500, 1))\n",
        "\n",
        "        # Add acceleration bonus if continuing in same direction\n",
        "        acceleration_bonus = 0.1 if abs(movement) > 0 else 0\n",
        "\n",
        "        return base_reward + speed_bonus + proximity_bonus + acceleration_bonus\n",
        "\n",
        "    @staticmethod\n",
        "    def tactical_positioning(env: WarehouseBrawl) -> float:\n",
        "        \"\"\"Normalized tactical positioning reward\"\"\"\n",
        "        player: Player = env.objects[\"player\"]\n",
        "        opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "        # Normalize advantages to [-1, 1] range\n",
        "        stock_advantage = (player.stocks - opponent.stocks) / 3.0\n",
        "        damage_advantage = (opponent.damage - player.damage) / 100.0\n",
        "\n",
        "        # Weighted sum with normalized values\n",
        "        total_advantage = stock_advantage * 0.7 + damage_advantage * 0.3\n",
        "\n",
        "        # Movement reward based on direction and input\n",
        "        movement = player.body.position.x - player.prev_x\n",
        "        correct_input = False\n",
        "\n",
        "        # Safely check key status with null checks\n",
        "        right_key = player.input.key_status.get(\"right\")\n",
        "        left_key = player.input.key_status.get(\"left\")\n",
        "\n",
        "        if movement > 0 and right_key is not None and right_key.held:\n",
        "            correct_input = True\n",
        "        elif movement < 0 and left_key is not None and left_key.held:\n",
        "            correct_input = True\n",
        "\n",
        "        # Base reward from original logic\n",
        "        base_reward = (\n",
        "            0.05\n",
        "            if (total_advantage > 0)\n",
        "            == (movement * (opponent.body.position.x - player.body.position.x) > 0)\n",
        "            else -0.05\n",
        "        )\n",
        "\n",
        "        # Add movement reward\n",
        "        movement_reward = abs(movement) * 0.5 * (3 if correct_input else 1)\n",
        "\n",
        "        return base_reward + movement_reward\n",
        "\n",
        "    @staticmethod\n",
        "    def keyboard_efficiency(env: WarehouseBrawl) -> float:\n",
        "        \"\"\"Penalize excessive button pressing\"\"\"\n",
        "        player: Player = env.objects[\"player\"]\n",
        "\n",
        "        # Count number of pressed buttons this frame\n",
        "        pressed = sum(1 for key in player.input.key_status.values() if key.held)\n",
        "\n",
        "        # Penalize pressing more than 2 buttons at once\n",
        "        if pressed > 3:\n",
        "            return -0.1 * pressed\n",
        "        return 0\n",
        "\n",
        "    @staticmethod\n",
        "    def health_disadvantage_time(env: WarehouseBrawl) -> float:\n",
        "        \"\"\"Penalize time spent with equal or worse health than opponent\"\"\"\n",
        "        player: Player = env.objects[\"player\"]\n",
        "        opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "        if player.damage >= opponent.damage:\n",
        "            return -0.1 * env.dt\n",
        "        return 0\n",
        "\n",
        "    @staticmethod\n",
        "    def on_win_reward(env: WarehouseBrawl, agent: str) -> float:\n",
        "        \"\"\"Large reward for winning\"\"\"\n",
        "        return on_win_reward(env, agent)\n",
        "\n",
        "    @staticmethod\n",
        "    def on_knockout_reward(env: WarehouseBrawl, agent: str) -> float:\n",
        "        \"\"\"Reward for scoring a knockout\"\"\"\n",
        "        return on_knockout_reward(env, agent)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9EqQkRvfLRZ"
      },
      "source": [
        "## Run Training\n",
        "\n",
        "Run this cell to run training. Be sure to set your agent under the `my_agent` variable, and modify the training using the `reward_manager`, `selfplay_handler`, `save_handler`, and `opponent_cfg`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 1. Create agents\n",
        "my_agent = SubmittedAgent(device=\"cuda\")\n",
        "\n",
        "SAVE_CONFIG = {\n",
        "    \"save_freq\": 25_000,\n",
        "    \"max_saved\": 3,\n",
        "    \"save_path\": \"checkpoints\",\n",
        "}\n",
        "\n",
        "# 2. Training setup\n",
        "reward_functions = {\n",
        "    \"lives_advantage\": RewTerm(func=RewardFunctions.lives_advantage, weight=1.5),\n",
        "    \"damage_advantage\": RewTerm(func=RewardFunctions.damage_advantage, weight=1.5),\n",
        "    \"keyboard_efficiency\": RewTerm(\n",
        "        func=RewardFunctions.keyboard_efficiency, weight=0.25\n",
        "    ),\n",
        "    \"health_disadvantage\": RewTerm(\n",
        "        func=RewardFunctions.health_disadvantage_time, weight=0.5\n",
        "    ),\n",
        "    \"tactical_positioning\": RewTerm(\n",
        "        func=RewardFunctions.tactical_positioning, weight=1\n",
        "    ),\n",
        "}\n",
        "signal_subscriptions = {\n",
        "    \"on_win\": (\"win_signal\", RewTerm(func=RewardFunctions.on_win_reward, weight=2.5)),\n",
        "    \"on_knockout\": (\n",
        "        \"knockout_signal\",\n",
        "        RewTerm(func=RewardFunctions.on_knockout_reward, weight=2.5),\n",
        "    ),\n",
        "}\n",
        "reward_manager = RewardManager(reward_functions, signal_subscriptions)\n",
        "\n",
        "opponents = {\n",
        "    \"random_agent\": (0.2, partial(RandomAgent)),\n",
        "    \"constant_agent\": (0.8, partial(ConstantAgent)),\n",
        "}\n",
        "opponent_cfg = OpponentsCfg(opponents=opponents)\n",
        "\n",
        "save_handler = SaveHandler(\n",
        "    agent=my_agent,\n",
        "    run_name=\"sample\",\n",
        "    mode=SaveHandlerMode.FORCE,\n",
        "    **SAVE_CONFIG,\n",
        ")\n",
        "\n",
        "train(\n",
        "    my_agent,\n",
        "    reward_manager,\n",
        "    save_handler,\n",
        "    opponent_cfg,\n",
        "    CameraResolution.LOW,\n",
        "    train_timesteps=200_000,\n",
        "    train_logging=TrainLogging.PLOT,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best model is checkpoints\\sample\\rl_model_0_steps.zip\n",
            "Obs space [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11]\n",
            "Action space [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 1.12e+03 |\n",
            "|    ep_rew_mean     | 1.22e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 577      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 14       |\n",
            "|    total_timesteps | 8100     |\n",
            "---------------------------------\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.03e+03    |\n",
            "|    ep_rew_mean          | 1.24e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 359         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 16200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018077496 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60.8       |\n",
            "|    explained_variance   | 0.976       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.48        |\n",
            "|    n_updates            | 12243       |\n",
            "|    policy_gradient_loss | -0.0167     |\n",
            "|    std                  | 115         |\n",
            "|    value_loss           | 24.9        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4134801_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.24e+03    |\n",
            "|    ep_rew_mean          | 906         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 318         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 76          |\n",
            "|    total_timesteps      | 24300       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014654625 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60.9       |\n",
            "|    explained_variance   | 0.986       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.25        |\n",
            "|    n_updates            | 12253       |\n",
            "|    policy_gradient_loss | -0.0218     |\n",
            "|    std                  | 116         |\n",
            "|    value_loss           | 33.9        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected constant_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.4e+03     |\n",
            "|    ep_rew_mean          | 974         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 305         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 106         |\n",
            "|    total_timesteps      | 32400       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017229265 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60.9       |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.02        |\n",
            "|    n_updates            | 12263       |\n",
            "|    policy_gradient_loss | -0.0189     |\n",
            "|    std                  | 116         |\n",
            "|    value_loss           | 20.3        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4154802_steps.\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.54e+03    |\n",
            "|    ep_rew_mean          | 1.02e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 296         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 136         |\n",
            "|    total_timesteps      | 40500       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013074223 |\n",
            "|    clip_fraction        | 0.124       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60.9       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.89        |\n",
            "|    n_updates            | 12273       |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    std                  | 116         |\n",
            "|    value_loss           | 16.4        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.5e+03     |\n",
            "|    ep_rew_mean          | 1.12e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 290         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 167         |\n",
            "|    total_timesteps      | 48600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016715754 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61         |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.64        |\n",
            "|    n_updates            | 12283       |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    std                  | 117         |\n",
            "|    value_loss           | 22          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.6e+03    |\n",
            "|    ep_rew_mean          | 1.08e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 287        |\n",
            "|    iterations           | 7          |\n",
            "|    time_elapsed         | 197        |\n",
            "|    total_timesteps      | 56700      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01571272 |\n",
            "|    clip_fraction        | 0.145      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -61        |\n",
            "|    explained_variance   | 0.979      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 34.7       |\n",
            "|    n_updates            | 12293      |\n",
            "|    policy_gradient_loss | -0.0177    |\n",
            "|    std                  | 117        |\n",
            "|    value_loss           | 18.6       |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4174803_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.57e+03    |\n",
            "|    ep_rew_mean          | 1.13e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 285         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 226         |\n",
            "|    total_timesteps      | 64800       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013831329 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.1       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.53        |\n",
            "|    n_updates            | 12303       |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    std                  | 118         |\n",
            "|    value_loss           | 23.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected based_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.62e+03    |\n",
            "|    ep_rew_mean          | 1.04e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 284         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 256         |\n",
            "|    total_timesteps      | 72900       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012677455 |\n",
            "|    clip_fraction        | 0.123       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.1       |\n",
            "|    explained_variance   | 0.986       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.76        |\n",
            "|    n_updates            | 12313       |\n",
            "|    policy_gradient_loss | -0.0177     |\n",
            "|    std                  | 119         |\n",
            "|    value_loss           | 21.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4194804_steps.\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.57e+03    |\n",
            "|    ep_rew_mean          | 1.09e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 282         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 286         |\n",
            "|    total_timesteps      | 81000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011880221 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.2       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.1         |\n",
            "|    n_updates            | 12323       |\n",
            "|    policy_gradient_loss | -0.0167     |\n",
            "|    std                  | 119         |\n",
            "|    value_loss           | 23.4        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.53e+03    |\n",
            "|    ep_rew_mean          | 1.15e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 279         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 319         |\n",
            "|    total_timesteps      | 89100       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015714167 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.2       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.9         |\n",
            "|    n_updates            | 12333       |\n",
            "|    policy_gradient_loss | -0.0189     |\n",
            "|    std                  | 119         |\n",
            "|    value_loss           | 18.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.53e+03    |\n",
            "|    ep_rew_mean          | 1.23e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 275         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 353         |\n",
            "|    total_timesteps      | 97200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013335735 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.2       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.07        |\n",
            "|    n_updates            | 12343       |\n",
            "|    policy_gradient_loss | -0.0166     |\n",
            "|    std                  | 119         |\n",
            "|    value_loss           | 12          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4214805_steps.\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.52e+03    |\n",
            "|    ep_rew_mean          | 1.28e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 273         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 385         |\n",
            "|    total_timesteps      | 105300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013883445 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.3       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.98        |\n",
            "|    n_updates            | 12353       |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    std                  | 121         |\n",
            "|    value_loss           | 14.3        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.54e+03    |\n",
            "|    ep_rew_mean          | 1.25e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 271         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 418         |\n",
            "|    total_timesteps      | 113400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012678295 |\n",
            "|    clip_fraction        | 0.116       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.3       |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 31.5        |\n",
            "|    n_updates            | 12363       |\n",
            "|    policy_gradient_loss | -0.0179     |\n",
            "|    std                  | 121         |\n",
            "|    value_loss           | 26.9        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Saving agent to checkpoints\\sample\\rl_model_4234806_steps.\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.52e+03    |\n",
            "|    ep_rew_mean          | 1.19e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 271         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 448         |\n",
            "|    total_timesteps      | 121500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014168399 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.4       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.28        |\n",
            "|    n_updates            | 12373       |\n",
            "|    policy_gradient_loss | -0.0174     |\n",
            "|    std                  | 122         |\n",
            "|    value_loss           | 22.8        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.52e+03    |\n",
            "|    ep_rew_mean          | 1.24e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 480         |\n",
            "|    total_timesteps      | 129600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015290356 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.4       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.86        |\n",
            "|    n_updates            | 12383       |\n",
            "|    policy_gradient_loss | -0.0228     |\n",
            "|    std                  | 122         |\n",
            "|    value_loss           | 22.8        |\n",
            "-----------------------------------------\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.49e+03    |\n",
            "|    ep_rew_mean          | 1.26e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 513         |\n",
            "|    total_timesteps      | 137700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014902342 |\n",
            "|    clip_fraction        | 0.156       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.4       |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.68        |\n",
            "|    n_updates            | 12393       |\n",
            "|    policy_gradient_loss | -0.0205     |\n",
            "|    std                  | 123         |\n",
            "|    value_loss           | 21          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4254807_steps.\n",
            "Selected based_agent\n",
            "Selected constant_agent\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.39e+03    |\n",
            "|    ep_rew_mean          | 1.16e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 267         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 544         |\n",
            "|    total_timesteps      | 145800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015753817 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.5       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.98        |\n",
            "|    n_updates            | 12403       |\n",
            "|    policy_gradient_loss | -0.0215     |\n",
            "|    std                  | 123         |\n",
            "|    value_loss           | 20.6        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.43e+03   |\n",
            "|    ep_rew_mean          | 1.2e+03    |\n",
            "| time/                   |            |\n",
            "|    fps                  | 268        |\n",
            "|    iterations           | 19         |\n",
            "|    time_elapsed         | 573        |\n",
            "|    total_timesteps      | 153900     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01554568 |\n",
            "|    clip_fraction        | 0.153      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -61.5      |\n",
            "|    explained_variance   | 0.976      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 26.6       |\n",
            "|    n_updates            | 12413      |\n",
            "|    policy_gradient_loss | -0.0227    |\n",
            "|    std                  | 124        |\n",
            "|    value_loss           | 37         |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\sample\\rl_model_4274808_steps.\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.46e+03   |\n",
            "|    ep_rew_mean          | 1.21e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 267        |\n",
            "|    iterations           | 20         |\n",
            "|    time_elapsed         | 604        |\n",
            "|    total_timesteps      | 162000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01781274 |\n",
            "|    clip_fraction        | 0.171      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -61.6      |\n",
            "|    explained_variance   | 0.985      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 4.22       |\n",
            "|    n_updates            | 12423      |\n",
            "|    policy_gradient_loss | -0.0195    |\n",
            "|    std                  | 125        |\n",
            "|    value_loss           | 19.9       |\n",
            "----------------------------------------\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.44e+03    |\n",
            "|    ep_rew_mean          | 1.26e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 267         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 635         |\n",
            "|    total_timesteps      | 170100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015758157 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.6       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.38        |\n",
            "|    n_updates            | 12433       |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    std                  | 126         |\n",
            "|    value_loss           | 14.3        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.41e+03     |\n",
            "|    ep_rew_mean          | 1.34e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 266          |\n",
            "|    iterations           | 22           |\n",
            "|    time_elapsed         | 668          |\n",
            "|    total_timesteps      | 178200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0135649815 |\n",
            "|    clip_fraction        | 0.147        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -61.6        |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 12.2         |\n",
            "|    n_updates            | 12443        |\n",
            "|    policy_gradient_loss | -0.0231      |\n",
            "|    std                  | 126          |\n",
            "|    value_loss           | 24.7         |\n",
            "------------------------------------------\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4294809_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected sb3_agent2\n",
            "Selected random_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.41e+03    |\n",
            "|    ep_rew_mean          | 1.27e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 266         |\n",
            "|    iterations           | 23          |\n",
            "|    time_elapsed         | 698         |\n",
            "|    total_timesteps      | 186300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017354388 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.7       |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.22        |\n",
            "|    n_updates            | 12453       |\n",
            "|    policy_gradient_loss | -0.0233     |\n",
            "|    std                  | 126         |\n",
            "|    value_loss           | 18.8        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected based_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.35e+03    |\n",
            "|    ep_rew_mean          | 1.23e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 269         |\n",
            "|    iterations           | 24          |\n",
            "|    time_elapsed         | 722         |\n",
            "|    total_timesteps      | 194400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017996632 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.7       |\n",
            "|    explained_variance   | 0.979       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.6        |\n",
            "|    n_updates            | 12463       |\n",
            "|    policy_gradient_loss | -0.0269     |\n",
            "|    std                  | 127         |\n",
            "|    value_loss           | 27.4        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4314810_steps.\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.36e+03    |\n",
            "|    ep_rew_mean          | 1.26e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 272         |\n",
            "|    iterations           | 25          |\n",
            "|    time_elapsed         | 742         |\n",
            "|    total_timesteps      | 202500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013341627 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.7       |\n",
            "|    explained_variance   | 0.975       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.37        |\n",
            "|    n_updates            | 12473       |\n",
            "|    policy_gradient_loss | -0.0246     |\n",
            "|    std                  | 127         |\n",
            "|    value_loss           | 36.3        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.33e+03    |\n",
            "|    ep_rew_mean          | 1.29e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 276         |\n",
            "|    iterations           | 26          |\n",
            "|    time_elapsed         | 762         |\n",
            "|    total_timesteps      | 210600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019048316 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.8       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.55        |\n",
            "|    n_updates            | 12483       |\n",
            "|    policy_gradient_loss | -0.0173     |\n",
            "|    std                  | 128         |\n",
            "|    value_loss           | 10.9        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.36e+03    |\n",
            "|    ep_rew_mean          | 1.32e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 278         |\n",
            "|    iterations           | 27          |\n",
            "|    time_elapsed         | 783         |\n",
            "|    total_timesteps      | 218700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014083491 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.8       |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 21.2        |\n",
            "|    n_updates            | 12493       |\n",
            "|    policy_gradient_loss | -0.0198     |\n",
            "|    std                  | 129         |\n",
            "|    value_loss           | 24.4        |\n",
            "-----------------------------------------\n",
            "Saving agent to checkpoints\\sample\\rl_model_4334811_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.36e+03    |\n",
            "|    ep_rew_mean          | 1.26e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 281         |\n",
            "|    iterations           | 28          |\n",
            "|    time_elapsed         | 805         |\n",
            "|    total_timesteps      | 226800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017730912 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.9       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.907       |\n",
            "|    n_updates            | 12503       |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    std                  | 129         |\n",
            "|    value_loss           | 17.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.38e+03    |\n",
            "|    ep_rew_mean          | 1.26e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 283         |\n",
            "|    iterations           | 29          |\n",
            "|    time_elapsed         | 828         |\n",
            "|    total_timesteps      | 234900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014849491 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -61.9       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.59        |\n",
            "|    n_updates            | 12513       |\n",
            "|    policy_gradient_loss | -0.023      |\n",
            "|    std                  | 130         |\n",
            "|    value_loss           | 23.3        |\n",
            "-----------------------------------------\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4354812_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.39e+03    |\n",
            "|    ep_rew_mean          | 1.25e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 288         |\n",
            "|    iterations           | 30          |\n",
            "|    time_elapsed         | 843         |\n",
            "|    total_timesteps      | 243000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014830498 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62         |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.34        |\n",
            "|    n_updates            | 12523       |\n",
            "|    policy_gradient_loss | -0.0213     |\n",
            "|    std                  | 130         |\n",
            "|    value_loss           | 35.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.37e+03    |\n",
            "|    ep_rew_mean          | 1.29e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 291         |\n",
            "|    iterations           | 31          |\n",
            "|    time_elapsed         | 861         |\n",
            "|    total_timesteps      | 251100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015940947 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62         |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.89        |\n",
            "|    n_updates            | 12533       |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    std                  | 131         |\n",
            "|    value_loss           | 18.1        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.41e+03    |\n",
            "|    ep_rew_mean          | 1.37e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 294         |\n",
            "|    iterations           | 32          |\n",
            "|    time_elapsed         | 878         |\n",
            "|    total_timesteps      | 259200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016116988 |\n",
            "|    clip_fraction        | 0.146       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62         |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.1        |\n",
            "|    n_updates            | 12543       |\n",
            "|    policy_gradient_loss | -0.0177     |\n",
            "|    std                  | 131         |\n",
            "|    value_loss           | 13.7        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4374813_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.38e+03    |\n",
            "|    ep_rew_mean          | 1.25e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 298         |\n",
            "|    iterations           | 33          |\n",
            "|    time_elapsed         | 896         |\n",
            "|    total_timesteps      | 267300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010536261 |\n",
            "|    clip_fraction        | 0.102       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.1       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11          |\n",
            "|    n_updates            | 12553       |\n",
            "|    policy_gradient_loss | -0.0162     |\n",
            "|    std                  | 131         |\n",
            "|    value_loss           | 19.5        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.35e+03    |\n",
            "|    ep_rew_mean          | 1.18e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 301         |\n",
            "|    iterations           | 34          |\n",
            "|    time_elapsed         | 912         |\n",
            "|    total_timesteps      | 275400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015309634 |\n",
            "|    clip_fraction        | 0.162       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.1       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.26        |\n",
            "|    n_updates            | 12563       |\n",
            "|    policy_gradient_loss | -0.0217     |\n",
            "|    std                  | 132         |\n",
            "|    value_loss           | 24.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4394814_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.35e+03    |\n",
            "|    ep_rew_mean          | 1.15e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 304         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 930         |\n",
            "|    total_timesteps      | 283500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019007124 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.2       |\n",
            "|    explained_variance   | 0.963       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.06        |\n",
            "|    n_updates            | 12573       |\n",
            "|    policy_gradient_loss | -0.0228     |\n",
            "|    std                  | 133         |\n",
            "|    value_loss           | 30.2        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.31e+03    |\n",
            "|    ep_rew_mean          | 1.14e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 306         |\n",
            "|    iterations           | 36          |\n",
            "|    time_elapsed         | 950         |\n",
            "|    total_timesteps      | 291600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015558192 |\n",
            "|    clip_fraction        | 0.163       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.2       |\n",
            "|    explained_variance   | 0.967       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.22        |\n",
            "|    n_updates            | 12583       |\n",
            "|    policy_gradient_loss | -0.0272     |\n",
            "|    std                  | 133         |\n",
            "|    value_loss           | 39.5        |\n",
            "-----------------------------------------\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.32e+03    |\n",
            "|    ep_rew_mean          | 1.17e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 309         |\n",
            "|    iterations           | 37          |\n",
            "|    time_elapsed         | 968         |\n",
            "|    total_timesteps      | 299700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014828581 |\n",
            "|    clip_fraction        | 0.132       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.2       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.75        |\n",
            "|    n_updates            | 12593       |\n",
            "|    policy_gradient_loss | -0.0181     |\n",
            "|    std                  | 133         |\n",
            "|    value_loss           | 18          |\n",
            "-----------------------------------------\n",
            "Saving agent to checkpoints\\sample\\rl_model_4414815_steps.\n",
            "Selected sb3_agent2\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.28e+03    |\n",
            "|    ep_rew_mean          | 1.07e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 312         |\n",
            "|    iterations           | 38          |\n",
            "|    time_elapsed         | 984         |\n",
            "|    total_timesteps      | 307800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018627372 |\n",
            "|    clip_fraction        | 0.177       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.2       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.29        |\n",
            "|    n_updates            | 12603       |\n",
            "|    policy_gradient_loss | -0.0194     |\n",
            "|    std                  | 134         |\n",
            "|    value_loss           | 17          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.33e+03    |\n",
            "|    ep_rew_mean          | 1.14e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 315         |\n",
            "|    iterations           | 39          |\n",
            "|    time_elapsed         | 1000        |\n",
            "|    total_timesteps      | 315900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014589027 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.3       |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.54        |\n",
            "|    n_updates            | 12613       |\n",
            "|    policy_gradient_loss | -0.0175     |\n",
            "|    std                  | 135         |\n",
            "|    value_loss           | 18          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4434816_steps.\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.35e+03    |\n",
            "|    ep_rew_mean          | 1.25e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 318         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 1015        |\n",
            "|    total_timesteps      | 324000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020993438 |\n",
            "|    clip_fraction        | 0.181       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.3       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.52        |\n",
            "|    n_updates            | 12623       |\n",
            "|    policy_gradient_loss | -0.0163     |\n",
            "|    std                  | 135         |\n",
            "|    value_loss           | 10.7        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.22e+03    |\n",
            "|    ep_rew_mean          | 1.17e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 321         |\n",
            "|    iterations           | 41          |\n",
            "|    time_elapsed         | 1032        |\n",
            "|    total_timesteps      | 332100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014654461 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.3       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.45        |\n",
            "|    n_updates            | 12633       |\n",
            "|    policy_gradient_loss | -0.0204     |\n",
            "|    std                  | 135         |\n",
            "|    value_loss           | 31.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4454817_steps.\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.23e+03    |\n",
            "|    ep_rew_mean          | 1.15e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 324         |\n",
            "|    iterations           | 42          |\n",
            "|    time_elapsed         | 1047        |\n",
            "|    total_timesteps      | 340200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015696649 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.4       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.6        |\n",
            "|    n_updates            | 12643       |\n",
            "|    policy_gradient_loss | -0.019      |\n",
            "|    std                  | 136         |\n",
            "|    value_loss           | 23.1        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected based_agent\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.21e+03    |\n",
            "|    ep_rew_mean          | 1.08e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 327         |\n",
            "|    iterations           | 43          |\n",
            "|    time_elapsed         | 1063        |\n",
            "|    total_timesteps      | 348300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018771525 |\n",
            "|    clip_fraction        | 0.186       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.4       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.738       |\n",
            "|    n_updates            | 12653       |\n",
            "|    policy_gradient_loss | -0.023      |\n",
            "|    std                  | 137         |\n",
            "|    value_loss           | 13.9        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.22e+03    |\n",
            "|    ep_rew_mean          | 1.12e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 330         |\n",
            "|    iterations           | 44          |\n",
            "|    time_elapsed         | 1079        |\n",
            "|    total_timesteps      | 356400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013625713 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.4       |\n",
            "|    explained_variance   | 0.975       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.49        |\n",
            "|    n_updates            | 12663       |\n",
            "|    policy_gradient_loss | -0.0219     |\n",
            "|    std                  | 137         |\n",
            "|    value_loss           | 39.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4474818_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.22e+03   |\n",
            "|    ep_rew_mean          | 1.04e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 332        |\n",
            "|    iterations           | 45         |\n",
            "|    time_elapsed         | 1095       |\n",
            "|    total_timesteps      | 364500     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01928408 |\n",
            "|    clip_fraction        | 0.169      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -62.5      |\n",
            "|    explained_variance   | 0.993      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.41       |\n",
            "|    n_updates            | 12673      |\n",
            "|    policy_gradient_loss | -0.0175    |\n",
            "|    std                  | 137        |\n",
            "|    value_loss           | 11.2       |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.26e+03    |\n",
            "|    ep_rew_mean          | 1.01e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 333         |\n",
            "|    iterations           | 46          |\n",
            "|    time_elapsed         | 1116        |\n",
            "|    total_timesteps      | 372600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013726151 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.5       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.01        |\n",
            "|    n_updates            | 12683       |\n",
            "|    policy_gradient_loss | -0.0184     |\n",
            "|    std                  | 138         |\n",
            "|    value_loss           | 35.1        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4494819_steps.\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.22e+03    |\n",
            "|    ep_rew_mean          | 935         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 335         |\n",
            "|    iterations           | 47          |\n",
            "|    time_elapsed         | 1133        |\n",
            "|    total_timesteps      | 380700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018691856 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.5       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.89        |\n",
            "|    n_updates            | 12693       |\n",
            "|    policy_gradient_loss | -0.0194     |\n",
            "|    std                  | 138         |\n",
            "|    value_loss           | 21.7        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.24e+03    |\n",
            "|    ep_rew_mean          | 969         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 337         |\n",
            "|    iterations           | 48          |\n",
            "|    time_elapsed         | 1152        |\n",
            "|    total_timesteps      | 388800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018980924 |\n",
            "|    clip_fraction        | 0.164       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.6       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.15        |\n",
            "|    n_updates            | 12703       |\n",
            "|    policy_gradient_loss | -0.0222     |\n",
            "|    std                  | 140         |\n",
            "|    value_loss           | 24.2        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.25e+03    |\n",
            "|    ep_rew_mean          | 1.05e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 338         |\n",
            "|    iterations           | 49          |\n",
            "|    time_elapsed         | 1171        |\n",
            "|    total_timesteps      | 396900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014425894 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.7       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 27.3        |\n",
            "|    n_updates            | 12713       |\n",
            "|    policy_gradient_loss | -0.0224     |\n",
            "|    std                  | 140         |\n",
            "|    value_loss           | 30.7        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4514820_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.28e+03    |\n",
            "|    ep_rew_mean          | 1.12e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 340         |\n",
            "|    iterations           | 50          |\n",
            "|    time_elapsed         | 1189        |\n",
            "|    total_timesteps      | 405000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017296638 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.7       |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.99        |\n",
            "|    n_updates            | 12723       |\n",
            "|    policy_gradient_loss | -0.0204     |\n",
            "|    std                  | 141         |\n",
            "|    value_loss           | 23.4        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.32e+03    |\n",
            "|    ep_rew_mean          | 1.13e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 342         |\n",
            "|    iterations           | 51          |\n",
            "|    time_elapsed         | 1204        |\n",
            "|    total_timesteps      | 413100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014050006 |\n",
            "|    clip_fraction        | 0.147       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.8       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.81        |\n",
            "|    n_updates            | 12733       |\n",
            "|    policy_gradient_loss | -0.017      |\n",
            "|    std                  | 142         |\n",
            "|    value_loss           | 18.1        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4534821_steps.\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.38e+03    |\n",
            "|    ep_rew_mean          | 1.08e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 344         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 1221        |\n",
            "|    total_timesteps      | 421200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014838031 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.8       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.702       |\n",
            "|    n_updates            | 12743       |\n",
            "|    policy_gradient_loss | -0.021      |\n",
            "|    std                  | 142         |\n",
            "|    value_loss           | 32.2        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.38e+03    |\n",
            "|    ep_rew_mean          | 1.07e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 346         |\n",
            "|    iterations           | 53          |\n",
            "|    time_elapsed         | 1237        |\n",
            "|    total_timesteps      | 429300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016353415 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -62.9       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.41        |\n",
            "|    n_updates            | 12753       |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    std                  | 143         |\n",
            "|    value_loss           | 25.3        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.35e+03   |\n",
            "|    ep_rew_mean          | 1.05e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 348        |\n",
            "|    iterations           | 54         |\n",
            "|    time_elapsed         | 1253       |\n",
            "|    total_timesteps      | 437400     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01355051 |\n",
            "|    clip_fraction        | 0.13       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -62.9      |\n",
            "|    explained_variance   | 0.988      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.27       |\n",
            "|    n_updates            | 12763      |\n",
            "|    policy_gradient_loss | -0.0172    |\n",
            "|    std                  | 144        |\n",
            "|    value_loss           | 26.6       |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\sample\\rl_model_4554822_steps.\n",
            "Selected based_agent\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.34e+03    |\n",
            "|    ep_rew_mean          | 996         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 350         |\n",
            "|    iterations           | 55          |\n",
            "|    time_elapsed         | 1270        |\n",
            "|    total_timesteps      | 445500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012880759 |\n",
            "|    clip_fraction        | 0.141       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63         |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.73        |\n",
            "|    n_updates            | 12773       |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    std                  | 145         |\n",
            "|    value_loss           | 28.8        |\n",
            "-----------------------------------------\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.28e+03    |\n",
            "|    ep_rew_mean          | 926         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 352         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 1287        |\n",
            "|    total_timesteps      | 453600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015440943 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.1       |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.7         |\n",
            "|    n_updates            | 12783       |\n",
            "|    policy_gradient_loss | -0.0219     |\n",
            "|    std                  | 146         |\n",
            "|    value_loss           | 20.2        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Saving agent to checkpoints\\sample\\rl_model_4574823_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.32e+03    |\n",
            "|    ep_rew_mean          | 960         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 353         |\n",
            "|    iterations           | 57          |\n",
            "|    time_elapsed         | 1305        |\n",
            "|    total_timesteps      | 461700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015369932 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.1       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.6        |\n",
            "|    n_updates            | 12793       |\n",
            "|    policy_gradient_loss | -0.021      |\n",
            "|    std                  | 147         |\n",
            "|    value_loss           | 20          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.33e+03   |\n",
            "|    ep_rew_mean          | 1.02e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 355        |\n",
            "|    iterations           | 58         |\n",
            "|    time_elapsed         | 1322       |\n",
            "|    total_timesteps      | 469800     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01645524 |\n",
            "|    clip_fraction        | 0.146      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -63.1      |\n",
            "|    explained_variance   | 0.983      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.57       |\n",
            "|    n_updates            | 12803      |\n",
            "|    policy_gradient_loss | -0.0216    |\n",
            "|    std                  | 148        |\n",
            "|    value_loss           | 18.1       |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.34e+03    |\n",
            "|    ep_rew_mean          | 1.18e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 356         |\n",
            "|    iterations           | 59          |\n",
            "|    time_elapsed         | 1340        |\n",
            "|    total_timesteps      | 477900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014601363 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.2       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | -0.0398     |\n",
            "|    n_updates            | 12813       |\n",
            "|    policy_gradient_loss | -0.0198     |\n",
            "|    std                  | 148         |\n",
            "|    value_loss           | 12.1        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4594824_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.35e+03    |\n",
            "|    ep_rew_mean          | 1.22e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 357         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 1358        |\n",
            "|    total_timesteps      | 486000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015508467 |\n",
            "|    clip_fraction        | 0.166       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.3       |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.02        |\n",
            "|    n_updates            | 12823       |\n",
            "|    policy_gradient_loss | -0.0169     |\n",
            "|    std                  | 150         |\n",
            "|    value_loss           | 7.66        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.31e+03    |\n",
            "|    ep_rew_mean          | 1.23e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 359         |\n",
            "|    iterations           | 61          |\n",
            "|    time_elapsed         | 1376        |\n",
            "|    total_timesteps      | 494100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013698487 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.4       |\n",
            "|    explained_variance   | 0.986       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.14        |\n",
            "|    n_updates            | 12833       |\n",
            "|    policy_gradient_loss | -0.0194     |\n",
            "|    std                  | 151         |\n",
            "|    value_loss           | 22.4        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4614825_steps.\n",
            "Selected self_play\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.34e+03     |\n",
            "|    ep_rew_mean          | 1.31e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 359          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 1396         |\n",
            "|    total_timesteps      | 502200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0152544165 |\n",
            "|    clip_fraction        | 0.13         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -63.4        |\n",
            "|    explained_variance   | 0.985        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 3.64         |\n",
            "|    n_updates            | 12843        |\n",
            "|    policy_gradient_loss | -0.0185      |\n",
            "|    std                  | 152          |\n",
            "|    value_loss           | 30           |\n",
            "------------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.37e+03    |\n",
            "|    ep_rew_mean          | 1.39e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 359         |\n",
            "|    iterations           | 63          |\n",
            "|    time_elapsed         | 1419        |\n",
            "|    total_timesteps      | 510300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.029555557 |\n",
            "|    clip_fraction        | 0.181       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.5       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.17        |\n",
            "|    n_updates            | 12853       |\n",
            "|    policy_gradient_loss | -0.0205     |\n",
            "|    std                  | 153         |\n",
            "|    value_loss           | 9.83        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.36e+03    |\n",
            "|    ep_rew_mean          | 1.43e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 359         |\n",
            "|    iterations           | 64          |\n",
            "|    time_elapsed         | 1441        |\n",
            "|    total_timesteps      | 518400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013128303 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.5       |\n",
            "|    explained_variance   | 0.96        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 36.8        |\n",
            "|    n_updates            | 12863       |\n",
            "|    policy_gradient_loss | -0.0132     |\n",
            "|    std                  | 153         |\n",
            "|    value_loss           | 40.1        |\n",
            "-----------------------------------------\n",
            "Saving agent to checkpoints\\sample\\rl_model_4634826_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.39e+03    |\n",
            "|    ep_rew_mean          | 1.5e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 360         |\n",
            "|    iterations           | 65          |\n",
            "|    time_elapsed         | 1460        |\n",
            "|    total_timesteps      | 526500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015453586 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.5       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 13          |\n",
            "|    n_updates            | 12873       |\n",
            "|    policy_gradient_loss | -0.0223     |\n",
            "|    std                  | 153         |\n",
            "|    value_loss           | 17.9        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.44e+03     |\n",
            "|    ep_rew_mean          | 1.61e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 362          |\n",
            "|    iterations           | 66           |\n",
            "|    time_elapsed         | 1476         |\n",
            "|    total_timesteps      | 534600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0146414265 |\n",
            "|    clip_fraction        | 0.131        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -63.6        |\n",
            "|    explained_variance   | 0.985        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 11.5         |\n",
            "|    n_updates            | 12883        |\n",
            "|    policy_gradient_loss | -0.0147      |\n",
            "|    std                  | 154          |\n",
            "|    value_loss           | 25.1         |\n",
            "------------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4654827_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.43e+03  |\n",
            "|    ep_rew_mean          | 1.64e+03  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 363       |\n",
            "|    iterations           | 67        |\n",
            "|    time_elapsed         | 1494      |\n",
            "|    total_timesteps      | 542700    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0199332 |\n",
            "|    clip_fraction        | 0.198     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -63.6     |\n",
            "|    explained_variance   | 0.988     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 0.914     |\n",
            "|    n_updates            | 12893     |\n",
            "|    policy_gradient_loss | -0.0177   |\n",
            "|    std                  | 154       |\n",
            "|    value_loss           | 8.34      |\n",
            "---------------------------------------\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.39e+03     |\n",
            "|    ep_rew_mean          | 1.62e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 364          |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 1510         |\n",
            "|    total_timesteps      | 550800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0150069855 |\n",
            "|    clip_fraction        | 0.145        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -63.6        |\n",
            "|    explained_variance   | 0.98         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 15.9         |\n",
            "|    n_updates            | 12903        |\n",
            "|    policy_gradient_loss | -0.0189      |\n",
            "|    std                  | 155          |\n",
            "|    value_loss           | 24.6         |\n",
            "------------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.4e+03     |\n",
            "|    ep_rew_mean          | 1.66e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 365         |\n",
            "|    iterations           | 69          |\n",
            "|    time_elapsed         | 1527        |\n",
            "|    total_timesteps      | 558900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014062724 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.7       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.93        |\n",
            "|    n_updates            | 12913       |\n",
            "|    policy_gradient_loss | -0.0181     |\n",
            "|    std                  | 155         |\n",
            "|    value_loss           | 22.2        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\sample\\rl_model_4674828_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.42e+03    |\n",
            "|    ep_rew_mean          | 1.67e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 366         |\n",
            "|    iterations           | 70          |\n",
            "|    time_elapsed         | 1546        |\n",
            "|    total_timesteps      | 567000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012592669 |\n",
            "|    clip_fraction        | 0.125       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.7       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.47        |\n",
            "|    n_updates            | 12923       |\n",
            "|    policy_gradient_loss | -0.0184     |\n",
            "|    std                  | 156         |\n",
            "|    value_loss           | 21          |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.44e+03    |\n",
            "|    ep_rew_mean          | 1.68e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 71          |\n",
            "|    time_elapsed         | 1564        |\n",
            "|    total_timesteps      | 575100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016860995 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.8       |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.796       |\n",
            "|    n_updates            | 12933       |\n",
            "|    policy_gradient_loss | -0.0214     |\n",
            "|    std                  | 157         |\n",
            "|    value_loss           | 15.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4694829_steps.\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.47e+03    |\n",
            "|    ep_rew_mean          | 1.67e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 1582        |\n",
            "|    total_timesteps      | 583200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013971409 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.8       |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.56        |\n",
            "|    n_updates            | 12943       |\n",
            "|    policy_gradient_loss | -0.0218     |\n",
            "|    std                  | 157         |\n",
            "|    value_loss           | 13.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.49e+03  |\n",
            "|    ep_rew_mean          | 1.76e+03  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 369       |\n",
            "|    iterations           | 73        |\n",
            "|    time_elapsed         | 1600      |\n",
            "|    total_timesteps      | 591300    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0151805 |\n",
            "|    clip_fraction        | 0.14      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -63.8     |\n",
            "|    explained_variance   | 0.99      |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 2.47      |\n",
            "|    n_updates            | 12953     |\n",
            "|    policy_gradient_loss | -0.0202   |\n",
            "|    std                  | 158       |\n",
            "|    value_loss           | 22.4      |\n",
            "---------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.51e+03    |\n",
            "|    ep_rew_mean          | 1.82e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 370         |\n",
            "|    iterations           | 74          |\n",
            "|    time_elapsed         | 1616        |\n",
            "|    total_timesteps      | 599400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012379017 |\n",
            "|    clip_fraction        | 0.118       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -63.9       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.48        |\n",
            "|    n_updates            | 12963       |\n",
            "|    policy_gradient_loss | -0.0176     |\n",
            "|    std                  | 159         |\n",
            "|    value_loss           | 20          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4714830_steps.\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.51e+03    |\n",
            "|    ep_rew_mean          | 1.78e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 371         |\n",
            "|    iterations           | 75          |\n",
            "|    time_elapsed         | 1633        |\n",
            "|    total_timesteps      | 607500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014625983 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64         |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.61        |\n",
            "|    n_updates            | 12973       |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    std                  | 160         |\n",
            "|    value_loss           | 15.4        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.52e+03    |\n",
            "|    ep_rew_mean          | 1.82e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 372         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 1652        |\n",
            "|    total_timesteps      | 615600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013240375 |\n",
            "|    clip_fraction        | 0.13        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64         |\n",
            "|    explained_variance   | 0.952       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.63        |\n",
            "|    n_updates            | 12983       |\n",
            "|    policy_gradient_loss | -0.0183     |\n",
            "|    std                  | 160         |\n",
            "|    value_loss           | 40.8        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4734831_steps.\n",
            "Selected sb3_agent1\n",
            "Selected sb3_agent2\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.55e+03    |\n",
            "|    ep_rew_mean          | 1.81e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 373         |\n",
            "|    iterations           | 77          |\n",
            "|    time_elapsed         | 1671        |\n",
            "|    total_timesteps      | 623700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015273313 |\n",
            "|    clip_fraction        | 0.143       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64         |\n",
            "|    explained_variance   | 0.993       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.8         |\n",
            "|    n_updates            | 12993       |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    std                  | 160         |\n",
            "|    value_loss           | 14.8        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.56e+03   |\n",
            "|    ep_rew_mean          | 1.69e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 373        |\n",
            "|    iterations           | 78         |\n",
            "|    time_elapsed         | 1689       |\n",
            "|    total_timesteps      | 631800     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01523095 |\n",
            "|    clip_fraction        | 0.148      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -64        |\n",
            "|    explained_variance   | 0.994      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 0.335      |\n",
            "|    n_updates            | 13003      |\n",
            "|    policy_gradient_loss | -0.0174    |\n",
            "|    std                  | 161        |\n",
            "|    value_loss           | 16.4       |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 1.57e+03  |\n",
            "|    ep_rew_mean          | 1.58e+03  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 374       |\n",
            "|    iterations           | 79        |\n",
            "|    time_elapsed         | 1707      |\n",
            "|    total_timesteps      | 639900    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0177806 |\n",
            "|    clip_fraction        | 0.17      |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -64.1     |\n",
            "|    explained_variance   | 0.983     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 6.84      |\n",
            "|    n_updates            | 13013     |\n",
            "|    policy_gradient_loss | -0.0235   |\n",
            "|    std                  | 162       |\n",
            "|    value_loss           | 25.3      |\n",
            "---------------------------------------\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\sample\\rl_model_4754832_steps.\n",
            "Selected self_play\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.56e+03   |\n",
            "|    ep_rew_mean          | 1.51e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 375        |\n",
            "|    iterations           | 80         |\n",
            "|    time_elapsed         | 1723       |\n",
            "|    total_timesteps      | 648000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02012185 |\n",
            "|    clip_fraction        | 0.181      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -64.1      |\n",
            "|    explained_variance   | 0.989      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.92       |\n",
            "|    n_updates            | 13023      |\n",
            "|    policy_gradient_loss | -0.0255    |\n",
            "|    std                  | 162        |\n",
            "|    value_loss           | 12.6       |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.45e+03    |\n",
            "|    ep_rew_mean          | 1.29e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 377         |\n",
            "|    iterations           | 81          |\n",
            "|    time_elapsed         | 1740        |\n",
            "|    total_timesteps      | 656100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016785048 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.1       |\n",
            "|    explained_variance   | 0.989       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.45        |\n",
            "|    n_updates            | 13033       |\n",
            "|    policy_gradient_loss | -0.0157     |\n",
            "|    std                  | 163         |\n",
            "|    value_loss           | 32.7        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4774833_steps.\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.47e+03   |\n",
            "|    ep_rew_mean          | 1.23e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 378        |\n",
            "|    iterations           | 82         |\n",
            "|    time_elapsed         | 1756       |\n",
            "|    total_timesteps      | 664200     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01618978 |\n",
            "|    clip_fraction        | 0.17       |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -64.2      |\n",
            "|    explained_variance   | 0.991      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.09       |\n",
            "|    n_updates            | 13043      |\n",
            "|    policy_gradient_loss | -0.0204    |\n",
            "|    std                  | 163        |\n",
            "|    value_loss           | 17.2       |\n",
            "----------------------------------------\n",
            "Selected constant_agent\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.48e+03    |\n",
            "|    ep_rew_mean          | 1.18e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 378         |\n",
            "|    iterations           | 83          |\n",
            "|    time_elapsed         | 1774        |\n",
            "|    total_timesteps      | 672300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012434824 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.2       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.12        |\n",
            "|    n_updates            | 13053       |\n",
            "|    policy_gradient_loss | -0.0141     |\n",
            "|    std                  | 163         |\n",
            "|    value_loss           | 16.9        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4794834_steps.\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.42e+03    |\n",
            "|    ep_rew_mean          | 984         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 379         |\n",
            "|    iterations           | 84          |\n",
            "|    time_elapsed         | 1793        |\n",
            "|    total_timesteps      | 680400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016375285 |\n",
            "|    clip_fraction        | 0.168       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.2       |\n",
            "|    explained_variance   | 0.988       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.07        |\n",
            "|    n_updates            | 13063       |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    std                  | 163         |\n",
            "|    value_loss           | 23          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.44e+03    |\n",
            "|    ep_rew_mean          | 968         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 380         |\n",
            "|    iterations           | 85          |\n",
            "|    time_elapsed         | 1811        |\n",
            "|    total_timesteps      | 688500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015992392 |\n",
            "|    clip_fraction        | 0.148       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.2       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.51        |\n",
            "|    n_updates            | 13073       |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    std                  | 164         |\n",
            "|    value_loss           | 17.9        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.48e+03    |\n",
            "|    ep_rew_mean          | 990         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 381         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 1828        |\n",
            "|    total_timesteps      | 696600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015872624 |\n",
            "|    clip_fraction        | 0.165       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.3       |\n",
            "|    explained_variance   | 0.981       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.86        |\n",
            "|    n_updates            | 13083       |\n",
            "|    policy_gradient_loss | -0.0153     |\n",
            "|    std                  | 165         |\n",
            "|    value_loss           | 14          |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\sample\\rl_model_4814835_steps.\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.54e+03    |\n",
            "|    ep_rew_mean          | 1.08e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 381         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 1845        |\n",
            "|    total_timesteps      | 704700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014338326 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.3       |\n",
            "|    explained_variance   | 0.984       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.4        |\n",
            "|    n_updates            | 13093       |\n",
            "|    policy_gradient_loss | -0.0176     |\n",
            "|    std                  | 166         |\n",
            "|    value_loss           | 16.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.49e+03    |\n",
            "|    ep_rew_mean          | 1.08e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 382         |\n",
            "|    iterations           | 88          |\n",
            "|    time_elapsed         | 1862        |\n",
            "|    total_timesteps      | 712800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014643486 |\n",
            "|    clip_fraction        | 0.133       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.4       |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 88.4        |\n",
            "|    n_updates            | 13103       |\n",
            "|    policy_gradient_loss | -0.0186     |\n",
            "|    std                  | 167         |\n",
            "|    value_loss           | 33.1        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4834836_steps.\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.48e+03     |\n",
            "|    ep_rew_mean          | 1.17e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 383          |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 1878         |\n",
            "|    total_timesteps      | 720900       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0148028955 |\n",
            "|    clip_fraction        | 0.158        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -64.4        |\n",
            "|    explained_variance   | 0.978        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.29         |\n",
            "|    n_updates            | 13113        |\n",
            "|    policy_gradient_loss | -0.0202      |\n",
            "|    std                  | 168          |\n",
            "|    value_loss           | 16.8         |\n",
            "------------------------------------------\n",
            "Selected random_agent\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.49e+03     |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 384          |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 1894         |\n",
            "|    total_timesteps      | 729000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0142215965 |\n",
            "|    clip_fraction        | 0.142        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -64.5        |\n",
            "|    explained_variance   | 0.983        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.76         |\n",
            "|    n_updates            | 13123        |\n",
            "|    policy_gradient_loss | -0.0159      |\n",
            "|    std                  | 169          |\n",
            "|    value_loss           | 31.3         |\n",
            "------------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.48e+03     |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 385          |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 1911         |\n",
            "|    total_timesteps      | 737100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0155714005 |\n",
            "|    clip_fraction        | 0.157        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -64.5        |\n",
            "|    explained_variance   | 0.97         |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 23.2         |\n",
            "|    n_updates            | 13133        |\n",
            "|    policy_gradient_loss | -0.0203      |\n",
            "|    std                  | 170          |\n",
            "|    value_loss           | 28.3         |\n",
            "------------------------------------------\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4854837_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.46e+03    |\n",
            "|    ep_rew_mean          | 1.1e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 386         |\n",
            "|    iterations           | 92          |\n",
            "|    time_elapsed         | 1928        |\n",
            "|    total_timesteps      | 745200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016136916 |\n",
            "|    clip_fraction        | 0.154       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.6       |\n",
            "|    explained_variance   | 0.977       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 13.7        |\n",
            "|    n_updates            | 13143       |\n",
            "|    policy_gradient_loss | -0.0225     |\n",
            "|    std                  | 171         |\n",
            "|    value_loss           | 30.8        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.51e+03     |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 386          |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 1947         |\n",
            "|    total_timesteps      | 753300       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0152319195 |\n",
            "|    clip_fraction        | 0.162        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -64.6        |\n",
            "|    explained_variance   | 0.981        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.48         |\n",
            "|    n_updates            | 13153        |\n",
            "|    policy_gradient_loss | -0.0203      |\n",
            "|    std                  | 170          |\n",
            "|    value_loss           | 14.5         |\n",
            "------------------------------------------\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4874838_steps.\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.5e+03     |\n",
            "|    ep_rew_mean          | 1.1e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 387         |\n",
            "|    iterations           | 94          |\n",
            "|    time_elapsed         | 1965        |\n",
            "|    total_timesteps      | 761400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015577124 |\n",
            "|    clip_fraction        | 0.178       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.6       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.85        |\n",
            "|    n_updates            | 13163       |\n",
            "|    policy_gradient_loss | -0.0182     |\n",
            "|    std                  | 171         |\n",
            "|    value_loss           | 15.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.45e+03    |\n",
            "|    ep_rew_mean          | 1.02e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 387         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 1984        |\n",
            "|    total_timesteps      | 769500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014061093 |\n",
            "|    clip_fraction        | 0.145       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.7       |\n",
            "|    explained_variance   | 0.978       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.98        |\n",
            "|    n_updates            | 13173       |\n",
            "|    policy_gradient_loss | -0.0179     |\n",
            "|    std                  | 172         |\n",
            "|    value_loss           | 26.3        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.42e+03   |\n",
            "|    ep_rew_mean          | 1.1e+03    |\n",
            "| time/                   |            |\n",
            "|    fps                  | 388        |\n",
            "|    iterations           | 96         |\n",
            "|    time_elapsed         | 2001       |\n",
            "|    total_timesteps      | 777600     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01650259 |\n",
            "|    clip_fraction        | 0.163      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -64.7      |\n",
            "|    explained_variance   | 0.98       |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.94       |\n",
            "|    n_updates            | 13183      |\n",
            "|    policy_gradient_loss | -0.019     |\n",
            "|    std                  | 173        |\n",
            "|    value_loss           | 21.6       |\n",
            "----------------------------------------\n",
            "Saving agent to checkpoints\\sample\\rl_model_4894839_steps.\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.41e+03    |\n",
            "|    ep_rew_mean          | 1.23e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 389         |\n",
            "|    iterations           | 97          |\n",
            "|    time_elapsed         | 2018        |\n",
            "|    total_timesteps      | 785700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015628368 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.8       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.12        |\n",
            "|    n_updates            | 13193       |\n",
            "|    policy_gradient_loss | -0.0169     |\n",
            "|    std                  | 174         |\n",
            "|    value_loss           | 21.1        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.4e+03     |\n",
            "|    ep_rew_mean          | 1.2e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 389         |\n",
            "|    iterations           | 98          |\n",
            "|    time_elapsed         | 2035        |\n",
            "|    total_timesteps      | 793800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018125936 |\n",
            "|    clip_fraction        | 0.171       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.8       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 44.6        |\n",
            "|    n_updates            | 13203       |\n",
            "|    policy_gradient_loss | -0.0215     |\n",
            "|    std                  | 174         |\n",
            "|    value_loss           | 28.3        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4914840_steps.\n",
            "Selected constant_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.47e+03    |\n",
            "|    ep_rew_mean          | 1.24e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 390         |\n",
            "|    iterations           | 99          |\n",
            "|    time_elapsed         | 2053        |\n",
            "|    total_timesteps      | 801900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016918737 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.8       |\n",
            "|    explained_variance   | 0.986       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.28        |\n",
            "|    n_updates            | 13213       |\n",
            "|    policy_gradient_loss | -0.0204     |\n",
            "|    std                  | 174         |\n",
            "|    value_loss           | 16.4        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 1.4e+03    |\n",
            "|    ep_rew_mean          | 1.27e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 391        |\n",
            "|    iterations           | 100        |\n",
            "|    time_elapsed         | 2069       |\n",
            "|    total_timesteps      | 810000     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01337618 |\n",
            "|    clip_fraction        | 0.135      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -64.9      |\n",
            "|    explained_variance   | 0.988      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 3.25       |\n",
            "|    n_updates            | 13223      |\n",
            "|    policy_gradient_loss | -0.0205    |\n",
            "|    std                  | 175        |\n",
            "|    value_loss           | 22.9       |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.38e+03    |\n",
            "|    ep_rew_mean          | 1.35e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 392         |\n",
            "|    iterations           | 101         |\n",
            "|    time_elapsed         | 2086        |\n",
            "|    total_timesteps      | 818100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015551485 |\n",
            "|    clip_fraction        | 0.159       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -64.9       |\n",
            "|    explained_variance   | 0.987       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.63        |\n",
            "|    n_updates            | 13233       |\n",
            "|    policy_gradient_loss | -0.0185     |\n",
            "|    std                  | 176         |\n",
            "|    value_loss           | 18.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4934841_steps.\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.42e+03    |\n",
            "|    ep_rew_mean          | 1.29e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 392         |\n",
            "|    iterations           | 102         |\n",
            "|    time_elapsed         | 2103        |\n",
            "|    total_timesteps      | 826200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018366504 |\n",
            "|    clip_fraction        | 0.179       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65         |\n",
            "|    explained_variance   | 0.991       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.03        |\n",
            "|    n_updates            | 13243       |\n",
            "|    policy_gradient_loss | -0.023      |\n",
            "|    std                  | 177         |\n",
            "|    value_loss           | 14.5        |\n",
            "-----------------------------------------\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.36e+03    |\n",
            "|    ep_rew_mean          | 1.29e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 393         |\n",
            "|    iterations           | 103         |\n",
            "|    time_elapsed         | 2119        |\n",
            "|    total_timesteps      | 834300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013754958 |\n",
            "|    clip_fraction        | 0.128       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65         |\n",
            "|    explained_variance   | 0.983       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.99        |\n",
            "|    n_updates            | 13253       |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    std                  | 178         |\n",
            "|    value_loss           | 22.3        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4954842_steps.\n",
            "Selected sb3_agent2\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.4e+03     |\n",
            "|    ep_rew_mean          | 1.3e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 394         |\n",
            "|    iterations           | 104         |\n",
            "|    time_elapsed         | 2135        |\n",
            "|    total_timesteps      | 842400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016329758 |\n",
            "|    clip_fraction        | 0.153       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65         |\n",
            "|    explained_variance   | 0.969       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.64        |\n",
            "|    n_updates            | 13263       |\n",
            "|    policy_gradient_loss | -0.0215     |\n",
            "|    std                  | 178         |\n",
            "|    value_loss           | 30.2        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.39e+03    |\n",
            "|    ep_rew_mean          | 1.28e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 394         |\n",
            "|    iterations           | 105         |\n",
            "|    time_elapsed         | 2156        |\n",
            "|    total_timesteps      | 850500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016455937 |\n",
            "|    clip_fraction        | 0.173       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.1       |\n",
            "|    explained_variance   | 0.992       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.0766      |\n",
            "|    n_updates            | 13273       |\n",
            "|    policy_gradient_loss | -0.021      |\n",
            "|    std                  | 179         |\n",
            "|    value_loss           | 15.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.39e+03    |\n",
            "|    ep_rew_mean          | 1.25e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 394         |\n",
            "|    iterations           | 106         |\n",
            "|    time_elapsed         | 2177        |\n",
            "|    total_timesteps      | 858600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014882306 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.2       |\n",
            "|    explained_variance   | 0.971       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.18        |\n",
            "|    n_updates            | 13283       |\n",
            "|    policy_gradient_loss | -0.0181     |\n",
            "|    std                  | 181         |\n",
            "|    value_loss           | 15.8        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\sample\\rl_model_4974843_steps.\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.37e+03    |\n",
            "|    ep_rew_mean          | 1.26e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 394         |\n",
            "|    iterations           | 107         |\n",
            "|    time_elapsed         | 2196        |\n",
            "|    total_timesteps      | 866700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017445853 |\n",
            "|    clip_fraction        | 0.185       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.3       |\n",
            "|    explained_variance   | 0.96        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.62        |\n",
            "|    n_updates            | 13293       |\n",
            "|    policy_gradient_loss | -0.0207     |\n",
            "|    std                  | 182         |\n",
            "|    value_loss           | 11.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.34e+03    |\n",
            "|    ep_rew_mean          | 1.24e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 394         |\n",
            "|    iterations           | 108         |\n",
            "|    time_elapsed         | 2217        |\n",
            "|    total_timesteps      | 874800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014716838 |\n",
            "|    clip_fraction        | 0.142       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.3       |\n",
            "|    explained_variance   | 0.969       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.08        |\n",
            "|    n_updates            | 13303       |\n",
            "|    policy_gradient_loss | -0.0197     |\n",
            "|    std                  | 183         |\n",
            "|    value_loss           | 16.3        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_4994844_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.32e+03    |\n",
            "|    ep_rew_mean          | 1.15e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 395         |\n",
            "|    iterations           | 109         |\n",
            "|    time_elapsed         | 2234        |\n",
            "|    total_timesteps      | 882900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014585856 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.3       |\n",
            "|    explained_variance   | 0.99        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.933       |\n",
            "|    n_updates            | 13313       |\n",
            "|    policy_gradient_loss | -0.0205     |\n",
            "|    std                  | 184         |\n",
            "|    value_loss           | 19.4        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent2\n",
            "Selected sb3_agent1\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.32e+03    |\n",
            "|    ep_rew_mean          | 1.15e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 395         |\n",
            "|    iterations           | 110         |\n",
            "|    time_elapsed         | 2253        |\n",
            "|    total_timesteps      | 891000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015069434 |\n",
            "|    clip_fraction        | 0.152       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.4       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.35        |\n",
            "|    n_updates            | 13323       |\n",
            "|    policy_gradient_loss | -0.0202     |\n",
            "|    std                  | 185         |\n",
            "|    value_loss           | 22.8        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 1.38e+03     |\n",
            "|    ep_rew_mean          | 1.16e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 395          |\n",
            "|    iterations           | 111          |\n",
            "|    time_elapsed         | 2271         |\n",
            "|    total_timesteps      | 899100       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0135828275 |\n",
            "|    clip_fraction        | 0.146        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -65.4        |\n",
            "|    explained_variance   | 0.989        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 0.799        |\n",
            "|    n_updates            | 13333        |\n",
            "|    policy_gradient_loss | -0.0177      |\n",
            "|    std                  | 185          |\n",
            "|    value_loss           | 14.6         |\n",
            "------------------------------------------\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_5014845_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected constant_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.4e+03     |\n",
            "|    ep_rew_mean          | 1.13e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 395         |\n",
            "|    iterations           | 112         |\n",
            "|    time_elapsed         | 2291        |\n",
            "|    total_timesteps      | 907200      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016894368 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.4       |\n",
            "|    explained_variance   | 0.995       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.1         |\n",
            "|    n_updates            | 13343       |\n",
            "|    policy_gradient_loss | -0.0193     |\n",
            "|    std                  | 186         |\n",
            "|    value_loss           | 13.4        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.43e+03    |\n",
            "|    ep_rew_mean          | 1.17e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 396         |\n",
            "|    iterations           | 113         |\n",
            "|    time_elapsed         | 2309        |\n",
            "|    total_timesteps      | 915300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015049802 |\n",
            "|    clip_fraction        | 0.151       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.5       |\n",
            "|    explained_variance   | 0.994       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 35.4        |\n",
            "|    n_updates            | 13353       |\n",
            "|    policy_gradient_loss | -0.0154     |\n",
            "|    std                  | 186         |\n",
            "|    value_loss           | 21.2        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\sample\\rl_model_5034846_steps.\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.44e+03    |\n",
            "|    ep_rew_mean          | 1.17e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 396         |\n",
            "|    iterations           | 114         |\n",
            "|    time_elapsed         | 2328        |\n",
            "|    total_timesteps      | 923400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013781022 |\n",
            "|    clip_fraction        | 0.135       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.5       |\n",
            "|    explained_variance   | 0.973       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 17.8        |\n",
            "|    n_updates            | 13363       |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    std                  | 187         |\n",
            "|    value_loss           | 29.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.46e+03    |\n",
            "|    ep_rew_mean          | 1.15e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 396         |\n",
            "|    iterations           | 115         |\n",
            "|    time_elapsed         | 2347        |\n",
            "|    total_timesteps      | 931500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015064597 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.5       |\n",
            "|    explained_variance   | 0.982       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.29        |\n",
            "|    n_updates            | 13373       |\n",
            "|    policy_gradient_loss | -0.0203     |\n",
            "|    std                  | 188         |\n",
            "|    value_loss           | 26.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.48e+03    |\n",
            "|    ep_rew_mean          | 1.18e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 397         |\n",
            "|    iterations           | 116         |\n",
            "|    time_elapsed         | 2366        |\n",
            "|    total_timesteps      | 939600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016870148 |\n",
            "|    clip_fraction        | 0.169       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.6       |\n",
            "|    explained_variance   | 0.985       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.46        |\n",
            "|    n_updates            | 13383       |\n",
            "|    policy_gradient_loss | -0.0215     |\n",
            "|    std                  | 188         |\n",
            "|    value_loss           | 16.5        |\n",
            "-----------------------------------------\n",
            "Saving agent to checkpoints\\sample\\rl_model_5054847_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.43e+03    |\n",
            "|    ep_rew_mean          | 1.12e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 397         |\n",
            "|    iterations           | 117         |\n",
            "|    time_elapsed         | 2386        |\n",
            "|    total_timesteps      | 947700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022383794 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.6       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.741       |\n",
            "|    n_updates            | 13393       |\n",
            "|    policy_gradient_loss | -0.0217     |\n",
            "|    std                  | 189         |\n",
            "|    value_loss           | 15.1        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected sb3_agent2\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.49e+03    |\n",
            "|    ep_rew_mean          | 1.14e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 397         |\n",
            "|    iterations           | 118         |\n",
            "|    time_elapsed         | 2405        |\n",
            "|    total_timesteps      | 955800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012621767 |\n",
            "|    clip_fraction        | 0.124       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.7       |\n",
            "|    explained_variance   | 0.98        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.46        |\n",
            "|    n_updates            | 13403       |\n",
            "|    policy_gradient_loss | -0.0156     |\n",
            "|    std                  | 191         |\n",
            "|    value_loss           | 34.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\sample\\rl_model_5074848_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 1.53e+03    |\n",
            "|    ep_rew_mean          | 1.11e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 396         |\n",
            "|    iterations           | 119         |\n",
            "|    time_elapsed         | 2429        |\n",
            "|    total_timesteps      | 963900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.020546269 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -65.7       |\n",
            "|    explained_variance   | 0.968       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 0.666       |\n",
            "|    n_updates            | 13413       |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    std                  | 191         |\n",
            "|    value_loss           | 22.7        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n"
          ]
        }
      ],
      "source": [
        "### NOTE ####\n",
        "### CHANGE THE CHECKPOINT FILE NAME STEPS TO 0 ###\n",
        "### NOTE ####\n",
        "\n",
        "# 1. Create agents\n",
        "my_agent = SubmittedAgent(file_path=\"checkpoints/sample/rl_model_0_steps\", device=\"cpu\")\n",
        "\n",
        "SAVE_CONFIG = {\n",
        "    \"save_freq\": 20_000,\n",
        "    \"max_saved\": 3,\n",
        "    \"save_path\": \"checkpoints\",\n",
        "}\n",
        "\n",
        "# 2. Training setup\n",
        "reward_functions = {\n",
        "    \"lives_advantage\": RewTerm(func=RewardFunctions.lives_advantage, weight=1.5),\n",
        "    \"damage_advantage\": RewTerm(func=RewardFunctions.damage_advantage, weight=3),\n",
        "    # \"keyboard_efficiency\": RewTerm(\n",
        "    #     func=RewardFunctions.keyboard_efficiency, weight=0.25\n",
        "    # ),\n",
        "    \"health_disadvantage\": RewTerm(\n",
        "        func=RewardFunctions.health_disadvantage_time, weight=1.5\n",
        "    ),\n",
        "    # \"tactical_positioning\": RewTerm(\n",
        "    #     func=RewardFunctions.tactical_positioning, weight=1\n",
        "    # ),\n",
        "}\n",
        "signal_subscriptions = {\n",
        "    \"on_win\": (\"win_signal\", RewTerm(func=RewardFunctions.on_win_reward, weight=10)),\n",
        "    \"on_knockout\": (\n",
        "        \"knockout_signal\",\n",
        "        RewTerm(func=RewardFunctions.on_knockout_reward, weight=10),\n",
        "    ),\n",
        "}\n",
        "reward_manager = RewardManager(reward_functions, signal_subscriptions)\n",
        "\n",
        "selfplay_handler = SelfPlayHandler(\n",
        "    partial(SubmittedAgent), mode=SelfPlaySelectionMode.LATEST\n",
        ")\n",
        "\n",
        "opponents = {\n",
        "    \"self_play\": (0.8, selfplay_handler),\n",
        "    \"constant_agent\": (0.02, partial(ConstantAgent)),\n",
        "    \"based_agent\": (0.05, partial(BasedAgent)),\n",
        "    \"random_agent\": (0.03, partial(RandomAgent)),\n",
        "    \"sb3_agent1\": (\n",
        "        0.05,\n",
        "        partial(\n",
        "            SB3Agent,\n",
        "            file_path=\"checkpoints/sb3_1/rl_model_10850567_steps\",\n",
        "        ),\n",
        "    ),\n",
        "    \"sb3_agent2\": (\n",
        "        0.05,\n",
        "        partial(\n",
        "            SB3Agent,\n",
        "            file_path=\"checkpoints/sb3_2/rl_model_1104401_steps\",\n",
        "        ),\n",
        "    ),\n",
        "}\n",
        "opponent_cfg = OpponentsCfg(opponents=opponents)\n",
        "\n",
        "save_handler = SaveHandler(\n",
        "    agent=my_agent,\n",
        "    run_name=\"sample\",\n",
        "    mode=SaveHandlerMode.RESUME,\n",
        "    **SAVE_CONFIG,\n",
        ")\n",
        "\n",
        "train(\n",
        "    my_agent,\n",
        "    reward_manager,\n",
        "    save_handler,\n",
        "    opponent_cfg,\n",
        "    CameraResolution.LOW,\n",
        "    train_timesteps=5_000_000,\n",
        "    train_logging=TrainLogging.PLOT,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jD_M6pfp_Ue"
      },
      "source": [
        "# Evaluation / Inference\n",
        "\n",
        "Test your agents here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktPXnaXsqBIs",
        "outputId": "cd31b4f2-0e72-4881-c91c-04f9c8dcbc18"
      },
      "outputs": [],
      "source": [
        "agent1 = \"sample/rl_model_2185729_steps\"\n",
        "agent2 = \"sample/rl_model_2185729_steps\"\n",
        "\n",
        "agent1 = SubmittedAgent(file_path=f\"checkpoints/{agent1}\")\n",
        "agent2 = SubmittedAgent(file_path=f\"checkpoints/{agent2}\")\n",
        "\n",
        "match_time = 90\n",
        "\n",
        "run_match(\n",
        "    agent1,\n",
        "    agent_2=agent2,\n",
        "    video_path=\"vis-2.mp4\",\n",
        "    resolution=CameraResolution.LOW,\n",
        "    reward_manager=reward_manager,\n",
        "    max_timesteps=30 * match_time,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_5yPp_TRuT4P"
      },
      "outputs": [],
      "source": [
        "# Custom ClockworkAgent\n",
        "action_sheet = [\n",
        "    (20, [\"a\"]),\n",
        "    (15, []),\n",
        "    (10, [\"s\", \"j\"]),\n",
        "    (20, []),\n",
        "    (10, [\"w\", \"k\"]),\n",
        "    (20, []),\n",
        "    # (10, ['w', 'j']),\n",
        "    (10, [\"space\"]),\n",
        "    (1, [\"s\", \"j\"]),\n",
        "    (20, [\"a\"]),\n",
        "    (3, [\"a\", \"j\"]),\n",
        "    (30, []),\n",
        "    (7, [\"d\"]),\n",
        "    (1, [\"a\"]),\n",
        "    (4, [\"a\", \"l\"]),\n",
        "    (1, [\"a\"]),\n",
        "    (4, [\"a\", \"l\"]),\n",
        "    (1, [\"a\"]),\n",
        "    (4, [\"a\", \"k\"]),\n",
        "    (20, []),\n",
        "    (4, [\"d\", \"k\"]),\n",
        "    (20, []),\n",
        "    (15, [\"space\"]),\n",
        "    (5, []),\n",
        "    (15, [\"space\"]),\n",
        "    (5, []),\n",
        "    (15, [\"space\"]),\n",
        "    (5, []),\n",
        "    (15, [\"space\"]),\n",
        "    (5, []),\n",
        "    (15, [\"space\"]),\n",
        "    (5, []),\n",
        "]\n",
        "agent1 = ClockworkAgent(action_sheet=action_sheet)\n",
        "agent2 = ClockworkAgent(action_sheet=action_sheet)\n",
        "run_match(\n",
        "    agent1,\n",
        "    agent_2=agent2,\n",
        "    max_timesteps=40,\n",
        "    video_path=\"vis.mp4\",\n",
        "    resolution=CameraResolution.LOW,\n",
        ")\n",
        "\n",
        "Video(\"vis.mp4\", embed=True, width=800)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vNxUlQXyFNKb",
        "XjJxbq1oq8qj",
        "XL4UJviLrA9D",
        "oFPXjtaYpiPY",
        "z7ekY33Ka_h8",
        "hkU-Jg8iZSTC",
        "p7_2qwFINY3f",
        "JbPFpq8lNXdN",
        "ygKmVZpfsB8c",
        "3o43Fo3ssDPj",
        "yZwI-S1X3P9L",
        "ECELO0ke3LGO",
        "jww67QlGd4GL",
        "GHx_GIg5a8x0",
        "PNweZj-Wc_zc",
        "JwnWPBG_nQeg",
        "j7lJmFQncGw7",
        "UzQW3ZFWjYdP",
        "z_joKiMJlVSC",
        "6YTVrhr0dRvv",
        "OPOX5xjac5XC",
        "d6fV-UuVEsGz",
        "vRxHMA07ZKE0",
        "J_-olBqx4-J7",
        "ZYrlmB7ymkBY",
        "bGcNyfuaZWZL",
        "2khY6i4Iq297",
        "YlEWjV-ere9z",
        "83MTxzRvh58N",
        "bpulsG6R6eGG",
        "MRa5uPgibFAm",
        "WxD2EcJGrjvj",
        "404LO62Oqsqz",
        "tyCz9XRL0tLW",
        "NdErHuFqNSFl",
        "-MMYCnbOrXs6",
        "4b_3pL3gk9gI",
        "i7wH-mhyfDpZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "poet",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
