{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LuOF52AMoQTH"
      },
      "source": [
        "# Welcome to UTMIST AI2!\n",
        "\n",
        "[Technical Guide Notebook](https://colab.research.google.com/drive/1qMs336DclBwdn6JBASa5ioDIfvenW8Ha?usp=sharing#scrollTo=-XAOXXMPTiHJ)\n",
        "\n",
        "[Introductory RL Notebook](https://colab.research.google.com/drive/1JRQFLU5jkMrIJ5cWs3xKEO0e9QKuE0Hi#scrollTo=9UCawVuAI3k0)\n",
        "\n",
        "[Discord Server](https://discord.com/invite/TTGB62BE9U)\n",
        "\n",
        "The link to the **LATEST VERSION** of this Colab will always be [here](https://docs.google.com/document/d/1SvlgQSUMLoO3cNx26hzViZOYVPAGa3ECrUSvUyVKvR4/edit?usp=sharing).\n",
        "\n",
        "Credits:\n",
        "- General Event Organization: Asad, Efe, Andrew, Matthew, Kaden\n",
        "- Notebook code: Kaden, Martin, Andrew\n",
        "- Notebook art/animations: EchoTecho, Andy\n",
        "- Website code: Zain, Sarva, Adam, Aina\n",
        "- Workshops: Jingmin, Asad, Tyler, Wai Lim, Napasorn, Sara, San, Alden\n",
        "- Tournament Server: Ambrose, Doga, Steven\n",
        "- Technical guide + Conference brochure: Matthew, Caitlin, Lucie"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vNxUlQXyFNKb"
      },
      "source": [
        "# PATCH: Run this cell first"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zYnCLbw8FS3x"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading assets.zip...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1F2MJQ5enUPVtyi3s410PUuv8LiWr8qCz\n",
            "To: c:\\Users\\klamb\\OneDrive\\Documents\\PythonCode\\POETS\\assets.zip\n",
            "100%|██████████| 5.01M/5.01M [00:00<00:00, 10.4MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading attacks.zip...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1LAOL8sYCUfsCk3TEA3vvyJCLSl0EdwYB\n",
            "To: c:\\Users\\klamb\\OneDrive\\Documents\\PythonCode\\POETS\\attacks.zip\n",
            "100%|██████████| 9.24k/9.24k [00:00<00:00, 9.24MB/s]\n"
          ]
        }
      ],
      "source": [
        "# Delete assets.zip and /content/assets/\n",
        "import shutil, gdown, os\n",
        "import zipfile\n",
        "\n",
        "if os.path.exists('assets'):\n",
        "    shutil.rmtree('assets')\n",
        "if os.path.exists('assets.zip'):\n",
        "    os.remove('assets.zip')\n",
        "\n",
        "# Redownload from Drive\n",
        "data_path = \"assets.zip\"\n",
        "print(\"Downloading assets.zip...\")\n",
        "url = \"https://drive.google.com/file/d/1F2MJQ5enUPVtyi3s410PUuv8LiWr8qCz/view?usp=sharing\"\n",
        "gdown.download(url, output=data_path, fuzzy=True)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')\n",
        "\n",
        "# Delete attacks.zip and /content/attacks/\n",
        "if os.path.exists('attacks'):\n",
        "    shutil.rmtree('attacks')\n",
        "if os.path.exists('attacks.zip'):\n",
        "    os.remove('attacks.zip')\n",
        "\n",
        "# Redownload from Drive\n",
        "data_path = \"attacks.zip\"\n",
        "print(\"Downloading attacks.zip...\")\n",
        "url = \"https://drive.google.com/file/d/1LAOL8sYCUfsCk3TEA3vvyJCLSl0EdwYB/view?usp=sharing\"\n",
        "gdown.download(url, output=data_path, fuzzy=True)\n",
        "\n",
        "# Unzip\n",
        "with zipfile.ZipFile(data_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XL4UJviLrA9D"
      },
      "source": [
        "# pip installs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "0d1cDyIvNRB7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "15"
            ]
          },
          "execution_count": 2,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import gc\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKYrsgxVI0LV"
      },
      "outputs": [],
      "source": [
        "# Download the requirements.txt from Google Drive\n",
        "import gdown, os\n",
        "data_path = \"requirements.txt\"\n",
        "if not os.path.isfile(data_path):\n",
        "    print(\"Downloading requirements.txt...\")\n",
        "    url = \"https://drive.google.com/file/d/1-4f6NGWtejcn6Q9wUETelVXMFWaA5X0D/view?usp=sharing\"\n",
        "    gdown.download(url, output=data_path, fuzzy=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "z8SDSRGrRFkf"
      },
      "outputs": [],
      "source": [
        "# Malachite and RL requirements\n",
        "#!pip install torch==2.4.1 gymnasium pygame==2.6.1 pymunk==6.2.1 scikit-image scikit-video sympy==1.5.1 stable_baselines3 sb3-contrib\n",
        "#!pip install memory_profiler==0.61.0\n",
        "#!pip install torch==2.4.1 triton==3.0.0 gymnasium pygame==2.6.1 pymunk==6.2.1 scikit-image scikit-video sympy==1.5.1 stable_baselines3 sb3-contrib jupyter gdown opencv-python\n",
        "\n",
        "#!pip freeze > /content/requirements_v0.txt\n",
        "# !pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XjJxbq1oq8qj"
      },
      "source": [
        "# Competition Code (DO NOT EDIT)\n",
        "Please **run this cell** to set your Jupyter Notebook up with all necessary code. Then feel free to move to the `SUBMISSION` sections of this notebook for further instruction.\n",
        "\n",
        "Note: This cell may take time to run, as it installs the necessary modules then imports them.\n",
        "\n",
        "## Summary of content:\n",
        "\n",
        "These cells contain our custom Multi-Agent Reinforcement Learning (MARL) Solution, Malachite, alongside an implementation of a 1v1 platform fighter we've titled Warehouse Brawl. You may look through these cells to get a better sense for the dynamics and functionality of the enviroment, as wel as the various pip installs and modules available for use in your own code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFPXjtaYpiPY"
      },
      "source": [
        "## Malachite (DO NOT MODIFY UNLESS YOU KNOW WHAT YOU'RE DOING)\n",
        "The following cells store some code for our custom Multi-Agent Reinforcement Learning (MARL) Solution, called Malachite. It extends some Stable-Baselines 3 functionality to Multi-Agent systems in the context of the AI2 Tournament.\n",
        "\n",
        "You would only want to modify this if you want to add custom rewards and are dissatisfied with the current flexible rewards system. That's ok! But note that this default environment is what will be used in the tournament, and you will **NOT** have access to any additional data or modifications you may choose to make here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z7ekY33Ka_h8"
      },
      "source": [
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "RWDiccN4uNeo"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "from typing import TYPE_CHECKING, Any, Generic, \\\n",
        " SupportsFloat, TypeVar, Type, Optional, List, Dict, Callable\n",
        "from enum import Enum, auto\n",
        "from abc import ABC, abstractmethod\n",
        "from dataclasses import dataclass, field, MISSING\n",
        "from collections import defaultdict\n",
        "from functools import partial\n",
        "from typing import Tuple, Any\n",
        "\n",
        "from PIL import Image, ImageSequence\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import gdown, os, math, random, shutil, json\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import nn\n",
        "\n",
        "import gymnasium\n",
        "from gymnasium import spaces\n",
        "\n",
        "import pygame\n",
        "import pygame.gfxdraw\n",
        "import pymunk\n",
        "import pymunk.pygame_util\n",
        "from pymunk.space_debug_draw_options import SpaceDebugColor\n",
        "from pymunk.vec2d import Vec2d\n",
        "\n",
        "import cv2\n",
        "import skimage.transform as st\n",
        "import skvideo\n",
        "import skvideo.io\n",
        "from IPython.display import Video"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EUO-SS6bCRS"
      },
      "source": [
        "### MalachiteEnv Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t0TsFsnsp0mD"
      },
      "outputs": [],
      "source": [
        "ObsType = TypeVar(\"ObsType\")\n",
        "ActType = TypeVar(\"ActType\")\n",
        "AgentID = TypeVar(\"AgentID\")\n",
        "\n",
        "# Reference PettingZoo AECEnv\n",
        "class MalachiteEnv(ABC, Generic[ObsType, ActType, AgentID]):\n",
        "\n",
        "    agents: list[AgentID]\n",
        "\n",
        "    action_spaces: dict[AgentID, gymnasium.spaces.Space]\n",
        "    observation_spaces: dict[\n",
        "        AgentID, gymnasium.spaces.Space\n",
        "    ]\n",
        "\n",
        "    # Whether each agent has just reached a terminal state\n",
        "    terminations: dict[AgentID, bool]\n",
        "    truncations: dict[AgentID, bool]\n",
        "    rewards: dict[AgentID, float]  # Reward from the last step for each agent\n",
        "    # Cumulative rewards for each agent\n",
        "    _cumulative_rewards: dict[AgentID, float]\n",
        "    infos: dict[\n",
        "        AgentID, dict[str, Any]\n",
        "    ]  # Additional information from the last step for each agent\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def step(self, action: dict[AgentID, ActType]) -> tuple[ObsType,]:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def reset(self, seed: int | None = None, options: dict | None = None) -> None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def observe(self, agent: AgentID) -> ObsType | None:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def render(self) -> None | np.ndarray | str | list:\n",
        "        pass\n",
        "\n",
        "    @abstractmethod\n",
        "    def close(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def show_image(self, image: np.ndarray) -> None:\n",
        "        plt.imshow(image)\n",
        "        plt.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "    def observation_space(self, agent: AgentID) -> gymnasium.spaces.Space:\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def action_space(self, agent: AgentID) -> gymnasium.spaces.Space:\n",
        "        return self.action_spaces[agent]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBfJLZ-jpf8M"
      },
      "source": [
        "## Environment (DO NOT MODIFY)\n",
        "Defines the environment for the game. Code adapted from the following sources:\n",
        "- [Shootout AI](https://github.com/ajwm8103/shootoutai/tree/main)\n",
        "- [Diffusion Policy](https://diffusion-policy.cs.columbia.edu/)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkU-Jg8iZSTC"
      },
      "source": [
        "### Low High Class\n",
        "Helps structure observation and action spaces."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "3QHEnEBSZRky"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class ActHelper():\n",
        "    low: list[Any] = field(default_factory=list)\n",
        "    high: list[Any] = field(default_factory=list)\n",
        "    sections: Dict[str, int] = field(default_factory=dict)\n",
        "\n",
        "    def get_as_np(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Return the low and high bounds as NumPy arrays.\"\"\"\n",
        "        return np.array(self.low), np.array(self.high)\n",
        "\n",
        "    def get_as_box(self) -> spaces.Box:\n",
        "        lowarray, higharray = self.get_as_np()\n",
        "        return spaces.Box(\n",
        "            low=lowarray,\n",
        "            high=higharray,\n",
        "            shape=lowarray.shape,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def zeros(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns a zeros vector with the same total dimension as defined by the low vector.\n",
        "        \"\"\"\n",
        "        return np.zeros(len(self.low))\n",
        "\n",
        "    def add_key(self, name: str):\n",
        "        \"\"\"\n",
        "        Adds a new section with a label to the overall low and high lists.\n",
        "\n",
        "        Parameters:\n",
        "            name: A string that identifies the section (e.g., \"global_position\").\n",
        "            low_values: A list of low values for this section.\n",
        "            high_values: A list of high values for this section.\n",
        "\n",
        "        The method appends the values to the overall lists and records the indices\n",
        "        where this section is stored. This is later used for observation parsing.\n",
        "        \"\"\"\n",
        "        name = name.lower()\n",
        "        self.low += [0]\n",
        "        self.high += [1]\n",
        "        self.sections[name] = len(self.low)-1\n",
        "\n",
        "    def press_keys(self, keys: str | List[str], action: Optional[np.ndarray]=None) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Set a part of the action vector corresponding to the named section.\n",
        "\n",
        "        Parameters:\n",
        "            action: The full action vector (np.ndarray) that will be modified.\n",
        "            partial_action: The values to set for the section.\n",
        "            name: The section name whose slice is to be replaced.\n",
        "\n",
        "        Returns:\n",
        "            The updated action vector.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If the partial action's size does not match the section size.\n",
        "        \"\"\"\n",
        "        if isinstance(keys, str):\n",
        "            keys = [keys]\n",
        "        if action is None:\n",
        "            action = self.zeros()\n",
        "\n",
        "        for key in keys:\n",
        "            key = key.lower()\n",
        "            if key not in self.sections:\n",
        "                raise KeyError(f\"Key '{key}' not found in keys: {self.sections.keys()}\")\n",
        "            action[self.sections[key]] = 1\n",
        "        return action\n",
        "\n",
        "    def print_all_sections(self) -> None:\n",
        "        \"\"\"\n",
        "        Prints the names and indices of all sections.\n",
        "        \"\"\"\n",
        "        for name, (start, end) in self.sections.items():\n",
        "            print(f\"{name}: {end - start}\")\n",
        "\n",
        "@dataclass\n",
        "class ObsHelper():\n",
        "    low: list[Any] = field(default_factory=list)\n",
        "    high: list[Any] = field(default_factory=list)\n",
        "    sections: Dict[str, Tuple[int, int]] = field(default_factory=dict)\n",
        "\n",
        "    def get_as_np(self) -> Tuple[np.ndarray, np.ndarray]:\n",
        "        \"\"\"Return the low and high bounds as NumPy arrays.\"\"\"\n",
        "        return np.array(self.low), np.array(self.high)\n",
        "\n",
        "    def get_as_box(self) -> spaces.Box:\n",
        "        lowarray, higharray = self.get_as_np()\n",
        "        return spaces.Box(\n",
        "            low=lowarray,\n",
        "            high=higharray,\n",
        "            shape=lowarray.shape,\n",
        "            dtype=np.float32\n",
        "        )\n",
        "\n",
        "    def zeros(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns a zeros vector with the same total dimension as defined by the low vector.\n",
        "        \"\"\"\n",
        "        return np.zeros(len(self.low))\n",
        "\n",
        "    def add_section(self, low_values: List[Any], high_values: List[Any], name: str) :\n",
        "        \"\"\"\n",
        "        Adds a new section with a label to the overall low and high lists.\n",
        "\n",
        "        Parameters:\n",
        "            name: A string that identifies the section (e.g., \"global_position\").\n",
        "            low_values: A list of low values for this section.\n",
        "            high_values: A list of high values for this section.\n",
        "\n",
        "        The method appends the values to the overall lists and records the indices\n",
        "        where this section is stored. This is later used for observation parsing.\n",
        "        \"\"\"\n",
        "        name = name.lower()\n",
        "        start_idx = len(self.low)  # Starting index for this section.\n",
        "        self.low += low_values\n",
        "        self.high += high_values\n",
        "        end_idx = len(self.low)    # Ending index (exclusive) for this section.\n",
        "        self.sections[name] = (start_idx, end_idx)\n",
        "\n",
        "    def get_section(self, obs: np.ndarray, name: str) -> np.ndarray:\n",
        "        start, end = self.sections[name]\n",
        "        return obs[start:end]\n",
        "\n",
        "    def print_all_sections(self) -> None:\n",
        "        \"\"\"\n",
        "        Prints the names and indices of all sections.\n",
        "        \"\"\"\n",
        "        for name, (start, end) in self.sections.items():\n",
        "            print(f\"{name}: {end - start}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7_2qwFINY3f"
      },
      "source": [
        "### KeyIconPanel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "HyX2pwmPNaEB"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "import numpy as np\n",
        "\n",
        "class KeyIconPanel():\n",
        "    def __init__(self, side: str, edge_percentage: float,\n",
        "                 width_percentage: float, height_percentage: float,\n",
        "                 font_size: int = 12):\n",
        "        \"\"\"\n",
        "        :param side: \"left\" or \"right\". Determines which edge (far left or far right) is positioned at the given percentage.\n",
        "        :param edge_percentage: Fraction of the screen width at which the far edge of the panel is placed.\n",
        "                                For \"left\", this is the left edge; for \"right\", this is the right edge.\n",
        "        :param width_percentage: Panel width as a fraction of screen width.\n",
        "        :param height_percentage: Panel height as a fraction of screen height.\n",
        "        :param font_size: Font size for the key labels.\n",
        "        \"\"\"\n",
        "        self.side = side.lower()\n",
        "        self.edge_percentage = edge_percentage\n",
        "        self.width_percentage = width_percentage\n",
        "        self.height_percentage = height_percentage\n",
        "        self.font_size = font_size\n",
        "        # Define the keys in order: first 4 (W, A, S, D), then space, then 5 (G, H, J, K, L)\n",
        "        self.keys = [\"W\", \"A\", \"S\", \"D\", \"Space\", \"G\", \"H\", \"J\", \"K\", \"L\"]\n",
        "\n",
        "    def draw_key_icon(self, surface, rect: pygame.Rect, key_label: str, pressed: bool, font):\n",
        "        \"\"\"\n",
        "        Draws a key icon in the specified rect.\n",
        "          - Draws a rectangle with a 2-pixel border.\n",
        "          - If pressed, the border and text are red; if not, they are white.\n",
        "        \"\"\"\n",
        "        color = (255, 0, 0) if pressed else (255, 255, 255)\n",
        "        # Draw the rectangle outline\n",
        "        pygame.draw.rect(surface, color, rect, 1)\n",
        "        # Render the key label (centered)\n",
        "        text_surface = font.render(key_label, True, color)\n",
        "        text_rect = text_surface.get_rect(center=rect.center)\n",
        "        surface.blit(text_surface, text_rect)\n",
        "\n",
        "    def draw(self, camera, input_vector: np.ndarray):\n",
        "        \"\"\"\n",
        "        Draws the panel and key icons onto the given canvas.\n",
        "\n",
        "        :param canvas: The pygame.Surface on which to draw.\n",
        "        :param screen_size: Tuple (screen_width, screen_height).\n",
        "        :param input_vector: np.ndarray of booleans or 0/1 with length 10 in the order [W, A, S, D, Space, G, H, J, K, L].\n",
        "        \"\"\"\n",
        "        canvas = camera.canvas\n",
        "        screen_width, screen_height = camera.window_width, camera.window_height\n",
        "\n",
        "        # Calculate panel dimensions\n",
        "        panel_width = screen_width * self.width_percentage\n",
        "        panel_height = screen_height * self.height_percentage\n",
        "\n",
        "        # Determine panel x based on side\n",
        "        if self.side == \"left\":\n",
        "            x = screen_width * self.edge_percentage\n",
        "        elif self.side == \"right\":\n",
        "            x = screen_width * self.edge_percentage - panel_width\n",
        "        else:\n",
        "            # Default to centered horizontally if side is invalid.\n",
        "            x = (screen_width - panel_width) / 2\n",
        "\n",
        "        # For vertical placement, we'll position the panel at 10% from the top.\n",
        "        y = screen_height * 0.2\n",
        "        panel_rect = pygame.Rect(int(x), int(y), int(panel_width), int(panel_height))\n",
        "        # Draw panel background and border\n",
        "        pygame.draw.rect(canvas, (50, 50, 50), panel_rect)  # dark gray background\n",
        "        pygame.draw.rect(canvas, (255, 255, 255), panel_rect, 2)  # white border\n",
        "\n",
        "        # Create a font for the key icons.\n",
        "        font = pygame.font.Font(None, self.font_size)\n",
        "        # Divide the panel vertically into 3 rows.\n",
        "        row_height = panel_rect.height / 3\n",
        "\n",
        "        # Row 1: WASD (first 4 keys)\n",
        "        row1_keys = self.keys[0:4]\n",
        "        row1_count = len(row1_keys)\n",
        "        for idx, key in enumerate(row1_keys):\n",
        "            cell_width = panel_rect.width / row1_count\n",
        "            cell_rect = pygame.Rect(\n",
        "                panel_rect.x + idx * cell_width,\n",
        "                panel_rect.y,\n",
        "                cell_width,\n",
        "                row_height\n",
        "            )\n",
        "            # Add padding for the icon.\n",
        "            icon_rect = cell_rect.inflate(-2, -2)\n",
        "            pressed = input_vector[idx] > 0.5\n",
        "            self.draw_key_icon(canvas, icon_rect, key, pressed, font)\n",
        "\n",
        "        # Row 2: Spacebar (only one icon)\n",
        "        cell_rect = pygame.Rect(\n",
        "            panel_rect.x,\n",
        "            panel_rect.y + row_height,\n",
        "            panel_rect.width,\n",
        "            row_height\n",
        "        )\n",
        "        # Center the spacebar icon in its cell.\n",
        "        icon_rect = cell_rect.inflate(-2, -2)\n",
        "        pressed = input_vector[4] > 0.5\n",
        "        self.draw_key_icon(canvas, icon_rect, \"Space\", pressed, font)\n",
        "\n",
        "        # Row 3: GHJKL (last 5 keys)\n",
        "        row3_keys = self.keys[5:10]\n",
        "        row3_count = len(row3_keys)\n",
        "        for idx, key in enumerate(row3_keys):\n",
        "            cell_width = panel_rect.width / row3_count\n",
        "            cell_rect = pygame.Rect(\n",
        "                panel_rect.x + idx * cell_width,\n",
        "                panel_rect.y + 2 * row_height,\n",
        "                cell_width,\n",
        "                row_height\n",
        "            )\n",
        "            icon_rect = cell_rect.inflate(-2, -2)\n",
        "            pressed = input_vector[5 + idx] > 0.5\n",
        "            self.draw_key_icon(canvas, icon_rect, key, pressed, font)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbPFpq8lNXdN"
      },
      "source": [
        "### UIHandler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "_OpSupi63L1S"
      },
      "outputs": [],
      "source": [
        "class UIHandler():\n",
        "\n",
        "    def __init__(self, camera):\n",
        "        # Score images\n",
        "\n",
        "        SCALE_FACTOR = 0.11\n",
        "        self.agent_1_score = pygame.image.load('assets/ui/player1ui.png')\n",
        "        self.agent_1_score = pygame.transform.scale(self.agent_1_score, (int(SCALE_FACTOR * self.agent_1_score.get_width()), int(SCALE_FACTOR * self.agent_1_score.get_height())))\n",
        "        self.agent_2_score = pygame.image.load('assets/ui/player2ui.png')\n",
        "        self.agent_2_score = pygame.transform.scale(self.agent_2_score, (int(SCALE_FACTOR * self.agent_2_score.get_width()), int(SCALE_FACTOR * self.agent_2_score.get_height())))\n",
        "\n",
        "        # Life and death images\n",
        "        SCALE_FACTOR_2 = SCALE_FACTOR * 0.375\n",
        "        self.life = pygame.image.load('assets/ui/alicon_alive.png')\n",
        "        self.life = pygame.transform.scale(self.life, (int(SCALE_FACTOR_2 * self.life.get_width()), int(SCALE_FACTOR_2 * self.life.get_height())))\n",
        "        self.death = pygame.image.load('assets/ui/alicon_dead.png')\n",
        "        self.death = pygame.transform.scale(self.death, (int(SCALE_FACTOR_2 * self.death.get_width()), int(SCALE_FACTOR_2 * self.death.get_height())))\n",
        "\n",
        "        self.score_width, self.score_height = self.agent_1_score.get_size()\n",
        "        self.agent_1_score_pos = (10, -10)  # Top-left\n",
        "        self.agent_2_score_pos = (camera.window_width - self.score_width - 10, -10)  # Top-right\n",
        "\n",
        "    def render(self, camera, env):\n",
        "        canvas = camera.canvas\n",
        "\n",
        "        # Score UI positions\n",
        "\n",
        "\n",
        "        # Draw Score UI\n",
        "\n",
        "        canvas.blit(self.agent_1_score, self.agent_1_score_pos)\n",
        "        canvas.blit(self.agent_2_score, self.agent_2_score_pos)\n",
        "\n",
        "        # Agent lives\n",
        "        spacing = self.score_width / 3\n",
        "        for i in range(len(env.players)):\n",
        "            for j in range(env.players[i].stocks):\n",
        "                canvas.blit(self.life, (10+j*spacing + i*(camera.window_width - 1.2 * self.score_width), self.score_height - 30))\n",
        "\n",
        "            # Agent deaths\n",
        "            for j in range(3 - env.players[i].stocks):\n",
        "                canvas.blit(self.death, (10 + 2*spacing - j*spacing + i*(camera.window_width - 1.2 * self.score_width), self.score_height - 30))\n",
        "\n",
        "        self.display_percentages(camera, env)\n",
        "        self.display_team_name(camera, env)\n",
        "\n",
        "    def display_team_name(self, camera, env):\n",
        "        # Define the team name and the bounding rectangle for the text.\n",
        "        team_name = \"Testing this team name\"\n",
        "        # These values can be adjusted to suit your UI layout:\n",
        "        team_rect_1 = pygame.Rect(self.agent_1_score_pos[0] + 0.2 * self.score_width,\n",
        "                                self.agent_1_score_pos[1] + 0.75 * self.score_height,\n",
        "                                0.8 * self.score_width,\n",
        "                                0.2 * self.score_height)\n",
        "        team_rect_2 = pygame.Rect(self.agent_2_score_pos[0] + 0 * self.score_width,\n",
        "                                self.agent_2_score_pos[1] + 0.75 * self.score_height,\n",
        "                                0.8 * self.score_width,\n",
        "                                0.2 * self.score_height)\n",
        "        team_rects = [team_rect_1, team_rect_2]\n",
        "\n",
        "        # Create a font (same as used for percentages or adjust as needed)\n",
        "        font = pygame.font.Font(None, 20)\n",
        "\n",
        "        for i, team_rect in enumerate(team_rects):\n",
        "            # Render the team name and check if it fits in the rectangle.\n",
        "            text = env.agent_1_name if i == 0 else env.agent_2_name\n",
        "            text_surface = font.render(text, True, (255, 255, 255))\n",
        "\n",
        "            # If the text is too wide, shorten it and add an ellipsis.\n",
        "            if text_surface.get_width() > team_rect.width:\n",
        "                # Remove characters until it fits, then add ellipsis.\n",
        "                while text_surface.get_width() > team_rect.width and len(text) > 0:\n",
        "                    text = text[:-1]\n",
        "                    text_surface = font.render(text + \"...\", True, (255, 255, 255))\n",
        "                text = text + \"...\"\n",
        "                text_surface = font.render(text, True, (255, 255, 255))\n",
        "\n",
        "            # Draw a red rectangle outline for the team name.\n",
        "            pygame.draw.rect(camera.canvas, (255, 0, 0), team_rect, 2)\n",
        "\n",
        "            # Center the text in the rectangle and draw it.\n",
        "            text_rect = text_surface.get_rect(center=team_rect.center)\n",
        "            camera.canvas.blit(text_surface, text_rect)\n",
        "\n",
        "    # Percentages (like SSBU)\n",
        "    def display_percentages(self, camera, env):\n",
        "        WHITE = (255, 255, 255)\n",
        "        ORANGE = (255, 165, 0)\n",
        "        RED = (255, 0, 0)\n",
        "        YELLOW = (255, 255, 0)\n",
        "        DARK_RED = (139, 0, 0)\n",
        "\n",
        "        # Agent percentage text\n",
        "        font = pygame.font.Font(None, 35)\n",
        "        # render text & text colours:\n",
        "        for i in range(len(env.players)):\n",
        "            COLOUR = WHITE\n",
        "            if 50 < env.players[i].damage < 100:\n",
        "                COLOUR = YELLOW\n",
        "            elif 100 <= env.players[i].damage < 150:\n",
        "                COLOUR = ORANGE\n",
        "            elif 150 <= env.players[i].damage < 200:\n",
        "                COLOUR = RED\n",
        "            elif env.players[i].damage >= 200:\n",
        "                COLOUR = DARK_RED\n",
        "            percentage = env.players[i].damage * 5 / 7\n",
        "            text_surface = font.render(f'{percentage:.1f}%', True, COLOUR)\n",
        "            # text_rect_background = pygame.draw.rect(self.screen, (255,255,255), (220+i*100, 75, 70, 56))\n",
        "            # text_rect_background_border = pygame.draw.rect(self.screen, (0, 0, 0), (220+i*100, 75, 70, 56), 3)\n",
        "            text_rect = text_surface.get_rect(center=(self.score_width + i*(camera.window_width - 2 * self.score_width), self.score_height * 1.5/4))\n",
        "            camera.canvas.blit(text_surface, text_rect)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygKmVZpfsB8c"
      },
      "source": [
        "### Camera"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "CK5erYyisBKw"
      },
      "outputs": [],
      "source": [
        "class CameraResolution(Enum):\n",
        "    LOW = 1\n",
        "    MEDIUM = 2\n",
        "    HIGH = 3\n",
        "\n",
        "class RenderMode(Enum):\n",
        "    NONE = 0\n",
        "    RGB_ARRAY = 1\n",
        "    PYGAME_WINDOW = 2\n",
        "\n",
        "class Camera():\n",
        "    screen_width_tiles: float = 29.8\n",
        "    screen_height_tiles: float = 16.8\n",
        "    pixels_per_tile: float = 43\n",
        "    is_rendering: bool = False\n",
        "    space: pymunk.Space\n",
        "    pos: list[int] = [0,0]\n",
        "    zoom: float = 2.0\n",
        "\n",
        "\n",
        "    def reset(self, env):\n",
        "        self.space = env.space\n",
        "        self.objects = env.objects\n",
        "        self.resolution = env.resolution\n",
        "        self.resolutions = {\n",
        "            CameraResolution.LOW: (480, 720),\n",
        "            CameraResolution.MEDIUM: (720, 1280),\n",
        "            CameraResolution.HIGH: (1080, 1920)\n",
        "        }\n",
        "\n",
        "        self.window_height, self.window_width = self.resolutions[self.resolution]\n",
        "\n",
        "        # WIDTH HEIGHT in Pixels\n",
        "        #screen_width_tiles: float = 29.8\n",
        "        #screen_height_tiles: float = 16.8\n",
        "        self.pixels_per_tile = self.window_width // self.screen_width_tiles\n",
        "\n",
        "        #self.window_width = self.screen_width_tiles * self.pixels_per_tile\n",
        "        #self.window_height = self.screen_height_tiles * self.pixels_per_tile\n",
        "        self.steps = 0\n",
        "\n",
        "    def scale_gtp(self) -> float:\n",
        "        return self.pixels_per_tile * self.zoom\n",
        "\n",
        "    def _setup_render(self, mode) -> None:\n",
        "        pygame.init()\n",
        "\n",
        "        self.ui_handler = UIHandler(self)\n",
        "\n",
        "        self.key_panel_1 = KeyIconPanel(side=\"left\", edge_percentage=0.22, width_percentage=0.12, height_percentage=0.08)\n",
        "        self.key_panel_2 = KeyIconPanel(side=\"right\", edge_percentage=0.78, width_percentage=0.12, height_percentage=0.08)\n",
        "\n",
        "        if mode == RenderMode.PYGAME_WINDOW:\n",
        "            pygame.display.set_caption(\"Env\")\n",
        "            self.canvas = pygame.display.set_mode((self.window_width, self.window_height))\n",
        "            self.clock = pygame.time.Clock()\n",
        "\n",
        "        # Define font\n",
        "        self.font50 = pygame.font.Font(None, 50)  # Use the default font with size 50\n",
        "        self.font = pygame.font.Font(None, 50)\n",
        "\n",
        "    def process(self) -> None:\n",
        "        self.steps += 1\n",
        "\n",
        "    def ptg(self, x, y=None) -> tuple[int, int]:\n",
        "        if isinstance(x, list) or isinstance(x, tuple) or isinstance(x, np.ndarray):\n",
        "            x, y = x\n",
        "        elif isinstance(x, pymunk.Vec2d):\n",
        "            x, y = x.x, x.y\n",
        "\n",
        "        scale_cst = self.scale_gtp()\n",
        "        new_x = -self.screen_width_tiles / 2 + int(x / scale_cst)\n",
        "        new_y = self.screen_height_tiles / 2 - int(y / scale_cst)\n",
        "        return new_x, new_y\n",
        "\n",
        "    def gtp(self, x, y=None) -> tuple[float, float]:\n",
        "        if isinstance(x, list) or isinstance(x, tuple) or isinstance(x, np.ndarray):\n",
        "            x, y = x\n",
        "        elif isinstance(x, pymunk.Vec2d):\n",
        "            x, y = x.x, x.y\n",
        "\n",
        "        scale_cst = self.scale_gtp()\n",
        "        new_x = self.window_width / 2 + (x - self.pos[0]) * scale_cst\n",
        "        new_y = self.window_height / 2 + (y -self.pos[1]) * scale_cst\n",
        "\n",
        "        #new_x = self.window_width / 2 + x * self.pixels_per_tile\n",
        "        #new_y = self.window_height / 2 + y * self.pixels_per_tile\n",
        "        return new_x, new_y\n",
        "\n",
        "    def get_frame(self, env, mode=RenderMode.RGB_ARRAY, has_hitboxes=False):\n",
        "        if not self.is_rendering:\n",
        "            self._setup_render(mode)\n",
        "            self.is_rendering = True\n",
        "\n",
        "\n",
        "        # Expose the canvas for editing\n",
        "        if mode == RenderMode.RGB_ARRAY:\n",
        "            self.canvas = pygame.Surface((self.window_width, self.window_height))\n",
        "        #canvas = pygame.display.set_mode((self.window_width, self.window_height))\n",
        "        self.canvas.fill((0, 0, 0))\n",
        "\n",
        "        # Transform PyMunk objects to have (0,0) at center, and such that units are appropriate\n",
        "        #center_x = self.window_width // 2\n",
        "        #center_y = self.window_height // 2\n",
        "        #scale = self.pixels_per_tile\n",
        "        #transform = pymunk.Transform.identity().translated(center_x, center_y).scaled(scale)\n",
        "\n",
        "        #center_x = self.screen_width_tiles // 2 - self.pos[0]\n",
        "        #center_y = self.screen_height_tiles // 2 - self.pos[1]\n",
        "        center_x = self.window_width // 2\n",
        "        center_y = self.window_height // 2\n",
        "        scale = self.pixels_per_tile * self.zoom\n",
        "        transform = pymunk.Transform.identity().translated(center_x, center_y).scaled(scale).translated(self.pos[0], self.pos[1])\n",
        "        #transform = pymunk.Transform.identity().scaled(scale).translated(center_x, center_y).scaled(self.zoom)\n",
        "        draw_options = DrawOptions(self.canvas)\n",
        "        draw_options.transform = transform\n",
        "\n",
        "        # Draw PyMunk objects\n",
        "        #self.space.debug_draw(draw_options)\n",
        "\n",
        "        #print(self.env.space)\n",
        "        for obj_name, obj in self.objects.items():\n",
        "            obj.render(self.canvas, self)\n",
        "\n",
        "        # Draw UI + Text\n",
        "        env.handle_ui(self.canvas)\n",
        "\n",
        "        self.ui_handler.render(self, env)\n",
        "\n",
        "        if hasattr(env, 'cur_action'):\n",
        "            self.key_panel_1.draw(self, env.cur_action[0])\n",
        "            self.key_panel_2.draw(self, env.cur_action[1])\n",
        "\n",
        "        img = np.transpose(\n",
        "                np.array(pygame.surfarray.pixels3d(self.canvas)), axes=(1, 0, 2)\n",
        "            )\n",
        "\n",
        "        if mode == RenderMode.PYGAME_WINDOW:\n",
        "            pygame.display.flip()\n",
        "            pygame.event.pump()\n",
        "            #pygame.display.update()\n",
        "            self.clock.tick(50)\n",
        "\n",
        "        return img\n",
        "\n",
        "    def close(self) -> None:\n",
        "        pygame.quit()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3o43Fo3ssDPj"
      },
      "source": [
        "### Warehouse Brawl Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Yl2-4QtPoDiW"
      },
      "outputs": [],
      "source": [
        "class Signal():\n",
        "    def __init__(self, env):\n",
        "        self._handlers: List[Callable] = []\n",
        "        self.env = env\n",
        "\n",
        "    def connect(self, handler: Callable):\n",
        "        self._handlers.append(handler)\n",
        "\n",
        "    def emit(self, *args, **kwargs):\n",
        "        for handler in self._handlers:\n",
        "            handler(self.env, *args, **kwargs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "128rJJOGBtkn"
      },
      "outputs": [],
      "source": [
        "class Result(Enum):\n",
        "    WIN = \"win\"\n",
        "    LOSS = \"loss\"\n",
        "    DRAW = \"draw\"\n",
        "\n",
        "@dataclass\n",
        "class PlayerStats():\n",
        "    damage_taken: float\n",
        "    damage_done: float\n",
        "    lives_left: int\n",
        "\n",
        "@dataclass\n",
        "class MatchStats():\n",
        "    match_time: float  # Total match time in seconds\n",
        "    player1: PlayerStats\n",
        "    player2: PlayerStats\n",
        "    player1_result: Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "bFcO-JQsJE4n"
      },
      "outputs": [],
      "source": [
        "# Define an enumeration for the moves\n",
        "class MoveType(Enum):\n",
        "    NONE = auto()         # no move\n",
        "    NLIGHT = auto()       # grounded light neutral\n",
        "    DLIGHT = auto()       # grounded light down\n",
        "    SLIGHT = auto()       # grounded light side\n",
        "    NSIG = auto()         # grounded heavy neutral\n",
        "    DSIG = auto()         # grounded heavy down\n",
        "    SSIG = auto()         # grounded heavy side\n",
        "    NAIR = auto()         # aerial light neutral\n",
        "    DAIR = auto()         # aerial light down\n",
        "    SAIR = auto()         # aerial light side\n",
        "    RECOVERY = auto()     # aerial heavy neutral and aerial heavy side\n",
        "    GROUNDPOUND = auto()  # aerial heavy down\n",
        "\n",
        "    def __int__(self):\n",
        "        return self.value\n",
        "\n",
        "    def __float__(self):\n",
        "        return float(self.value)\n",
        "\n",
        "# Define a frozen dataclass for the key\n",
        "@dataclass(frozen=True)\n",
        "class CompactMoveState():\n",
        "    grounded: bool\n",
        "    heavy: bool\n",
        "    direction_type: int\n",
        "\n",
        "# Create the dictionary mapping CompactMoveState to a Move\n",
        "m_state_to_move = {\n",
        "    CompactMoveState(True, False, 0): MoveType.NLIGHT,      # grounded light neutral\n",
        "    CompactMoveState(True, False, 1): MoveType.DLIGHT,      # grounded light down\n",
        "    CompactMoveState(True, False, 2): MoveType.SLIGHT,      # grounded light side\n",
        "    CompactMoveState(True, True, 0): MoveType.NSIG,          # grounded heavy neutral\n",
        "    CompactMoveState(True, True, 1): MoveType.DSIG,          # grounded heavy down\n",
        "    CompactMoveState(True, True, 2): MoveType.SSIG,          # grounded heavy side\n",
        "    CompactMoveState(False, False, 0): MoveType.NAIR,        # aerial light neutral\n",
        "    CompactMoveState(False, False, 1): MoveType.DAIR,        # aerial light down\n",
        "    CompactMoveState(False, False, 2): MoveType.SAIR,        # aerial light side\n",
        "    CompactMoveState(False, True, 0): MoveType.RECOVERY,     # aerial heavy neutral\n",
        "    CompactMoveState(False, True, 1): MoveType.GROUNDPOUND,  # aerial heavy down\n",
        "    CompactMoveState(False, True, 2): MoveType.RECOVERY,     # aerial heavy side\n",
        "}\n",
        "\n",
        "class Facing(Enum):\n",
        "    RIGHT = 1\n",
        "    LEFT = -1\n",
        "\n",
        "    def __int__(self):\n",
        "        return self.value\n",
        "\n",
        "    @staticmethod\n",
        "    def flip(facing):\n",
        "        return Facing.LEFT if facing == Facing.RIGHT else Facing.RIGHT\n",
        "\n",
        "    @staticmethod\n",
        "    def from_direction(direction: float) -> \"Facing\":\n",
        "        return Facing.RIGHT if direction > 0 else Facing.LEFT\n",
        "\n",
        "    @staticmethod\n",
        "    def turn_check(facing, direction) -> bool:\n",
        "        if facing == Facing.RIGHT and direction < 0:\n",
        "            return True\n",
        "        if facing == Facing.LEFT and direction > 0:\n",
        "            return True\n",
        "        return False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JQ4OqVrSRP3m"
      },
      "outputs": [],
      "source": [
        "from pymunk.pygame_util import DrawOptions\n",
        "\n",
        "\"\"\"Coord system\n",
        "    +------ > x\n",
        "    |\n",
        "    |   . (2, 2)\n",
        "    |\n",
        "    |      . (3, 3)\n",
        "    v\n",
        "    y\n",
        "\"\"\"\n",
        "\n",
        "class WarehouseBrawl(MalachiteEnv[np.ndarray, np.ndarray, int]):\n",
        "\n",
        "    BRAWL_TO_UNITS = 1.024 / 320  # Conversion factor\n",
        "\n",
        "    def __init__(self, mode: RenderMode=RenderMode.RGB_ARRAY, resolution: CameraResolution=CameraResolution.LOW, train_mode: bool = False):\n",
        "        super(WarehouseBrawl, self).__init__()\n",
        "\n",
        "        self.stage_width_tiles: float = 29.8\n",
        "        self.stage_height_tiles: float = 16.8\n",
        "\n",
        "        self.mode = mode\n",
        "        self.resolution = resolution\n",
        "        self.train_mode = train_mode\n",
        "\n",
        "        self.agents = [0, 1] # Agent 0, agent 1\n",
        "        self.logger = ['', '']\n",
        "\n",
        "        # Params\n",
        "        self.fps = 30\n",
        "        self.dt = 1 / self.fps\n",
        "        self.max_timesteps = self.fps * 90\n",
        "\n",
        "        self.agent_1_name = 'Team 1'\n",
        "        self.agent_2_name = 'Team 2'\n",
        "\n",
        "        # Signals\n",
        "        self.knockout_signal = Signal(self)\n",
        "        self.win_signal = Signal(self)\n",
        "        self.hit_during_stun = Signal(self)\n",
        "\n",
        "        # Observation Space\n",
        "        self.observation_space = self.get_observation_space()\n",
        "\n",
        "        self.camera = Camera()\n",
        "\n",
        "        # Action Space\n",
        "        # WASD\n",
        "        self.action_space = self.get_action_space()\n",
        "        # spaces.Box(low=np.array([0] * 4), high=np.array([1] * 4), shape=(4,), dtype=np.float32)\n",
        "\n",
        "        self.action_spaces, self.observation_spaces = {}, {}\n",
        "        for agent_id in self.agents:\n",
        "            self.action_spaces[agent_id] = self.action_space\n",
        "            self.observation_spaces[agent_id] = self.observation_space\n",
        "\n",
        "        self.load_attacks()\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    def get_observation_space(self):\n",
        "        # lowarray = np.array(\n",
        "        #     [0, -self.screen_width_tiles/2, -self.screen_width_tiles/2, 0, 0, 0, 0, 0] +\n",
        "        #     [0 for _ in range(len(Player.states))] +\n",
        "        #     [0] +\n",
        "        #     [(0, -self.screen_width_tiles, -self.screen_width_tiles, 0, 0)[i%5] for i in range(self.max_ammo*5)] +\n",
        "        #     [0, -self.screen_width_tiles/2, -self.screen_width_tiles/2, 0, -self.screen_width_tiles, -self.screen_width_tiles, -self.screen_width_tiles, -self.screen_width_tiles,\n",
        "        #     0, 0, 0, 0] +\n",
        "        #     [0 for _ in range(len(Player.states))] +\n",
        "        #     [(0, -self.screen_width_tiles, -self.screen_width_tiles, 0, 0)[i%5] for i in range(self.max_ammo*5)] +\n",
        "        #     [0]\n",
        "        # )\n",
        "        # higharray = np.array(\n",
        "        #     [1, self.screen_width_tiles/2, self.screen_width_tiles/2, self.screen_width_tiles/2, 2 * math.pi, 10, 20, 3] +\n",
        "        #     [1 for _ in range(len(Player.states))] +\n",
        "        #     [2*math.pi] +\n",
        "        #     [(1, self.screen_width_tiles, self.screen_width_tiles, 2*math.pi, 2*math.pi)[i%5] for i in range(self.max_ammo*5)] +\n",
        "        #     [1, self.screen_width_tiles/2, self.screen_width_tiles/2, self.screen_width_tiles/2, self.screen_width_tiles, self.screen_width_tiles, self.screen_width_tiles, self.screen_width_tiles,\n",
        "        #     2 * math.pi, 2 * math.pi, 20, 3] +\n",
        "        #     [1 for _ in range(len(Player.states))] +\n",
        "        #     [(1, self.screen_width_tiles, self.screen_width_tiles, 2*math.pi, 2*math.pi)[i%5] for i in range(self.max_ammo*5)] +\n",
        "        #     [self.time_limit]\n",
        "        # )\n",
        "\n",
        "        obs_helper = ObsHelper()\n",
        "        self.add_player_obs(obs_helper, 'player')\n",
        "        self.add_player_obs(obs_helper, 'opponent')\n",
        "\n",
        "        print('Obs space', obs_helper.low, obs_helper.high)\n",
        "\n",
        "        self.obs_helper = obs_helper\n",
        "\n",
        "        return self.obs_helper.get_as_box()\n",
        "\n",
        "    def add_player_obs(self, obs_helper, name: str='player') -> None:\n",
        "        # Note: Some low and high bounds are off here. To ensure everyone's code\n",
        "        # still works, we are not modifying them, but will elaborate in comments.\n",
        "        # Pos: Unnormalized, goes from [-18, -7], [18, 7], in game units\n",
        "        obs_helper.add_section([-1, -1], [1, 1], f\"{name}_pos\")\n",
        "        # Vel: Unnormalized, goes from [-10, -10], [10, 10] in game units\n",
        "        obs_helper.add_section([-1, -1], [1, 1], f\"{name}_vel\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_facing\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_grounded\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_aerial\")\n",
        "        obs_helper.add_section([0], [2], f\"{name}_jumps_left\")\n",
        "        obs_helper.add_section([0], [12], f\"{name}_state\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_recoveries_left\")\n",
        "        # Dodge timer: Unnormalized, goes from [0], [82] in frames.\n",
        "        # Represents the time remaining until can dodge again\n",
        "        obs_helper.add_section([0], [1], f\"{name}_dodge_timer\")\n",
        "        # Stun frames: Unnormalized, goes from [0], [80] in frames\n",
        "        # Represents the time remaining until the player transitions\n",
        "        # out of StunState.\n",
        "        obs_helper.add_section([0], [1], f\"{name}_stun_frames\")\n",
        "        obs_helper.add_section([0], [1], f\"{name}_damage\")\n",
        "        obs_helper.add_section([0], [3], f\"{name}_stocks\")\n",
        "        obs_helper.add_section([0], [11], f\"{name}_move_type\")\n",
        "\n",
        "    def get_action_space(self):\n",
        "        act_helper = ActHelper()\n",
        "        act_helper.add_key(\"w\") # W (Aim up)\n",
        "        act_helper.add_key(\"a\") # A (Left)\n",
        "        act_helper.add_key(\"s\") # S (Aim down/fastfall)\n",
        "        act_helper.add_key(\"d\") # D (Right)\n",
        "        act_helper.add_key(\"space\") # Space (Jump)\n",
        "        act_helper.add_key(\"h\") # H (Pickup/Throw)\n",
        "        act_helper.add_key(\"l\") # L (Dash/Dodge)\n",
        "        act_helper.add_key(\"j\") # J (Light Attack)\n",
        "        act_helper.add_key(\"k\") # K (Heavy Attack)\n",
        "        act_helper.add_key(\"g\") # G (Taunt)\n",
        "\n",
        "        print('Action space', act_helper.low, act_helper.high)\n",
        "\n",
        "        self.act_helper = act_helper\n",
        "\n",
        "        return self.act_helper.get_as_box()\n",
        "\n",
        "    def square_floor_collision(arbiter, space, data):\n",
        "        \"\"\"\n",
        "        Collision handler callback that is called when a square collides with the platform.\n",
        "        It sets the square's collision flag so that is_on_floor() returns True.\n",
        "        \"\"\"\n",
        "        shape_a, shape_b = arbiter.shapes\n",
        "        # Check both shapes; one of them should be a square.\n",
        "        if hasattr(shape_a, \"owner\") and isinstance(shape_a.owner, Player):\n",
        "            shape_a.owner.collided_this_step = True\n",
        "        if hasattr(shape_b, \"owner\") and isinstance(shape_b.owner, Player):\n",
        "            shape_b.owner.collided_this_step = True\n",
        "        return True\n",
        "\n",
        "    def get_stats(self, agent_id: int) -> PlayerStats:\n",
        "        player = self.players[agent_id]\n",
        "        return PlayerStats(\n",
        "            damage_taken=player.damage_taken_total,\n",
        "            damage_done=player.damage_done,\n",
        "            lives_left=player.stocks)\n",
        "\n",
        "    def load_attacks(self):\n",
        "        # load all from /content/attacks\n",
        "        self.attacks = {}\n",
        "\n",
        "        self.keys = {\n",
        "            'Unarmed NLight': MoveType.NLIGHT,\n",
        "            'Unarmed DLight': MoveType.DLIGHT,\n",
        "            'Unarmed SLight': MoveType.SLIGHT,\n",
        "            'Unarmed NSig':   MoveType.NSIG,\n",
        "            'Unarmed DSig':   MoveType.DSIG,\n",
        "            'Unarmed SSig':   MoveType.SSIG,\n",
        "            'Unarmed NAir':   MoveType.NAIR,\n",
        "            'Unarmed DAir':   MoveType.DAIR,\n",
        "            'Unarmed SAir':   MoveType.SAIR,\n",
        "            'Unarmed Recovery': MoveType.RECOVERY,\n",
        "            'Unarmed Groundpound': MoveType.GROUNDPOUND,\n",
        "        }\n",
        "\n",
        "        for file in sorted(os.listdir('attacks')):\n",
        "            name = file.split('.')[0]\n",
        "            if name not in self.keys.keys(): continue\n",
        "            with open(os.path.join('attacks', file)) as f:\n",
        "                move_data = json.load(f)\n",
        "\n",
        "            self.attacks[self.keys[name]] = move_data\n",
        "\n",
        "\n",
        "    def step(self, action: dict[int, np.ndarray]):\n",
        "        # Create new rewards dict\n",
        "        self.cur_action = action\n",
        "        self.rewards = {agent: 0 for agent in self.agents}\n",
        "        self.terminated = False\n",
        "        self.logger = ['', '']\n",
        "\n",
        "        self.camera.process()\n",
        "\n",
        "        # Process all other steps\n",
        "        for obj_name, obj in self.objects.items():\n",
        "            # If player\n",
        "            if isinstance(obj, Player):\n",
        "                continue\n",
        "            else:\n",
        "                obj.process()\n",
        "        # Pre-process player step\n",
        "        for agent in self.agents:\n",
        "            player = self.players[agent]\n",
        "            player.pre_process()\n",
        "\n",
        "        # Process player step\n",
        "        for agent in self.agents:\n",
        "            player = self.players[agent]\n",
        "            player.process(action[agent])\n",
        "            if player.stocks <= 0:\n",
        "                self.terminated = True\n",
        "                self.win_signal.emit(agent='player' if agent == 1 else 'opponent')\n",
        "\n",
        "\n",
        "        # Process physics info\n",
        "        for obj_name, obj in self.objects.items():\n",
        "            obj.physics_process(self.dt)\n",
        "\n",
        "        # PyMunk step\n",
        "        self.space.step(self.dt)\n",
        "        self.steps += 1\n",
        "\n",
        "        truncated = self.steps >= self.max_timesteps\n",
        "\n",
        "        # Collect observations\n",
        "        observations = {agent: self.observe(agent) for agent in self.agents}\n",
        "\n",
        "        return observations, self.rewards, self.terminated, truncated, {}\n",
        "\n",
        "    def add_reward(self, agent: int, reward: float) -> None:\n",
        "        # Not really in use\n",
        "        self.rewards[agent] += reward\n",
        "\n",
        "    def reset(self, seed=None) -> Tuple[dict[int, np.ndarray], dict[str, Any]]:\n",
        "        self.seed = seed\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        self.space = pymunk.Space()\n",
        "        self.dt = 1 / 30.0\n",
        "        self.space.gravity = 0, 17.808\n",
        "\n",
        "        self.steps = 0\n",
        "\n",
        "        # Other params\n",
        "        self.rewards = {agent: 0 for agent in self.agents}\n",
        "\n",
        "        # Game Objects\n",
        "        self.objects: dict[str, GameObject] = {}\n",
        "\n",
        "        self.players: list[Player] = []\n",
        "        self.camera.reset(self)\n",
        "        self._setup()\n",
        "\n",
        "        return {agent: self.observe(agent) for agent in self.agents}, {}\n",
        "\n",
        "    def observe(self, agent: int) -> np.ndarray:\n",
        "        #  lh = LowHigh()\n",
        "        # lh += [-1, -1], [1, 1] # 2d vector to goal\n",
        "        # lh += [-1, -1], [1, 1] # 2d vector of global position\n",
        "        # lh += [-1, -1], [1, 1] # 2d vector of global velocity\n",
        "\n",
        "        obs = []\n",
        "        obs += self.players[agent].get_obs()\n",
        "        obs += self.players[1-agent].get_obs()\n",
        "        #obs += self.players[agent].body.position.x, self.players[agent].body.position.y\n",
        "        #obs += self.players[agent].body.position.x, self.players[agent].body.position.y\n",
        "        #obs += self.players[agent].body.velocity.x, self.players[agent].body.velocity.y\n",
        "\n",
        "        return np.array(obs)\n",
        "\n",
        "    def render(self) -> None | np.ndarray | str | list:\n",
        "        return self.camera.get_frame(self)\n",
        "\n",
        "    def handle_ui(self, canvas: pygame.Surface) -> None:\n",
        "        # Define UI\n",
        "        # player_stat = f\"P1: {self.players[0].stocks}, P2: {self.players[1].stocks}\"\n",
        "        # text_surface = self.camera.font.render(player_stat, True, (255, 255, 255))  # White text\n",
        "        # text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 50))  # Center the text\n",
        "        # canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # # Damage\n",
        "        # small_font = pygame.font.Font(None, 20)\n",
        "        # text_surface = small_font.render(f\"{self.players[0].damage}%, {self.players[1].damage}%\", True, (255, 255, 255))  # White text\n",
        "        # text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 70))  # Center the text\n",
        "        # canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # Smaller text\n",
        "        small_font = pygame.font.Font(None, 30)\n",
        "        text_surface = small_font.render(f\"Time: {self.steps}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 30))  # Center the text\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # Smaller text\n",
        "        small_font = pygame.font.Font(None, 20)\n",
        "        text_surface = small_font.render(f\"P1: {self.logger[0]['transition']}, P2: {self.logger[1]['transition']}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 50))  # Center the text\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # Smaller text\n",
        "        small_font = pygame.font.Font(None, 20)\n",
        "        text_surface = small_font.render(f\"P1: {self.logger[0].get('move_type', '')}, P2: {self.logger[1].get('move_type', '')}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(self.camera.window_width // 2, 70))  # Center the text\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        # Smaller text\n",
        "        text_surface = small_font.render(f\"P1 Total Reward: {self.logger[0].get('total_reward', '')}, Reward {self.logger[0].get('reward', '')}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(0, self.camera.window_height - 40))  # Center the text\n",
        "        # make it left\n",
        "        text_rect.left = 0\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "        text_surface = small_font.render(f\"P2 Total Reward: {self.logger[1].get('total_reward', '')}, Reward {self.logger[1].get('reward', '')}\", True, (255, 255, 255))  # White text\n",
        "        text_rect = text_surface.get_rect(center=(0, self.camera.window_height - 20))  # Center the text\n",
        "        text_rect.left = 0\n",
        "        canvas.blit(text_surface, text_rect)\n",
        "\n",
        "\n",
        "\n",
        "    def observation_space(self, agent: AgentID) -> gymnasium.spaces.Space:\n",
        "        return self.observation_spaces[agent]\n",
        "\n",
        "    def action_space(self, agent: AgentID) -> gymnasium.spaces.Space:\n",
        "        return self.action_spaces[agent]\n",
        "\n",
        "    def close(self) -> None:\n",
        "        self.camera.close()\n",
        "\n",
        "    def _setup(self):\n",
        "        # Collsion fix\n",
        "        handler = self.space.add_collision_handler(3, 4)  # (Player1 collision_type, Player2 collision_type)\n",
        "        handler.begin = lambda *args, **kwargs: False\n",
        "\n",
        "        # Environment\n",
        "        ground = Ground(self.space, 0, 2.03, 10.67)\n",
        "        self.objects['ground'] = ground\n",
        "\n",
        "        # Players\n",
        "        # randomize start pos, binary\n",
        "        p1_right = bool(random.getrandbits(1))\n",
        "        p1_start_pos = [5, 0] if p1_right else [-5, 0]\n",
        "        p2_start_pos = [-5, 0] if p1_right else [5, 0]\n",
        "\n",
        "        # Uncomment this if you'd like. It makes train_mode RANDOMIZE the\n",
        "        # position of both players, so that they get used to many\n",
        "        # different positions in the map!\n",
        "\n",
        "        if self.train_mode:\n",
        "            p1_start_pos = [random.uniform(-5, 5), 0]\n",
        "            p2_start_pos = [random.uniform(-5, 5), 0]\n",
        "        else:\n",
        "            p1_start_pos = [5, 0] if p1_right else [-5, 0]\n",
        "            p2_start_pos = [-5, 0] if p1_right else [5, 0]\n",
        "\n",
        "        p1 = Player(self, 0, start_position=p1_start_pos, color=[0, 0, 255, 255])\n",
        "        p2 = Player(self, 1, start_position=p2_start_pos, color=[0, 255, 0, 255])\n",
        "\n",
        "        self.objects['player'] = p1\n",
        "        self.objects['opponent'] = p2\n",
        "\n",
        "        self.players += [p1, p2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yZwI-S1X3P9L"
      },
      "source": [
        "### GameObject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "3osNsDxv3PrR"
      },
      "outputs": [],
      "source": [
        "class GameObject(ABC):\n",
        "\n",
        "    def render(self, canvas: pygame.Surface, camera: Camera) -> None:\n",
        "        pass\n",
        "\n",
        "    def process(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def physics_process(self, dt: float) -> None:\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_image(canvas, img, pos, desired_width, camera, flipped: bool = False):\n",
        "        \"\"\"\n",
        "        Draws an image onto the canvas while correctly handling scaling and positioning.\n",
        "\n",
        "        Parameters:\n",
        "            canvas (pygame.Surface): The surface to draw onto.\n",
        "            img (pygame.Surface): The image to draw.\n",
        "            pos (tuple): The (x, y) position in game coordinates (center of the desired drawing).\n",
        "            desired_width (float): The width in game units.\n",
        "            camera (Camera): The camera object, which has a gtp() method for coordinate conversion.\n",
        "        \"\"\"\n",
        "        # Convert game coordinates to screen coordinates\n",
        "        screen_pos = camera.gtp(pos)\n",
        "\n",
        "        # Compute the new width in screen units\n",
        "        screen_width = int(desired_width * camera.scale_gtp())\n",
        "\n",
        "        # Maintain aspect ratio when scaling\n",
        "        aspect_ratio = img.get_height() / img.get_width()\n",
        "        screen_height = int(screen_width * aspect_ratio)\n",
        "\n",
        "        # Scale the image to the new size\n",
        "        scaled_img = pygame.transform.scale(img, (screen_width, screen_height))\n",
        "\n",
        "        if flipped:\n",
        "            scaled_img = pygame.transform.flip(scaled_img, True, False)\n",
        "\n",
        "        # Compute the top-left corner for blitting (since screen_pos is the center)\n",
        "        top_left = (screen_pos[0] - screen_width // 2, screen_pos[1] - screen_height // 2)\n",
        "\n",
        "        # Blit the scaled image onto the canvas\n",
        "        canvas.blit(scaled_img, top_left)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECELO0ke3LGO"
      },
      "source": [
        "### Other GameObjects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "iLHQtBKr3Kq_"
      },
      "outputs": [],
      "source": [
        "class Ground(GameObject):\n",
        "    def __init__(self, space, x, y, width_ground, color=(150, 150, 150, 255)):\n",
        "        self.body = pymunk.Body(x, y, body_type=pymunk.Body.STATIC)\n",
        "        self.shape = pymunk.Poly.create_box(self.body, (width_ground, 0.1))\n",
        "        self.shape.collision_type = 2 # Ground\n",
        "        self.shape.owner = self\n",
        "        self.shape.body.position = (x, y)\n",
        "        self.shape.friction = 0.7\n",
        "        self.shape.color = color\n",
        "\n",
        "        self.width_ground = width_ground\n",
        "\n",
        "        space.add(self.shape, self.body)\n",
        "        self.loaded = False\n",
        "\n",
        "    def load_assets(self):\n",
        "        if self.loaded: return\n",
        "        self.loaded = True\n",
        "        self.bg_img = pygame.image.load('assets/map/bg.jpg')\n",
        "        self.stage_img = pygame.image.load('assets/map/stage.png')\n",
        "\n",
        "    def render(self, canvas, camera) -> None:\n",
        "        self.load_assets()\n",
        "\n",
        "        #self.draw_image(canvas, self.bg_img, (0, 0), 29.8, camera)\n",
        "        self.draw_image(canvas, self.stage_img, (0, 0.8), self.width_ground * 3.2, camera)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpdrreEnsMVR"
      },
      "source": [
        "### Player"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jww67QlGd4GL"
      },
      "source": [
        "#### PlayerInputInfo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WYd3nArud4PC"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class KeyStatus():\n",
        "    just_pressed: bool = False\n",
        "    held: bool = False\n",
        "    just_released: bool = False\n",
        "\n",
        "class PlayerInputHandler():\n",
        "    def __init__(self):\n",
        "        # Define the key order corresponding to the action vector:\n",
        "        # Index 0: W, 1: A, 2: S, 3: D, 4: space\n",
        "        self.key_names = [\"W\", \"A\", \"S\", \"D\", \"space\", 'h', 'l', 'j', 'k', 'g']\n",
        "        # Previous frame key state (all start as not pressed).\n",
        "        self.prev_state = {key: False for key in self.key_names}\n",
        "        # The current status for each key.\n",
        "        self.key_status = {key: KeyStatus() for key in self.key_names}\n",
        "        # Raw axes computed from key states.\n",
        "        self.raw_vertical = 0.0   # +1 if W is held, -1 if S is held.\n",
        "        self.raw_horizontal = 0.0 # +1 if D is held, -1 if A is held.\n",
        "\n",
        "    def update(self, action: np.ndarray):\n",
        "        \"\"\"\n",
        "        Given an action vector (floats representing 0 or 1),\n",
        "        update the internal state for each key, including:\n",
        "          - whether it was just pressed\n",
        "          - whether it is held\n",
        "          - whether it was just released\n",
        "        Also computes the raw input axes for WS and AD.\n",
        "\n",
        "        Parameters:\n",
        "            action (np.ndarray): 5-element vector representing the current key states.\n",
        "        \"\"\"\n",
        "\n",
        "        # Update each key's status.\n",
        "        for i, key in enumerate(self.key_names):\n",
        "            # Treat a value > 0.5 as pressed.\n",
        "            current = action[i] > 0.5\n",
        "            previous = self.prev_state[key]\n",
        "            self.key_status[key].just_pressed = (not previous and current)\n",
        "            self.key_status[key].just_released = (previous and not current)\n",
        "            self.key_status[key].held = current\n",
        "            # Save the current state for the next update.\n",
        "            self.prev_state[key] = current\n",
        "\n",
        "        # Compute the raw axes:\n",
        "        # Vertical axis: W (+1) and S (-1)\n",
        "        self.raw_vertical = (1.0 if self.key_status[\"W\"].held else 0.0) + (-1.0 if self.key_status[\"S\"].held else 0.0)\n",
        "        # Horizontal axis: D (+1) and A (-1)\n",
        "        self.raw_horizontal = (1.0 if self.key_status[\"D\"].held else 0.0) + (-1.0 if self.key_status[\"A\"].held else 0.0)\n",
        "\n",
        "    def __repr__(self):\n",
        "        # For debugging: provide a summary of the key statuses and axes.\n",
        "        statuses = \", \".join(\n",
        "            f\"{key}: (just_pressed={self.key_status[key].just_pressed}, held={self.key_status[key].held}, just_released={self.key_status[key].just_released})\"\n",
        "            for key in self.key_names\n",
        "        )\n",
        "        return (f\"PlayerInputHandler({statuses}, \"\n",
        "                f\"raw_horizontal={self.raw_horizontal}, raw_vertical={self.raw_vertical})\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHx_GIg5a8x0"
      },
      "source": [
        "#### PlayerObjectState Abstract Base Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "AtO2OKiBa8bS"
      },
      "outputs": [],
      "source": [
        "class PlayerObjectState(ABC):\n",
        "    def __init__(self, player: \"Player\"):\n",
        "        self.p: \"Player\" = player\n",
        "        self.invincible_timer = 0\n",
        "        self.dodge_cooldown = 0\n",
        "        self.stun_time_stored = 0\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def stunned(self, stun_time: int=0):\n",
        "        self.stun_time_stored = stun_time\n",
        "\n",
        "    def vulnerable(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def is_grounded(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    def is_aerial(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    def physics_process(self, dt: float) -> \"PlayerObjectState\":\n",
        "        # Killbox\n",
        "        sides = abs(self.p.body.position.x) > self.p.env.stage_width_tiles // 2\n",
        "        tops = abs(self.p.body.position.y) > self.p.env.stage_height_tiles // 2\n",
        "        if sides or tops:\n",
        "            return self.p.states['KO']\n",
        "\n",
        "        #self != self.p.states['stun'] and\n",
        "        if self.stun_time_stored > 0:\n",
        "            if self == self.p.states['stun']:\n",
        "                self.p.env.hit_during_stun.emit(agent='player' if self.p.agent_id == 0 else 'opponent')\n",
        "            stun_state = self.p.states['stun']\n",
        "            stun_state.set_stun(self.stun_time_stored)\n",
        "            self.stun_time_stored = 0\n",
        "            if hasattr(self, 'jumps_left'):\n",
        "                stun_state.jumps_left = self.jumps_left\n",
        "            return stun_state\n",
        "\n",
        "        # Tick timers\n",
        "        self.invincible_timer = max(0, self.invincible_timer-1)\n",
        "        self.dodge_cooldown = max(0, self.dodge_cooldown-1)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def exit(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "\n",
        "    def reset(self, old) -> \"PlayerObjectState\":\n",
        "        self.p = old.p\n",
        "        self.stun_time_stored = 0\n",
        "        self.invincible_timer = old.invincible_timer\n",
        "        self.dodge_cooldown = old.dodge_cooldown\n",
        "\n",
        "        return self"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNweZj-Wc_zc"
      },
      "source": [
        "#### Basic States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tTAYZyJ2dCIu"
      },
      "outputs": [],
      "source": [
        "class GroundState(PlayerObjectState):\n",
        "    def can_control(self):\n",
        "        return True\n",
        "\n",
        "    def is_grounded(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def reset(self, old) -> None:\n",
        "        super().reset(old)\n",
        "        if hasattr(old, 'dash_timer'):\n",
        "            self.dash_timer = old.dash_timer\n",
        "        else:\n",
        "            self.dash_timer = 0\n",
        "\n",
        "    @staticmethod\n",
        "    def get_ground_state(p: \"Player\") -> PlayerObjectState:\n",
        "        if abs(p.input.raw_horizontal) > 1e-2:\n",
        "            return p.states['walking']\n",
        "        else:\n",
        "            return p.states['standing']\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        if not self.can_control(): return None\n",
        "\n",
        "        # Handle jump\n",
        "        direction = self.p.input.raw_horizontal\n",
        "        near_still = abs(direction) < 1e-2\n",
        "        if self.p.input.key_status[\"space\"].just_pressed and self.p.is_on_floor():\n",
        "            self.p.body.velocity = pymunk.Vec2d(self.p.body.velocity.x, -self.p.jump_speed)\n",
        "            self.p.facing = Facing.from_direction(direction)\n",
        "            in_air = self.p.states['in_air']\n",
        "            in_air.refresh()\n",
        "            return in_air\n",
        "\n",
        "        if not self.p.is_on_floor():\n",
        "            in_air = self.p.states['in_air']\n",
        "            in_air.refresh()\n",
        "            return in_air\n",
        "\n",
        "        # Handle dodge\n",
        "        if near_still and self.p.input.key_status['l'].just_pressed and self.dodge_cooldown <= 0:\n",
        "            self.dodge_cooldown = self.p.grounded_dodge_cooldown\n",
        "            dodge_state = self.p.states['dodge']\n",
        "            dodge_state.set_is_grounded(True)\n",
        "            return dodge_state\n",
        "\n",
        "        # Check for attack\n",
        "        move_type = self.p.get_move()\n",
        "        if move_type != MoveType.NONE:\n",
        "            attack_state = self.p.states['attack']\n",
        "            attack_state.give_move(move_type)\n",
        "            return attack_state\n",
        "\n",
        "        # Check for taunt\n",
        "        if self.p.input.key_status['g'].just_pressed:\n",
        "            taunt_state = self.p.states['taunt']\n",
        "            return taunt_state\n",
        "\n",
        "\n",
        "        return None\n",
        "\n",
        "class InAirState(PlayerObjectState):\n",
        "    def can_control(self):\n",
        "        return True\n",
        "\n",
        "    def is_aerial(self) -> bool:\n",
        "        return True\n",
        "\n",
        "    def refresh(self):\n",
        "        self.jump_timer = 0\n",
        "        self.jumps_left = 2\n",
        "        self.recoveries_left = 1\n",
        "\n",
        "    def set_jumps(self, jump_timer, jumps_left, recoveries_left):\n",
        "        self.jump_timer = jump_timer\n",
        "        self.jumps_left = jumps_left\n",
        "        self.recoveries_left = recoveries_left\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.is_base = True\n",
        "\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        if not self.can_control(): return None\n",
        "\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "        if self.is_base and Facing.turn_check(self.p.facing, direction):\n",
        "            air_turn = self.p.states['air_turnaround']\n",
        "            air_turn.send(self.jump_timer, self.jumps_left, self.recoveries_left)\n",
        "            return air_turn\n",
        "\n",
        "        vel_x = self.p.move_toward(self.p.body.velocity.x, direction * self.p.move_speed, self.p.in_air_ease)\n",
        "        #print(self.p.body.velocity.x, vel_x)\n",
        "        self.p.body.velocity = pymunk.Vec2d(vel_x, self.p.body.velocity.y)\n",
        "\n",
        "        #print(self.p.is_on_floor(), self.p.body.position)\n",
        "        if self.p.is_on_floor():\n",
        "            return GroundState.get_ground_state(self.p)\n",
        "\n",
        "        # Handle Jump\n",
        "        if self.p.input.key_status[\"space\"].just_pressed and self.can_jump():\n",
        "            self.p.body.velocity = pymunk.Vec2d(self.p.body.velocity.x, -self.p.jump_speed)\n",
        "            self.p.facing = Facing.from_direction(direction)\n",
        "            self.jump_timer = self.p.jump_cooldown\n",
        "            self.jumps_left -= 1\n",
        "\n",
        "        # Handle dodge\n",
        "        if self.p.input.key_status['l'].just_pressed and self.dodge_cooldown <= 0:\n",
        "            self.dodge_cooldown = self.p.air_dodge_cooldown\n",
        "            dodge_state = self.p.states['dodge']\n",
        "            dodge_state.jump_timer = self.jump_timer\n",
        "            dodge_state.jumps_left = self.jumps_left\n",
        "            dodge_state.recoveries_left = self.recoveries_left\n",
        "            dodge_state.set_is_grounded(False)\n",
        "            return dodge_state\n",
        "\n",
        "        # Check for attack\n",
        "        move_type = self.p.get_move()\n",
        "        if move_type != MoveType.NONE:\n",
        "            if move_type == MoveType.RECOVERY:\n",
        "                if self.recoveries_left > 0:\n",
        "                    self.recoveries_left -= 1\n",
        "                    attack_state = self.p.states['attack']\n",
        "                    attack_state.jumps_left = self.jumps_left\n",
        "                    attack_state.recoveries_left = self.recoveries_left\n",
        "                    attack_state.give_move(move_type)\n",
        "                    return attack_state\n",
        "            else:\n",
        "                attack_state = self.p.states['attack']\n",
        "                attack_state.jumps_left = self.jumps_left\n",
        "                attack_state.recoveries_left = self.recoveries_left\n",
        "                attack_state.give_move(move_type)\n",
        "                return attack_state\n",
        "\n",
        "        return None\n",
        "\n",
        "    def can_jump(self) -> bool:\n",
        "        return self.jump_timer <= 0 and self.jumps_left > 0\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        if self.p.body.velocity.y < 0:\n",
        "            self.p.animation_sprite_2d.play('alup')\n",
        "        else:\n",
        "            self.p.animation_sprite_2d.play('aldown')\n",
        "\n",
        "class TauntState(InAirState):\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.taunt_timer = self.p.taunt_time\n",
        "        self.seed = random.randint(0, 2)\n",
        "\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        self.taunt_timer = max(0, self.taunt_timer-1)\n",
        "        if self.taunt_timer <= 0:\n",
        "            if self.is_grounded:\n",
        "                return GroundState.get_ground_state(self.p)\n",
        "            else:\n",
        "                in_air = self.p.states['in_air']\n",
        "                if hasattr(self, 'jumps_left'):\n",
        "                    in_air.jumps_left = self.jumps_left\n",
        "                    in_air.jump_timer = 0\n",
        "                    in_air.recoveries_left = self.recoveries_left\n",
        "                return in_air\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        taunts = ['altroll', 'alhappy', 'alkai']\n",
        "        self.p.animation_sprite_2d.play(taunts[self.seed % 3])\n",
        "\n",
        "class WalkingState(GroundState):\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        # Leave walking if not moving\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "\n",
        "        # Check if turning\n",
        "        if Facing.turn_check(self.p.facing, direction):\n",
        "            if self.p.input.key_status[\"l\"].just_pressed:\n",
        "                return self.p.states['backdash']\n",
        "\n",
        "            return self.p.states['turnaround']\n",
        "        if abs(direction) < 1e-2:\n",
        "            return self.p.states['standing']\n",
        "\n",
        "        # Check for dash\n",
        "        if self.p.input.key_status[\"l\"].just_pressed:\n",
        "            return self.p.states['dash']\n",
        "\n",
        "        # Handle movement\n",
        "        self.p.body.velocity = pymunk.Vec2d(direction * self.p.move_speed, self.p.body.velocity.y)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('walk')\n",
        "\n",
        "class SprintingState(GroundState):\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        # Leave walking if not moving\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "        # Check if turning\n",
        "        if Facing.turn_check(self.p.facing, direction):\n",
        "            if self.p.input.key_status[\"l\"].just_pressed:\n",
        "                return self.p.states['backdash']\n",
        "            return self.p.states['turnaround']\n",
        "        if abs(direction) < 1e-2:\n",
        "            return self.p.states['standing']\n",
        "\n",
        "         # Check for dash\n",
        "        if self.p.input.key_status[\"l\"].just_pressed:\n",
        "            return self.p.states['dash']\n",
        "\n",
        "        # Handle movement\n",
        "        self.p.body.velocity = pymunk.Vec2d(direction * self.p.run_speed, self.p.body.velocity.y)\n",
        "\n",
        "        return None\n",
        "\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('run')\n",
        "\n",
        "class StandingState(GroundState):\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None: return new_state\n",
        "\n",
        "        # Leave standing if starting to move\n",
        "        direction: float = self.p.input.raw_horizontal\n",
        "        if Facing.turn_check(self.p.facing, direction):\n",
        "            if self.p.input.key_status[\"l\"].just_pressed:\n",
        "                return self.p.states['backdash']\n",
        "            return self.p.states['turnaround']\n",
        "        if abs(direction) > 1e-2:\n",
        "            self.p.facing = Facing.from_direction(direction)\n",
        "            return self.p.states['walking']\n",
        "\n",
        "\n",
        "        # gradual ease\n",
        "        vel_x = self.p.move_toward(self.p.body.velocity.x, 0, self.p.move_speed)\n",
        "        self.p.body.velocity = pymunk.Vec2d(vel_x, self.p.body.velocity.y)\n",
        "\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('idle')\n",
        "\n",
        "class TurnaroundState(GroundState):\n",
        "    def enter(self) -> None:\n",
        "        self.turnaround_timer = self.p.turnaround_time\n",
        "\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        if self.turnaround_timer <= 0:\n",
        "            # After the turnaround period, update the facing direction.\n",
        "            self.p.facing = Facing.flip(self.p.facing)\n",
        "            return GroundState.get_ground_state(self.p)\n",
        "\n",
        "\n",
        "        # Allow breaking out of turnaround by jumping.\n",
        "        if self.p.input.key_status[\"space\"].just_pressed and self.p.is_on_floor():\n",
        "            self.p.body.velocity = pymunk.Vec2d(self.p.body.velocity.x, -self.p.jump_speed)\n",
        "            return self.p.states['in_air']\n",
        "\n",
        "        if self.p.input.key_status[\"l\"].just_pressed:\n",
        "            return self.p.states['backdash']\n",
        "\n",
        "\n",
        "        self.turnaround_timer = max(0, self.turnaround_timer-1)\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('turn')\n",
        "\n",
        "class AirTurnaroundState(InAirState):\n",
        "\n",
        "    def send(self, jump_timer, jumps_left, recoveries_left):\n",
        "        self.jump_timer = jump_timer\n",
        "        self.jumps_left = jumps_left\n",
        "        self.recoveries_left = recoveries_left\n",
        "\n",
        "    def is_base(self):\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.turnaround_timer = self.p.turnaround_time\n",
        "        self.p.body.velocity = pymunk.Vec2d(self.p.body.velocity.x / 3, self.p.body.velocity.y)\n",
        "        self.is_base = False\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        if self.turnaround_timer <= 0:\n",
        "            # After the turnaround period, update the facing direction.\n",
        "            self.p.facing = Facing.flip(self.p.facing)\n",
        "            in_air = self.p.states['in_air']\n",
        "            in_air.set_jumps(self.jump_timer, self.jumps_left, self.recoveries_left)\n",
        "            return in_air\n",
        "\n",
        "\n",
        "        self.turnaround_timer = max(0, self.turnaround_timer-1)\n",
        "        return None\n",
        "\n",
        "    def can_jump(self) -> bool:\n",
        "        return self.jump_timer <= 0 and self.jumps_left > 0\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('turn')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwnWPBG_nQeg"
      },
      "source": [
        "#### Hurt States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "cug9v9L1nKaP"
      },
      "outputs": [],
      "source": [
        "class StunState(InAirState):\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def set_stun(self, stun_frames):\n",
        "        self.stun_frames = stun_frames\n",
        "        #print('stun', self.stun_frames)\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        self.stun_frames = max(0, self.stun_frames-1)\n",
        "\n",
        "        vel_x = self.p.move_toward(self.p.body.velocity.x, 0, self.p.in_air_ease / 1.5)\n",
        "        #print(self.p.body.velocity.x, vel_x)\n",
        "        self.p.body.velocity = pymunk.Vec2d(vel_x, self.p.body.velocity.y)\n",
        "\n",
        "        if self.stun_frames > 0: return None\n",
        "\n",
        "        if self.p.is_on_floor():\n",
        "            return GroundState.get_ground_state(self.p)\n",
        "        else:\n",
        "            in_air = self.p.states['in_air']\n",
        "            if hasattr(self, 'jumps_left'):\n",
        "                in_air.jumps_left = max(1, self.jumps_left)\n",
        "            else:\n",
        "                in_air.jumps_left = 1\n",
        "            return in_air\n",
        "\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('hurt_up')\n",
        "\n",
        "class KOState(GroundState):\n",
        "\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.p.env.knockout_signal.emit(agent='player' if self.p.agent_id == 0 else 'opponent')\n",
        "        self.timer = 30 * 3\n",
        "        self.p.stocks -= 1\n",
        "        self.p.body.velocity_func = DodgeState.no_gravity_velocity_func\n",
        "        self.p.body.velocity = pymunk.Vec2d(0, 0)\n",
        "\n",
        "    def exit(self) -> None:\n",
        "        self.invincible_timer = self.p.invincible_time\n",
        "        self.p.body.body_type = pymunk.Body.DYNAMIC\n",
        "        self.p.body.velocity_func = pymunk.Body.update_velocity\n",
        "        self.p.body.velocity = pymunk.Vec2d(0, 0)\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "\n",
        "        self.timer -= 1\n",
        "\n",
        "        if self.timer <= 0:\n",
        "            self.p.respawn()\n",
        "            in_air = self.p.states['in_air']\n",
        "            in_air.jumps_left = 0\n",
        "            in_air.recoveries_left = 0\n",
        "            return in_air\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('hurt_up')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7lJmFQncGw7"
      },
      "source": [
        "#### Dash States"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "0bp2tc-ncHzm"
      },
      "outputs": [],
      "source": [
        "class DashState(GroundState):\n",
        "    def enter(self) -> None:\n",
        "        self.dash_timer = self.p.dash_time\n",
        "        # Optionally, play a dash sound or animation here.\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        # Apply a strong forward velocity in the facing direction.\n",
        "        self.p.body.velocity = pymunk.Vec2d(int(self.p.facing) * self.p.dash_speed, self.p.body.velocity.y)\n",
        "        self.dash_timer = max(0, self.dash_timer-1)\n",
        "        if self.dash_timer <= 0:\n",
        "            return self.p.states['sprinting']\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('run')\n",
        "\n",
        "\n",
        "class BackDashState(GroundState):\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.backdash_timer = self.p.backdash_time\n",
        "        # Backdash is usually slower than a forward dash.\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        # Apply velocity opposite to the facing direction.\n",
        "        # Note: Backdash does not change facing_direction.\n",
        "        self.p.body.velocity = pymunk.Vec2d(-int(self.p.facing) * self.p.backdash_speed, self.p.body.velocity.y)\n",
        "        self.backdash_timer = max(0, self.backdash_timer-1)\n",
        "        if self.backdash_timer <= 0:\n",
        "            return GroundState.get_ground_state(self.p)\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('backdash')\n",
        "\n",
        "class DodgeState(InAirState):\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    @staticmethod\n",
        "    def no_gravity_velocity_func(body, gravity, damping, dt):\n",
        "        # Call the default velocity updater with gravity set to zero.\n",
        "        pymunk.Body.update_velocity(body, pymunk.Vec2d(0, 0), damping, dt)\n",
        "\n",
        "    def set_is_grounded(self, is_grounded: bool) -> None:\n",
        "        self.is_grounded = is_grounded\n",
        "\n",
        "    def is_aerial(self) -> bool:\n",
        "        return not self.is_grounded\n",
        "\n",
        "    def is_grounded(self) -> bool:\n",
        "        return self.is_grounded\n",
        "\n",
        "    def vulnerable(self) -> bool:\n",
        "        return False\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.dodge_timer = self.p.dodge_time\n",
        "        # disable player gravity\n",
        "        # Override the body's velocity function to ignore gravity.\n",
        "        self.p.body.velocity_func = DodgeState.no_gravity_velocity_func\n",
        "        self.p.body.velocity = pymunk.Vec2d(0, 0)\n",
        "\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        self.dodge_timer = max(0, self.dodge_timer-1)\n",
        "        if self.dodge_timer <= 0:\n",
        "            if self.is_grounded:\n",
        "                return GroundState.get_ground_state(self.p)\n",
        "            else:\n",
        "                in_air = self.p.states['in_air']\n",
        "                if hasattr(self, 'jumps_left'):\n",
        "                    in_air.jumps_left = self.jumps_left\n",
        "                    in_air.jump_timer = 0\n",
        "                    in_air.recoveries_left = self.recoveries_left\n",
        "                return in_air\n",
        "        return None\n",
        "\n",
        "    def exit(self) -> None:\n",
        "        self.p.body.body_type = pymunk.Body.DYNAMIC\n",
        "        self.p.body.velocity_func = pymunk.Body.update_velocity\n",
        "        self.p.body.velocity = pymunk.Vec2d(0, 0)\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        self.p.attack_sprite.play(None)\n",
        "        self.p.animation_sprite_2d.play('dodge')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzQW3ZFWjYdP"
      },
      "source": [
        "#### Move"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xFB5GgOqjcHA"
      },
      "outputs": [],
      "source": [
        "class MoveManager():\n",
        "    def __init__(self, player: \"Player\", move_data):\n",
        "        self.p = player\n",
        "        self.move_data = move_data\n",
        "        self.all_hit_agents: List = []             # List of LegendAgent instances (to be defined elsewhere)\n",
        "        initial_power = move_data['powers'][move_data['move']['initialPowerIndex']]\n",
        "        self.current_power = Power.get_power(initial_power)\n",
        "        self.current_power.p = self.p\n",
        "        self.frame = 0\n",
        "        self.move_facing_direction = self.p.facing\n",
        "        self.hit_agent = None\n",
        "        self.keys = {\n",
        "            'LIGHT': 'j',\n",
        "            'HEAVY': 'k',\n",
        "            'THROW': 'l'\n",
        "        }\n",
        "\n",
        "    def do_move(self, is_holding_move_type: bool) -> bool:\n",
        "        \"\"\"\n",
        "        action: list of ints (e.g. 0 or 1) representing input keys.\n",
        "        is_holding_move_type: whether the move key is held.\n",
        "        \"\"\"\n",
        "        self.move_facing_direction = self.p.facing\n",
        "        key = self.keys[self.move_data['move']['actionKey']]\n",
        "        holding_move_key = self.p.input.key_status[key].held\n",
        "        done, next_power = self.current_power.do_power(holding_move_key, is_holding_move_type, self)\n",
        "        if next_power is not None:\n",
        "            self.current_power = next_power\n",
        "        self.frame += 1\n",
        "        return done"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_joKiMJlVSC"
      },
      "source": [
        "#### Frame Change Stuff"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "k3-oNTUflUf1"
      },
      "outputs": [],
      "source": [
        "class HurtboxPositionChange():\n",
        "    def __init__(self, xOffset=0, yOffset=0, width=0, height=0, active=False):\n",
        "        self.xOffset = xOffset\n",
        "        self.yOffset = yOffset\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.active = active\n",
        "\n",
        "class CasterPositionChange():\n",
        "    def __init__(self, x=0, y=0, active=False):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "        self.active = active\n",
        "\n",
        "class DealtPositionTarget():\n",
        "    def __init__(self, xOffset=0, yOffset=0, active=False):\n",
        "        self.xOffset = xOffset\n",
        "        self.yOffset = yOffset\n",
        "        self.active = active\n",
        "\n",
        "class CasterVelocitySet():\n",
        "    def __init__(self, magnitude=0.0, directionDeg=0.0, active=False):\n",
        "        self.magnitude = magnitude\n",
        "        self.directionDeg = directionDeg\n",
        "        self.active = active\n",
        "\n",
        "class CasterVelocitySetXY():\n",
        "    def __init__(self, magnitudeX=0.0, magnitudeY=0.0, activeX=False, activeY=False):\n",
        "        self.magnitudeX = magnitudeX\n",
        "        self.magnitudeY = -magnitudeY\n",
        "        self.activeX = activeX\n",
        "        self.activeY = activeY\n",
        "\n",
        "class CasterVelocityDampXY():\n",
        "    def __init__(self, dampX=1.0, dampY=1.0, activeX=False, activeY=False):\n",
        "        self.dampX = dampX\n",
        "        self.dampY = dampY\n",
        "        self.activeX = activeX\n",
        "        self.activeY = activeY\n",
        "\n",
        "class CastFrameChangeHolder():\n",
        "    def __init__(self, data):\n",
        "        \"\"\"\n",
        "        data: a dictionary representing a single frame change from the cast data.\n",
        "        For each element, if its data is present in the dictionary, instantiate the corresponding class;\n",
        "        otherwise, use a default instance.\n",
        "        \"\"\"\n",
        "        self.frame = data.get(\"frame\", 0)\n",
        "\n",
        "        # For each change, if its key is present, create an instance with the provided data.\n",
        "        # Otherwise, instantiate with default values.\n",
        "        if \"casterPositionChange\" in data:\n",
        "            cp_data = data[\"casterPositionChange\"]\n",
        "            self.caster_position_change = CasterPositionChange(\n",
        "                x=cp_data.get(\"x\", 0),\n",
        "                y=cp_data.get(\"y\", 0),\n",
        "                active=cp_data.get(\"active\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.caster_position_change = CasterPositionChange()\n",
        "\n",
        "        if \"dealtPositionTarget\" in data:\n",
        "            dpt_data = data[\"dealtPositionTarget\"]\n",
        "            self.dealt_position_target = DealtPositionTarget(\n",
        "                xOffset=dpt_data.get(\"xOffset\", 0),\n",
        "                yOffset=dpt_data.get(\"yOffset\", 0),\n",
        "                active=dpt_data.get(\"active\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.dealt_position_target = DealtPositionTarget()\n",
        "\n",
        "        if \"casterVelocitySet\" in data:\n",
        "            cvs_data = data[\"casterVelocitySet\"]\n",
        "            self.caster_velocity_set = CasterVelocitySet(\n",
        "                magnitude=cvs_data.get(\"magnitude\", 0.0),\n",
        "                directionDeg=cvs_data.get(\"directionDeg\", 0.0),\n",
        "                active=cvs_data.get(\"active\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.caster_velocity_set = None\n",
        "\n",
        "        if \"casterVelocitySetXY\" in data:\n",
        "            cvsxy_data = data[\"casterVelocitySetXY\"]\n",
        "            self.caster_velocity_set_xy = CasterVelocitySetXY(\n",
        "                magnitudeX=cvsxy_data.get(\"magnitudeX\", 0.0),\n",
        "                magnitudeY=cvsxy_data.get(\"magnitudeY\", 0.0),\n",
        "                activeX=cvsxy_data.get(\"activeX\", False),\n",
        "                activeY=cvsxy_data.get(\"activeY\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.caster_velocity_set_xy = None\n",
        "\n",
        "        if \"casterVelocityDampXY\" in data:\n",
        "            cvdxy_data = data[\"casterVelocityDampXY\"]\n",
        "            self.caster_velocity_damp_xy = CasterVelocityDampXY(\n",
        "                dampX=cvdxy_data.get(\"dampX\", 1.0),\n",
        "                dampY=cvdxy_data.get(\"dampY\", 1.0),\n",
        "                activeX=cvdxy_data.get(\"activeX\", False),\n",
        "                activeY=cvdxy_data.get(\"activeY\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.caster_velocity_damp_xy = None\n",
        "\n",
        "        if \"hurtboxPositionChange\" in data:\n",
        "            hpc_data = data[\"hurtboxPositionChange\"]\n",
        "            self.hurtbox_position_change = HurtboxPositionChange(\n",
        "                xOffset=hpc_data.get(\"xOffset\", 0),\n",
        "                yOffset=hpc_data.get(\"yOffset\", 0),\n",
        "                width=hpc_data.get(\"width\", 0),\n",
        "                height=hpc_data.get(\"height\", 0),\n",
        "                active=hpc_data.get(\"active\", False)\n",
        "            )\n",
        "        else:\n",
        "            self.hurtbox_position_change = HurtboxPositionChange()\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<CastFrameChangeHolder frame={self.frame}>\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YTVrhr0dRvv"
      },
      "source": [
        "#### Cast"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "oVlMkEWPdSWm"
      },
      "outputs": [],
      "source": [
        "class Cast():\n",
        "    def __init__(self, cast_data):\n",
        "        self.frame_idx = 0\n",
        "        self.cast_data = cast_data\n",
        "        self.startup_frames = cast_data.get(\"startupFrames\", 0) // 2\n",
        "        self.attack_frames = cast_data.get(\"attackFrames\", 0) // 2\n",
        "        self.base_damage = cast_data.get(\"baseDamage\", 0)\n",
        "        self.variable_force = cast_data.get(\"variableForce\", 0.0)\n",
        "        self.fixed_force = cast_data.get(\"fixedForce\", 0.0)\n",
        "        self.hit_angle_deg = cast_data.get(\"hitAngleDeg\", 0.0)\n",
        "        self.must_be_held = cast_data.get(\"mustBeHeld\", False)\n",
        "        self.collision_check_points = cast_data.get(\"collisionCheckPoints\", [])\n",
        "        self.hitboxes = cast_data.get(\"hitboxes\", [])\n",
        "\n",
        "    @staticmethod\n",
        "    def get_cast(cast_data) -> \"Cast\":\n",
        "        return Cast(cast_data)\n",
        "\n",
        "    def get_frame_data(self, idx):\n",
        "        \"\"\"\n",
        "        Iterate through the cast_data's 'frameChanges' list (if present) and return a\n",
        "        CastFrameChangeHolder built from the dictionary whose 'frame' equals idx.\n",
        "        If none is found, return None.\n",
        "        \"\"\"\n",
        "        frame_changes = self.cast_data.get(\"frameChanges\", [])\n",
        "        for change_data in frame_changes:\n",
        "            # Only use the data that is present; don't create a new change if not provided.\n",
        "            if change_data.get(\"frame\") == idx:\n",
        "                return CastFrameChangeHolder(change_data)\n",
        "        return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPOX5xjac5XC"
      },
      "source": [
        "#### Power"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "yWsjNY8pc64M"
      },
      "outputs": [],
      "source": [
        "class Power():\n",
        "\n",
        "    def __init__(self, power_data, casts):\n",
        "        \"\"\"\n",
        "        power_data: an object (or dict) representing the PowerScriptableObject.\n",
        "                    Expected to have attributes like recovery, fixedRecovery,\n",
        "                    onHitNextPower, onMissNextPower, hitAngleDeg, minCharge, isCharge, etc.\n",
        "        \"\"\"\n",
        "        self.power_data = power_data\n",
        "        self.casts = casts\n",
        "        self.cast_idx = 0\n",
        "        self.total_frame_count = 0\n",
        "        self.frames_into_recovery = 0\n",
        "        self.recovery_frames = 0\n",
        "        self.hit_anyone = False\n",
        "        self.dealt_position_target_exists = False\n",
        "        self.current_dealt_position_target = (0.0, 0.0)\n",
        "        self.agents_in_move = []\n",
        "        self.is_switching_casts = True\n",
        "        self.past_point_positions = []\n",
        "\n",
        "        # deal with the power data\n",
        "        self.power_id = power_data.get('powerID', -1)\n",
        "        self.fixed_recovery = power_data.get('fixedRecovery', 0) // 2\n",
        "        self.recovery = power_data.get('recovery', 0) // 2\n",
        "        self.cooldown = power_data.get('cooldown', 0) // 2\n",
        "        self.min_charge = power_data.get('minCharge', 0) // 2\n",
        "        self.stun_time = power_data.get('stunTime', 0) // 2\n",
        "        self.hit_angle_deg = power_data.get('hitAngleDeg', 0.0)\n",
        "        self.is_charge = power_data.get('isCharge', False)\n",
        "        self.damage_over_life_of_hitbox = power_data.get('damageOverLifeOfHitbox', False)\n",
        "        self.disable_caster_gravity = power_data.get('disableCasterGravity', False)\n",
        "        self.disable_hit_gravity = power_data.get('disableHitGravity', False)\n",
        "        self.target_all_hit_agents = power_data.get('targetAllHitAgents', False)\n",
        "        self.transition_on_instant_hit = power_data.get('transitionOnInstantHit', False)\n",
        "        self.on_hit_velocity_set_active = power_data.get('onHitVelocitySetActive', False)\n",
        "        self.on_hit_velocity_set_magnitude = power_data.get('onHitVelocitySetMagnitude', 0.0)\n",
        "        self.on_hit_velocity_set_direction_deg = power_data.get('onHitVelocitySetDirectionDeg', 0.0)\n",
        "        self.enable_floor_drag = power_data.get('enableFloorDrag', False)\n",
        "\n",
        "        # Next-power indices (set to -1 if not provided)\n",
        "        self.on_hit_next_power_index = power_data.get('onHitNextPowerIndex', -1)\n",
        "        self.on_miss_next_power_index = power_data.get('onMissNextPowerIndex', -1)\n",
        "        self.on_ground_next_power_index = power_data.get('onGroundNextPowerIndex', -1)\n",
        "\n",
        "        # last_power is True if both onHitNextPower and onMissNextPower are None.\n",
        "        self.last_power = (self.on_hit_next_power_index == -1 and self.on_miss_next_power_index == -1)\n",
        "\n",
        "        if casts and len(casts) > 0:\n",
        "            # Use the last cast to determine recoveryFrames.\n",
        "            self.recovery_frames = self.recovery + self.fixed_recovery\n",
        "\n",
        "    @staticmethod\n",
        "    def get_power(power_data) -> \"Power\":\n",
        "        casts = [Cast.get_cast(cast) for cast in power_data['casts']]\n",
        "        return Power(power_data, casts)\n",
        "\n",
        "    def do_power(self, holding_key, is_holding_move_type, move_manager):\n",
        "        \"\"\"\n",
        "        Execute one frame of the power.\n",
        "\n",
        "        Parameters:\n",
        "          holding_key (bool): whether the move key is held.\n",
        "          is_holding_move_type (bool): e.g. whether a charge modifier is held.\n",
        "          move_manager: the MoveManager (with attributes such as moveFacingDirection, hit_agent, all_hit_agents, etc.)\n",
        "\n",
        "        Returns a tuple (done, next_power):\n",
        "          - done (bool): whether this power (and move) is finished.\n",
        "          - next_power: the next Power instance to transition to (or None if finished).\n",
        "        \"\"\"\n",
        "        done = False\n",
        "        transitioning_to_next_power = False\n",
        "        next_power = None\n",
        "\n",
        "        # For recovery-block checks; initialize defaults in case not set later.\n",
        "        in_startup = False\n",
        "        in_attack = False\n",
        "\n",
        "        # Disable caster gravity.\n",
        "        self.p.set_gravity_disabled(self.disable_caster_gravity)\n",
        "\n",
        "        is_past_min_charge = self.total_frame_count > self.min_charge\n",
        "        last_cast = self.casts[-1]\n",
        "        is_past_max_charge = self.total_frame_count > last_cast.startup_frames\n",
        "\n",
        "        # If this power is a charge and either (a) not holding key and past min charge, or (b) past max charge, then switch.\n",
        "        if self.is_charge and ((not holding_key and is_past_min_charge) or is_past_max_charge):\n",
        "            if self.on_miss_next_power_index != -1:\n",
        "                miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                next_power = Power.get_power(miss_power)\n",
        "            else:\n",
        "                print(\"...how?\")\n",
        "        else:\n",
        "            current_cast: Cast = self.casts[self.cast_idx]\n",
        "            cfch = current_cast.get_frame_data(current_cast.frame_idx)\n",
        "            # Calculate hit vector\n",
        "\n",
        "            hit_vector = (0.0, 0.0, 0.0)\n",
        "            if cfch is not None and cfch.dealt_position_target is not None and cfch.dealt_position_target.active:\n",
        "                self.dealt_position_target_exists = True\n",
        "                self.current_dealt_position_target = (cfch.dealt_position_target.xOffset, cfch.dealt_position_target.yOffset)\n",
        "            else:\n",
        "                self.dealt_position_target_exists = False\n",
        "                self.current_dealt_position_target = (0.0, 0.0)\n",
        "            if not self.dealt_position_target_exists:\n",
        "                # No target: calculate force from angle.\n",
        "                # Assume hitAngleDeg may be a wrapped value with a 'Value' attribute; otherwise, use power_data.hitAngleDeg.\n",
        "                if current_cast.hit_angle_deg != 0.0:\n",
        "                    hit_angle_deg = current_cast.hit_angle_deg\n",
        "                else:\n",
        "                    hit_angle_deg = self.hit_angle_deg\n",
        "                hit_vector = (\n",
        "                    math.cos(math.radians(hit_angle_deg)),\n",
        "                    -math.sin(math.radians(hit_angle_deg)),\n",
        "                    0.0\n",
        "                )\n",
        "                # Multiply x by moveFacingDirection.\n",
        "                hit_vector = (hit_vector[0] * int(move_manager.move_facing_direction), hit_vector[1], hit_vector[2])\n",
        "\n",
        "            in_startup = current_cast.frame_idx < current_cast.startup_frames\n",
        "            is_in_attack_frames = current_cast.frame_idx < (current_cast.startup_frames + current_cast.attack_frames)\n",
        "            in_attack = (not in_startup) and (is_in_attack_frames or current_cast.must_be_held)\n",
        "\n",
        "            if in_startup:\n",
        "                self.p.do_cast_frame_changes_with_changes(cfch, self.enable_floor_drag, move_manager)\n",
        "                self.p.set_hitboxes_to_draw()\n",
        "            elif in_attack:\n",
        "                self.p.do_cast_frame_changes_with_changes(cfch, self.enable_floor_drag, move_manager)\n",
        "                self.p.set_hitboxes_to_draw(current_cast.hitboxes,\n",
        "                                                  current_cast.collision_check_points,\n",
        "                                                  move_manager.move_facing_direction)\n",
        "\n",
        "                cast_damage = current_cast.base_damage\n",
        "                if self.damage_over_life_of_hitbox:\n",
        "                    damage_to_deal = cast_damage / current_cast.attack_frames\n",
        "                else:\n",
        "                    damage_to_deal = cast_damage\n",
        "\n",
        "                # Check collision.\n",
        "                collided = False\n",
        "                if self.is_switching_casts:\n",
        "                    self.is_switching_casts = False\n",
        "                else:\n",
        "                    for i in range(len(current_cast.collision_check_points)):\n",
        "                        point = current_cast.collision_check_points[i]\n",
        "                        point_offset = Capsule.get_hitbox_offset(point['xOffset'], point['yOffset'])\n",
        "                        # Multiply x offset by moveFacingDirection.\n",
        "                        point_offset = (point_offset[0] * int(move_manager.move_facing_direction), point_offset[1])\n",
        "                        # Assume agent.position is a tuple (x, y)\n",
        "                        point_pos = (self.p.body.position[0] + point_offset[0], self.p.body.position[1] + point_offset[1])\n",
        "                        collided = point_pos[1] > 1.54\n",
        "\n",
        "                # Initialize past point positions for the next frame.\n",
        "                self.past_point_positions = []\n",
        "                for point in current_cast.collision_check_points:\n",
        "                    point_offset = Capsule.get_hitbox_offset(point['xOffset'], point['yOffset'])\n",
        "                    point_offset = (point_offset[0] * int(move_manager.move_facing_direction), point_offset[1])\n",
        "                    point_pos = (self.p.body.position[0] + point_offset[0], self.p.body.position[1] + point_offset[1])\n",
        "                    self.past_point_positions.append(point_pos)\n",
        "\n",
        "                if current_cast.must_be_held and (not is_holding_move_type):\n",
        "                    transitioning_to_next_power = True\n",
        "                    if self.on_miss_next_power_index != -1:\n",
        "                        miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                        next_power = Power.get_power(miss_power)\n",
        "                        next_power = move_manager.move_data.onMissNextPower.get_power()\n",
        "                if collided:\n",
        "                    transitioning_to_next_power = True\n",
        "                    if self.on_ground_next_power_index != -1:\n",
        "                        ground_power = move_manager.move_data['powers'][self.on_ground_next_power_index ]\n",
        "                        next_power = Power.get_power(ground_power)\n",
        "                    elif self.on_miss_next_power_index != -1:\n",
        "                        miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                        next_power = Power.get_power(miss_power)\n",
        "\n",
        "                # Check hitboxes.\n",
        "                hitbox_hit = False\n",
        "                hit_agents = []\n",
        "                for hitbox in current_cast.hitboxes:\n",
        "                    hitbox_offset = Capsule.get_hitbox_offset(hitbox['xOffset'], hitbox['yOffset'])\n",
        "                    hitbox_offset = (hitbox_offset[0] * int(move_manager.move_facing_direction), hitbox_offset[1])\n",
        "                    hitbox_pos = (self.p.body.position[0] + hitbox_offset[0], self.p.body.position[1] + hitbox_offset[1])\n",
        "                    hitbox_size = Capsule.get_hitbox_size(hitbox['width'], hitbox['height'])\n",
        "                    capsule1 = CapsuleCollider(center=hitbox_pos, width=hitbox_size[0], height=hitbox_size[1])\n",
        "                    intersects = self.p.opponent.hurtbox_collider.intersects(capsule1)\n",
        "                    hit_agent = self.p.opponent\n",
        "                    #print(self.p.opponent)\n",
        "                    #print(hitbox_pos, hitbox_size)\n",
        "                    #print(self.p.opponent.hurtbox_collider.center, self.p.opponent.hurtbox_collider.width, self.p.opponent.hurtbox_collider.height)\n",
        "                    if intersects and hit_agent.state.vulnerable():\n",
        "                        #print(self.p.opponent.hurtbox_collider, capsule1)\n",
        "                        hitbox_hit = True\n",
        "                        #print(f'Player {self.p.agent_id} HIT!')\n",
        "                        if not self.hit_anyone:\n",
        "                            if self.on_hit_velocity_set_active:\n",
        "                                on_hit_vel = (math.cos(math.radians(self.on_hit_velocity_set_direction_deg)),\n",
        "                                                math.sin(math.radians(self.on_hit_velocity_set_direction_deg)))\n",
        "                                on_hit_vel = (on_hit_vel[0] * self.on_hit_velocity_set_magnitude, on_hit_vel[1])\n",
        "\n",
        "                                self.p.body.velocity = pymunk.Vec2d(on_hit_vel[0], on_hit_vel[1])\n",
        "                        self.hit_anyone = True\n",
        "                        force_magnitude = (current_cast.fixed_force +\n",
        "                                            current_cast.variable_force * hit_agent.damage * 0.02622)\n",
        "                        if hit_agent not in hit_agents:\n",
        "                            if self.damage_over_life_of_hitbox:\n",
        "                                hit_agent.apply_damage(damage_to_deal, self.stun_time,\n",
        "                                                    (hit_vector[0] * (force_magnitude / current_cast.cast_data.attackFrames),\n",
        "                                                    hit_vector[1] * (force_magnitude / current_cast.cast_data.attackFrames)))\n",
        "                            hit_agents.append(hit_agent)\n",
        "                        if hit_agent not in self.agents_in_move:\n",
        "                            if move_manager.hit_agent is None:\n",
        "                                move_manager.hit_agent = hit_agent\n",
        "                            if not self.damage_over_life_of_hitbox:\n",
        "                                hit_agent.apply_damage(damage_to_deal, self.stun_time,\n",
        "                                                    (hit_vector[0] * force_magnitude, hit_vector[1] * force_magnitude))\n",
        "                            hit_agent.set_gravity_disabled(self.disable_hit_gravity)\n",
        "                            self.agents_in_move.append(hit_agent)\n",
        "                        if hit_agent not in move_manager.all_hit_agents:\n",
        "                            hit_agent.just_got_hit = True\n",
        "                            move_manager.all_hit_agents.append(hit_agent)\n",
        "                if hitbox_hit and self.transition_on_instant_hit:\n",
        "                    if self.on_hit_next_power_index != -1:\n",
        "                        hit_power = move_manager.move_data['powers'][self.on_hit_next_power_index]\n",
        "                        next_power = Power.get_power(hit_power)\n",
        "                    elif self.on_miss_next_power_index != -1:\n",
        "                        miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                        next_power = Power.get_power(miss_power)\n",
        "                if self.cast_idx == len(self.casts) - 1 and self.last_power:\n",
        "                    self.frames_into_recovery += 1\n",
        "\n",
        "            # Increment the current cast's frame index.\n",
        "            current_cast.frame_idx += 1\n",
        "\n",
        "            # Recovery handling: if not transitioning and not in startup or attack.\n",
        "            if (not transitioning_to_next_power) and (not in_attack) and (not in_startup):\n",
        "                self.p.set_hitboxes_to_draw()\n",
        "                if self.cast_idx == len(self.casts) - 1:\n",
        "                    if self.frames_into_recovery >= self.recovery_frames:\n",
        "                        if self.last_power:\n",
        "                            done = True\n",
        "                        else:\n",
        "                            if self.hit_anyone:\n",
        "                                if self.on_hit_next_power_index != -1:\n",
        "                                    hit_power = move_manager.move_data['powers'][self.on_hit_next_power_index]\n",
        "                                    next_power = Power.get_power(hit_power)\n",
        "                                elif self.on_miss_next_power_index != -1:\n",
        "                                    miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                                    next_power = Power.get_power(miss_power)\n",
        "                            else:\n",
        "                                if self.on_miss_next_power_index != -1:\n",
        "                                    miss_power = move_manager.move_data['powers'][self.on_miss_next_power_index]\n",
        "                                    next_power = Power.get_power(miss_power)\n",
        "                    else:\n",
        "                        self.frames_into_recovery += 1\n",
        "                else:\n",
        "                    self.cast_idx += 1\n",
        "                    self.is_switching_casts = True\n",
        "\n",
        "        self.total_frame_count += 1\n",
        "        if next_power is not None:\n",
        "            next_power.p = self.p\n",
        "        return done, next_power"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6fV-UuVEsGz"
      },
      "source": [
        "#### Attacking State"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "bWqy_FUnEvsp"
      },
      "outputs": [],
      "source": [
        "class AttackState(PlayerObjectState):\n",
        "\n",
        "    def can_control(self):\n",
        "        return False\n",
        "\n",
        "    def give_move(self, move_type: \"MoveType\") -> None:\n",
        "        self.move_type = move_type\n",
        "        # load json Unarmed SLight.json\n",
        "        #with open('Unarmed SLight.json') as f:\n",
        "        #    move_data = json.load(f)\n",
        "        move_data = self.p.env.attacks[move_type]\n",
        "        self.move_manager = MoveManager(self.p, move_data)\n",
        "\n",
        "    def enter(self) -> None:\n",
        "        self.dash_timer = self.p.dash_time\n",
        "        # get random number from 1 to 12\n",
        "        self.seed = random.randint(1, 12)\n",
        "        # Optionally, play a dash sound or animation here.\n",
        "\n",
        "    def exit(self) -> None:\n",
        "        self.p.set_hitboxes_to_draw()\n",
        "\n",
        "    def physics_process(self, dt: float) -> PlayerObjectState:\n",
        "        new_state = super().physics_process(dt)\n",
        "        if new_state is not None:\n",
        "            return new_state\n",
        "\n",
        "        is_holding_move_type = self.move_type == self.p.get_move()\n",
        "\n",
        "        done = self.move_manager.do_move(is_holding_move_type)\n",
        "\n",
        "        if done:\n",
        "            self.p.set_hitboxes_to_draw()\n",
        "\n",
        "            if self.p.is_on_floor():\n",
        "                return GroundState.get_ground_state(self.p)\n",
        "            else:\n",
        "                in_air = self.p.states['in_air']\n",
        "                if hasattr(self, 'jumps_left'):\n",
        "                    in_air.jumps_left = self.jumps_left\n",
        "                    in_air.recoveries_left = self.recoveries_left\n",
        "                    in_air.jump_timer = 0\n",
        "                return in_air\n",
        "        return None\n",
        "\n",
        "    def animate_player(self, camera) -> None:\n",
        "        player_anim, attack_anim = self.p.attack_anims[self.move_type]\n",
        "        current_power = self.move_manager.current_power\n",
        "        if isinstance(player_anim, str):\n",
        "            self.p.animation_sprite_2d.play(player_anim)\n",
        "        elif isinstance(player_anim, dict):\n",
        "\n",
        "            player_anim = player_anim[current_power.power_id]\n",
        "            if isinstance(player_anim, list):\n",
        "                current_cast = current_power.casts[current_power.cast_idx]\n",
        "                in_startup = current_cast.frame_idx < current_cast.startup_frames\n",
        "                self.p.animation_sprite_2d.play(player_anim[0 if in_startup else 1])\n",
        "            else:\n",
        "                self.p.animation_sprite_2d.play(player_anim[current_power.power_id])\n",
        "        else:\n",
        "            self.p.animation_sprite_2d.play(player_anim[self.seed % len(player_anim)])\n",
        "        #self.p.animation_sprite_2d.play('run')\n",
        "        if isinstance(attack_anim, str):\n",
        "            self.p.attack_sprite.play(attack_anim)\n",
        "        elif isinstance(attack_anim, dict):\n",
        "            attack_anim = attack_anim[current_power.power_id]\n",
        "            if isinstance(attack_anim, list):\n",
        "                current_cast = current_power.casts[current_power.cast_idx]\n",
        "                in_startup = current_cast.frame_idx < current_cast.startup_frames\n",
        "                self.p.attack_sprite.play(attack_anim[0 if in_startup else 1])\n",
        "            elif isinstance(attack_anim, tuple):\n",
        "                self.p.attack_sprite.play(attack_anim[self.seed % len(attack_anim)])\n",
        "            else:\n",
        "                self.p.attack_sprite.play(attack_anim)\n",
        "        else:\n",
        "            self.p.attack_sprite.play(attack_anim[self.seed % len(attack_anim)])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vRxHMA07ZKE0"
      },
      "source": [
        "#### AnimatedSprite 2D"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "HlGhclgrZLW4"
      },
      "outputs": [],
      "source": [
        "def hex_to_rgb(hex_color):\n",
        "    \"\"\"Convert a hex string (e.g., '#FE9000') to an RGB tuple.\"\"\"\n",
        "    hex_color = hex_color.lstrip('#')\n",
        "    return tuple(int(hex_color[i:i+2], 16) for i in (0, 2, 4))\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class Animation():\n",
        "    frames: list[np.ndarray]\n",
        "    frame_durations: list[float]\n",
        "    frames_per_step: list[float]\n",
        "\n",
        "class AnimationSprite2D(GameObject):\n",
        "    ENV_FPS = 30  # Environment FPS\n",
        "    albert_palette = {\n",
        "        \"base\": hex_to_rgb(\"#FE9000\"),\n",
        "        \"sides\": hex_to_rgb(\"#A64A00\"),\n",
        "        \"top_bottom\": hex_to_rgb(\"#FFB55A\"),\n",
        "        \"outline\": hex_to_rgb(\"#A02800\")\n",
        "    }\n",
        "\n",
        "    kai_palette = {\n",
        "        \"base\": hex_to_rgb(\"#00A1FE\"),\n",
        "        \"sides\": hex_to_rgb(\"#006080\"),\n",
        "        \"top_bottom\": hex_to_rgb(\"#74CEFF\"),\n",
        "        \"outline\": hex_to_rgb(\"#0069BA\")\n",
        "    }\n",
        "\n",
        "\n",
        "\n",
        "    def __init__(self, camera, scale, animation_folder, agent_id):\n",
        "        super().__init__()\n",
        "        self.finished = False\n",
        "        self.scale = scale\n",
        "        self.agent_id = agent_id\n",
        "        self.current_frame_index = 0\n",
        "        self.frame_timer = 0\n",
        "        self.animation_folder = animation_folder\n",
        "\n",
        "        self.animations: dict[str, Animation] = {}\n",
        "        self.current_animation = None\n",
        "        self.frames = []\n",
        "        self.current_frame_index = 0\n",
        "\n",
        "        self.anim_data = {\n",
        "            #'altroll': [1.0],\n",
        "            #'alhappy': [1.0],\n",
        "            'default': [1.4],\n",
        "            'unarmednsig_paper': [1.6],\n",
        "            'unarmednsig_rock': [1.6],\n",
        "            'unarmednsig_scissors': [1.6],\n",
        "            'unarmedrecovery': [1.0],\n",
        "            'unarmeddlight': [1.2],\n",
        "        }\n",
        "\n",
        "        self.color_mapping = {self.albert_palette[key]: self.kai_palette[key] for key in self.albert_palette}\n",
        "\n",
        "\n",
        "        self.loaded = False\n",
        "\n",
        "    def load_animations(self, animation_folder):\n",
        "        \"\"\"\n",
        "        Loads animations from the specified folder.\n",
        "        \"\"\"\n",
        "        self.loaded = True\n",
        "        if not os.path.exists(animation_folder):\n",
        "            print(f\"Assets folder {animation_folder} not found!\")\n",
        "            return\n",
        "\n",
        "        for category in os.listdir(animation_folder):\n",
        "            category_path = os.path.join(animation_folder, category)\n",
        "            if os.path.isdir(category_path):\n",
        "                frames = []\n",
        "                for file in sorted(os.listdir(category_path)):\n",
        "                    file_name = os.path.splitext(file)[0]\n",
        "                    self.animations[file_name] = self.load_animation(os.path.join(category_path, file))\n",
        "            else:\n",
        "                file_name = os.path.splitext(category)[0]\n",
        "                self.animations[file_name] = self.load_animation(category_path)\n",
        "\n",
        "\n",
        "    def remap_colors(self, image, mapping):\n",
        "        \"\"\"\n",
        "        Given an image as a numpy ndarray (H x W x 3 or 4) and a mapping dictionary\n",
        "        mapping RGB tuples to new RGB tuples, return a new image with the colors replaced.\n",
        "        \"\"\"\n",
        "        # Make a copy so as not to modify the original.\n",
        "        out = image.copy()\n",
        "\n",
        "        # Determine whether the image has an alpha channel.\n",
        "        has_alpha = out.shape[2] == 4\n",
        "\n",
        "        # For each mapping entry, create a mask and replace the RGB channels.\n",
        "        for old_color, new_color in mapping.items():\n",
        "            # Create a boolean mask for pixels that match old_color.\n",
        "            # Compare only the first 3 channels.\n",
        "            mask = (out[..., :3] == old_color).all(axis=-1)\n",
        "\n",
        "            # Replace the pixel's R, G, B values with the new_color.\n",
        "            out[mask, 0] = new_color[0]\n",
        "            out[mask, 1] = new_color[1]\n",
        "            out[mask, 2] = new_color[2]\n",
        "            # The alpha channel (if present) remains unchanged.\n",
        "\n",
        "        return out\n",
        "\n",
        "\n",
        "    def load_animation(self, file_path):\n",
        "        # Load GIF and extract frames\n",
        "        gif = Image.open(file_path)\n",
        "        frames = []\n",
        "        frame_durations = []  # Store frame durations in milliseconds\n",
        "        total_duration = 0\n",
        "\n",
        "        # get file name without extension\n",
        "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
        "\n",
        "\n",
        "        for frame in ImageSequence.Iterator(gif):\n",
        "            # Convert and scale frame\n",
        "\n",
        "            pygame_frame = pygame.image.fromstring(frame.convert(\"RGBA\").tobytes(), frame.size, \"RGBA\")\n",
        "\n",
        "            # if self.agent_id == 1:\n",
        "            #     # Convert the pygame surface to a numpy array.\n",
        "            #     frame_array = pygame.surfarray.array3d(pygame_frame).transpose(1, 0, 2)  # shape (H, W, 3)\n",
        "\n",
        "            #     # Remap colors using our mapping.\n",
        "            #     new_frame_array = self.remap_colors(frame_array, self.color_mapping)\n",
        "\n",
        "            #     # Optionally, create a new pygame surface from the new_frame_array.\n",
        "            #     # (If you need to convert back to a surface, note that pygame expects (width, height).)\n",
        "            #     pygame_frame = pygame.surfarray.make_surface(new_frame_array.transpose(1, 0, 2))\n",
        "            #scaled_frame = pygame.transform.scale(pygame_frame, (int(frame.width * scale), int(frame.height * scale)))\n",
        "            frames.append(pygame_frame)\n",
        "\n",
        "            # Extract frame duration\n",
        "            duration = frame.info.get('duration', 100)  # Default 100ms if missing\n",
        "            frame_durations.append(duration)\n",
        "            total_duration += duration\n",
        "\n",
        "        gif.close()\n",
        "\n",
        "        # Compute how many game steps each GIF frame should last\n",
        "        frames_per_step = [max(1, round((duration / 1000) * self.ENV_FPS)) for duration in frame_durations]\n",
        "\n",
        "        return Animation(frames, frame_durations, frames_per_step)\n",
        "\n",
        "    def play(self, animation_name):\n",
        "        \"\"\"\n",
        "        Plays the given animation.\n",
        "        \"\"\"\n",
        "        if animation_name == None:\n",
        "            self.current_animation = None\n",
        "            return\n",
        "        if animation_name in self.animations and self.current_animation != animation_name:\n",
        "            #print(animation_name, 'from', self.current_animation)\n",
        "            self.current_animation = animation_name\n",
        "            self.frames = self.animations[animation_name].frames\n",
        "            self.current_data = self.anim_data.get(animation_name, self.anim_data['default'])\n",
        "            self.frame_durations = self.animations[animation_name].frame_durations\n",
        "            self.frames_per_step = self.animations[animation_name].frames_per_step\n",
        "            self.frame_timer = 0\n",
        "            self.current_frame_index = 0\n",
        "\n",
        "    def process(self, position):\n",
        "        \"\"\"\n",
        "        Advances the animation, ensuring it syncs properly with a 30 FPS game loop.\n",
        "        \"\"\"\n",
        "\n",
        "        self.position = position\n",
        "        if self.current_animation is None: return\n",
        "        if not self.finished:\n",
        "            self.frame_timer += 1  # Increment frame timer (game steps)\n",
        "\n",
        "            # Move to the next frame only when enough game steps have passed\n",
        "            if self.frame_timer >= self.frames_per_step[self.current_frame_index]:\n",
        "                self.frame_timer = 0\n",
        "                self.current_frame_index += 1\n",
        "                if self.current_frame_index >= len(self.frames):\n",
        "                    self.current_frame_index = 0\n",
        "                    #self.finished = True  # Mark for deletion\n",
        "\n",
        "    def render(self, camera: Camera, flipped: bool = False) -> None:\n",
        "        \"\"\"\n",
        "        Draws the current animation frame on the screen at a fixed position.\n",
        "        \"\"\"\n",
        "        if not self.loaded:\n",
        "            self.load_animations(self.animation_folder)\n",
        "        if self.current_animation is None or self.current_animation == '': return\n",
        "        if not self.finished:\n",
        "            #camera.canvas.blit(self.frames[self.current_frame_index], (0,0))\n",
        "            width = self.current_data[0]\n",
        "            self.draw_image(camera.canvas, self.frames[self.current_frame_index], self.position, self.scale * width, camera, flipped=flipped)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_-olBqx4-J7"
      },
      "source": [
        "#### Player GameObject"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "Tt6DNLXqsMaM"
      },
      "outputs": [],
      "source": [
        "class Player(GameObject):\n",
        "    PLAYER_RADIUS = 10\n",
        "\n",
        "    def __init__(self, env, agent_id: int, start_position=[0,0], color=[200, 200, 0, 255]):\n",
        "        self.env = env\n",
        "\n",
        "        self.delta = env.dt\n",
        "        self.agent_id = agent_id\n",
        "        self.space = self.env.space\n",
        "\n",
        "        hitbox_size = Capsule.get_hitbox_size(290//2, 320//2)\n",
        "        self.hurtbox_collider = CapsuleCollider(center=(0, 0), width=hitbox_size[0], height=hitbox_size[1])\n",
        "\n",
        "        self.start_position = start_position\n",
        "\n",
        "        # Create input handlers\n",
        "        self.input = PlayerInputHandler()\n",
        "\n",
        "        # Attack anim stuff\n",
        "\n",
        "        self.attack_anims = {\n",
        "            MoveType.NLIGHT : ('idle', 'unarmednlightfinisher'),\n",
        "            MoveType.DLIGHT : ('idle', 'unarmeddlight'),\n",
        "            MoveType.SLIGHT : ('alpunch', 'unarmedslight'),\n",
        "            MoveType.NSIG   : ('alup', {28: 'unarmednsig_held', 29: ('unarmednsig_paper', 'unarmednsig_rock', 'unarmednsig_scissors')}),\n",
        "            MoveType.DSIG   : ('idle', {26: 'unarmeddsig_held', 27: 'unarmeddsig_end'}),\n",
        "            MoveType.SSIG   : ('alssig', {21: 'unarmedssig_held', 22: 'unarmedssig_end'}),\n",
        "            MoveType.NAIR   : ('alup', 'unarmednlightnofinisher'),\n",
        "            MoveType.DAIR   : ('alpunch', 'unarmeddair'),\n",
        "            MoveType.SAIR   : ('alpunch', 'unarmedsair'),\n",
        "            MoveType.RECOVERY : ('alup', 'unarmedrecovery'),\n",
        "            MoveType.GROUNDPOUND : ('algroundpound', {16: ['unarmedgp', 'unarmedgp_held'], 17: 'unarmedgp_end', 18: 'unarmedgp_end', 19: 'unarmedgp_end'}),\n",
        "        }\n",
        "\n",
        "        # Create player states\n",
        "        self.states_types: dict[str, PlayerObjectState] = {\n",
        "            'walking': WalkingState,\n",
        "            'standing': StandingState,\n",
        "            'turnaround': TurnaroundState,\n",
        "            'air_turnaround': AirTurnaroundState,\n",
        "            'sprinting': SprintingState,\n",
        "            'stun': StunState,\n",
        "            'in_air': InAirState,\n",
        "            'dodge': DodgeState,\n",
        "            'attack': AttackState,\n",
        "            'dash': DashState,\n",
        "            'backdash': BackDashState,\n",
        "            'KO': KOState,\n",
        "            'taunt': TauntState,\n",
        "        }\n",
        "        self.state_mapping = {\n",
        "            'WalkingState': 0,\n",
        "            'StandingState': 1,\n",
        "            'TurnaroundState': 2,\n",
        "            'AirTurnaroundState': 3,\n",
        "            'SprintingState': 4,\n",
        "            'StunState': 5,\n",
        "            'InAirState': 6,\n",
        "            'DodgeState': 7,\n",
        "            'AttackState': 8,\n",
        "            'DashState': 9,\n",
        "            'BackDashState': 10,\n",
        "            'KOState': 11,\n",
        "            'TauntState': 12,\n",
        "        }\n",
        "\n",
        "        self.states: dict[str, PlayerObjectState] = {\n",
        "            state_name: state_type(self) for state_name, state_type in self.states_types.items()\n",
        "        }\n",
        "        self.state = self.states['in_air']\n",
        "        self.state.jumps_left = 0\n",
        "        self.state.jump_timer = 0\n",
        "        self.state.recoveries_left = 0\n",
        "        self.state.is_base = True\n",
        "\n",
        "        # Other living stats\n",
        "        self.facing = Facing.RIGHT if start_position[0] < 0 else Facing.LEFT\n",
        "        self.damage = 0\n",
        "        self.smoothXVel = 0\n",
        "        self.damage_taken_this_stock = 0\n",
        "        self.damage_taken_total = 0\n",
        "        self.damage_done = 0\n",
        "        self.stocks = 3\n",
        "\n",
        "        self.prev_x = start_position[0]\n",
        "        self.prev_y = start_position[1]\n",
        "        self.damage_velocity = (0, 0)\n",
        "        self.target_vel = (0, 0)\n",
        "\n",
        "        self.cur_action = np.zeros(10)\n",
        "\n",
        "        self.hitboxes_to_draw = []\n",
        "        self.points_to_draw = []\n",
        "\n",
        "        # PyMunk Params\n",
        "        x, y = self.start_position\n",
        "        width, height = 0.87, 1.0\n",
        "        self.mass = 1\n",
        "\n",
        "        # Create PyMunk Object\n",
        "        self.shape = pymunk.Poly.create_box(None, size=(width, height))\n",
        "        self.shape.collision_type = 3 if agent_id == 0 else 4\n",
        "        self.shape.owner = self\n",
        "        #self.moment = pymunk.moment_for_poly(self.mass, self.shape.get_vertices())\n",
        "        self.moment = 1e9\n",
        "        self.body = pymunk.Body(self.mass, self.moment)\n",
        "        self.shape.body = self.body\n",
        "        self.shape.body.position = (x, y)\n",
        "        self.shape.friction = 0.7\n",
        "        self.shape.color = color\n",
        "\n",
        "        # Parameters\n",
        "        self.move_speed = 6.75\n",
        "        self.jump_speed = 8.9\n",
        "        self.in_air_ease = 6.75 / self.env.fps\n",
        "        self.run_speed = 8\n",
        "        self.dash_speed = 10\n",
        "        self.backdash_speed = 4\n",
        "        self.turnaround_time = 4\n",
        "        self.taunt_time = 30\n",
        "        self.backdash_time = 7\n",
        "        self.dodge_time = 10\n",
        "        self.grounded_dodge_cooldown = 30\n",
        "        self.smoothTimeX = 0.33 * self.env.fps\n",
        "        self.air_dodge_cooldown = 82\n",
        "        self.invincible_time = self.env.fps * 3\n",
        "        self.jump_cooldown = self.env.fps * 0.5\n",
        "        self.dash_time = self.env.fps * 0.3\n",
        "        self.dash_cooldown = 8\n",
        "\n",
        "        # Signals\n",
        "        self.just_got_hit = False\n",
        "\n",
        "        self.state_str = 'InAirState'\n",
        "\n",
        "        self.space.add(self.shape, self.body)\n",
        "\n",
        "        # Assets\n",
        "        self.assets_loaded = False\n",
        "        animation_folder = 'assets'\n",
        "        if not os.path.exists(animation_folder):\n",
        "            self.load_assets()\n",
        "        self.animation_sprite_2d = AnimationSprite2D(self.env.camera, 1.0, 'assets/player', agent_id)\n",
        "        self.attack_sprite = AnimationSprite2D(self.env.camera, 2.0, 'assets/attacks', agent_id)\n",
        "\n",
        "    def get_obs(self) -> list[float]:\n",
        "\n",
        "        obs = []\n",
        "        pos = self.body.position\n",
        "        # Clamp values to [-1, 1] (or replace with proper normalization if needed)\n",
        "        x_norm = max(-18, min(18, pos.x))\n",
        "        y_norm = max(-7, min(7, pos.y))\n",
        "        obs.extend([x_norm, y_norm])\n",
        "\n",
        "        vel = self.body.velocity\n",
        "        vx_norm = max(-10.0, min(10.0, vel.x))\n",
        "        vy_norm = max(-10.0, min(10.0, vel.y))\n",
        "        obs.extend([vx_norm, vy_norm])\n",
        "\n",
        "        obs.append(1.0 if self.facing == Facing.RIGHT else 0.0)\n",
        "\n",
        "        grounded = 1.0 if self.is_on_floor() else 0.0\n",
        "        obs.append(grounded)\n",
        "\n",
        "        obs.append(0.0 if grounded == 1.0 else 1.0)\n",
        "\n",
        "        obs.append(float(self.state.jumps_left) if hasattr(self.state, 'jumps_left') else 0.0)\n",
        "\n",
        "        current_state_name = type(self.state).__name__\n",
        "        state_index = self.state_mapping.get(current_state_name, 0)\n",
        "        obs.append(float(state_index))\n",
        "\n",
        "        obs.append(float(self.state.recoveries_left) if hasattr(self.state, 'recoveries_left') else 0.0)\n",
        "\n",
        "        obs.append(float(self.state.dodge_timer) if hasattr(self.state, 'dodge_timer') else 0.0)\n",
        "\n",
        "        obs.append(float(self.state.stun_frames) if hasattr(self.state, 'stun_frames') else 0.0)\n",
        "\n",
        "        obs.append(float(self.damage) / 700.0)\n",
        "\n",
        "        # 12. Stocks – expected to be between 0 and 3.\n",
        "        obs.append(float(self.stocks))\n",
        "\n",
        "        # 13. Move type – if the state has a move_type attribute, otherwise 0.\n",
        "        obs.append(float(self.state.move_type) if hasattr(self.state, 'move_type') else 0.0)\n",
        "\n",
        "        return obs\n",
        "\n",
        "    def respawn(self) -> None:\n",
        "        self.body.position = self.start_position\n",
        "        self.body.velocity = pymunk.Vec2d(0, 0)\n",
        "        self.damage = 0\n",
        "        self.damage_taken_this_stock = 0\n",
        "        self.smoothXVel = 0\n",
        "        self.target_vel = (0, 0)\n",
        "\n",
        "    def apply_damage(self, damage_default: float, stun_dealt: int=0, velocity_dealt: Tuple[float, float]=(0,0)):\n",
        "        self.damage = min(700, self.damage + damage_default)\n",
        "        self.damage_taken_this_stock += damage_default\n",
        "        self.damage_taken_total += damage_default\n",
        "        self.damage_taken_this_frame += damage_default\n",
        "        self.state.stunned(stun_dealt)\n",
        "        scale = (1.024 / 320.0) * 12 # 0.165\n",
        "        self.damage_velocity = (velocity_dealt[0] * scale, velocity_dealt[1] * scale)\n",
        "\n",
        "        self.opponent.damage_done += damage_default\n",
        "\n",
        "    def load_assets(self):\n",
        "        if self.assets_loaded: return\n",
        "        if os.path.isdir('assets'): return\n",
        "\n",
        "        data_path = \"assets.zip\"\n",
        "        if not os.path.isfile(data_path):\n",
        "            print(\"Downloading assets.zip...\")\n",
        "            url = \"https://drive.google.com/file/d/1F2MJQ5enUPVtyi3s410PUuv8LiWr8qCz/view?usp=sharing\"\n",
        "            gdown.download(url, output=data_path, fuzzy=True)\n",
        "\n",
        "\n",
        "        # check if directory\n",
        "        !unzip -q \"/content/$data_path\"\n",
        "        print(\"Downloaded!\")\n",
        "\n",
        "        self.assets_loaded = True\n",
        "\n",
        "    def is_on_floor(self) -> bool:\n",
        "        old_cond = (abs(self.body.position.y - 1.540) < 0.03 and abs(self.body.position.x) < 5.77)\n",
        "        return self.shape.cache_bb().intersects(self.env.objects['ground'].shape.cache_bb()) or old_cond\n",
        "        #return abs(self.body.position.y - 1.540) < 0.03 and abs(self.body.position.x) < 5.77\n",
        "\n",
        "    def set_gravity_disabled(self, disabled:bool) -> None:\n",
        "        self.body.gravity_scale = 0 if disabled else 1\n",
        "\n",
        "    def render(self, screen, camera) -> None:\n",
        "        self.state.animate_player(camera)\n",
        "\n",
        "        position = self.body.position\n",
        "        self.animation_sprite_2d.process(position)\n",
        "        self.attack_sprite.process(position)\n",
        "        flipped = self.facing == Facing.LEFT\n",
        "        self.animation_sprite_2d.render(camera, flipped=flipped)\n",
        "        self.attack_sprite.render(camera, flipped=flipped)\n",
        "\n",
        "\n",
        "        hurtbox_offset = Capsule.get_hitbox_offset(0, 0)\n",
        "        hurtbox_offset = (hurtbox_offset[0] * int(self.facing), hurtbox_offset[1])\n",
        "        hurtbox_pos = (self.body.position[0] + hurtbox_offset[0], self.body.position[1] + hurtbox_offset[1])\n",
        "        hurtbox_data = np.array([\n",
        "            self.hurtbox_collider.center[0],\n",
        "            self.hurtbox_collider.center[1],\n",
        "            self.hurtbox_collider.width / (2 * WarehouseBrawl.BRAWL_TO_UNITS),\n",
        "            self.hurtbox_collider.height / (2 * WarehouseBrawl.BRAWL_TO_UNITS)\n",
        "        ])\n",
        "        Capsule.draw_hurtbox(camera, hurtbox_data, hurtbox_pos)\n",
        "\n",
        "        # Draw hitboxes\n",
        "        for hitbox in self.hitboxes_to_draw:\n",
        "            hitbox_offset = list(Capsule.get_hitbox_offset(hitbox['xOffset'], hitbox['yOffset']))\n",
        "            hitbox_offset[0] = hitbox_offset[0] * int(self.facing)\n",
        "            hitbox_pos = (self.body.position[0] + hitbox_offset[0], self.body.position[1] + hitbox_offset[1])\n",
        "            hitbox_data = np.array([\n",
        "                0,\n",
        "                0,\n",
        "                hitbox['width'],\n",
        "                hitbox['height']\n",
        "            ])\n",
        "            Capsule.draw_hitbox(camera, hitbox_data, hitbox_pos)\n",
        "\n",
        "        # draw circle\n",
        "        cc = (227, 138, 14) if self.agent_id == 0 else (18, 131, 201)\n",
        "        screen_pos = camera.gtp((int(position[0]), int(position[1])-1))\n",
        "        pygame.draw.circle(camera.canvas, cc, screen_pos, camera.scale_gtp() * 0.25)\n",
        "\n",
        "\n",
        "    def set_hitboxes_to_draw(self, hitboxes: Optional[List[Any]]=None,\n",
        "                             points: Optional[List[Any]]=None,\n",
        "                             move_facing: Optional[Facing]=None):\n",
        "        if hitboxes is None:\n",
        "            self.hitboxes_to_draw = []\n",
        "        else:\n",
        "            self.facing = move_facing\n",
        "            self.hitboxes_to_draw = hitboxes\n",
        "            self.points_to_draw = points\n",
        "\n",
        "    def smooth_damp(current, target, current_velocity, smooth_time, dt=0.016):\n",
        "        # This is a very rough approximation.\n",
        "        # In a real implementation, you'd compute the damped value properly.\n",
        "        diff = target - current\n",
        "        change = diff * dt / smooth_time if smooth_time != 0 else diff\n",
        "        new_value = current + change\n",
        "        new_velocity = change / dt\n",
        "        return new_value, new_velocity\n",
        "\n",
        "    def do_cast_frame_changes(self):\n",
        "        # Create a new CastFrameChangeHolder and force hurtbox change.\n",
        "        reset_holder = CastFrameChangeHolder()\n",
        "        # Activate the hurtbox change.\n",
        "        reset_holder.hurtbox_position_change.active = True\n",
        "\n",
        "        hpc = reset_holder.hurtbox_position_change\n",
        "        # Get the hurtbox offset from the utility.\n",
        "        hurtbox_offset = Capsule.get_hitbox_offset(hpc.xOffset, hpc.yOffset)\n",
        "        # Multiply the x component by the agent's facing direction.\n",
        "        hurtbox_offset = (hurtbox_offset[0] * int(self.facing), hurtbox_offset[1])\n",
        "        # Apply to the hurtbox collider.\n",
        "        self.hurtbox_collider.offset = hurtbox_offset\n",
        "        size = Capsule.get_hitbox_size(hpc.width, hpc.height)\n",
        "        self.hurtbox_collider.size = (2.0 * size[0], 2.0 * size[1])\n",
        "\n",
        "    # --- Second version: with changes, floor drag, and move manager ---\n",
        "    def do_cast_frame_changes_with_changes(self, changes, enable_floor_drag, mm):\n",
        "        # If floor drag is enabled, smooth-damp the x velocity toward 0.\n",
        "        if enable_floor_drag:\n",
        "            vel_x = self.move_toward(self.body.velocity.x, 0, self.in_air_ease)\n",
        "            self.body.velocity = pymunk.Vec2d(vel_x, self.body.velocity.y)\n",
        "\n",
        "        if changes is None:\n",
        "            return\n",
        "\n",
        "        # Process hurtbox position change.\n",
        "        hpc = changes.hurtbox_position_change\n",
        "        if hpc is not None and hpc.active:\n",
        "            hurtbox_offset = Capsule.get_hitbox_offset(hpc.xOffset, hpc.yOffset)\n",
        "            hurtbox_offset = (hurtbox_offset[0] * int(mm.move_facing_direction), hurtbox_offset[1])\n",
        "            # Set collider direction based on dimensions.\n",
        "\n",
        "            self.hurtbox_collider.offset = hurtbox_offset\n",
        "            size = Capsule.get_hitbox_size(hpc.width, hpc.height)\n",
        "            self.hurtbox_collider.size = (2.0 * size[0], 2.0 * size[1])\n",
        "\n",
        "        # Process caster position change (if any; currently no action).\n",
        "        cpc = changes.caster_position_change\n",
        "        if cpc is not None and cpc.active:\n",
        "            # Implement caster position change if needed.\n",
        "            pass\n",
        "\n",
        "        # Process dealt position target changes.\n",
        "        # (The original code has a commented-out block; here we check if the current power has a target.)\n",
        "        if hasattr(self.state, 'move_manager') and self.state.move_manager.current_power.dealt_position_target_exists:\n",
        "            mm = self.state.move_manager\n",
        "\n",
        "            target_pos = Capsule.get_hitbox_offset(mm.current_power.current_dealt_position_target[0],\n",
        "                                                               mm.current_power.current_dealt_position_target[1])\n",
        "            target_pos = (target_pos[0] * int(mm.move_facing_direction), target_pos[1])\n",
        "            # Assume self.position is available as self.position.\n",
        "            current_pos = self.body.position  # (x, y, z)\n",
        "            if mm.current_power.power_data.get(\"targetAllHitAgents\", False):\n",
        "                for agent in mm.all_hit_agents:\n",
        "                    # Compute a new velocity vector.\n",
        "                    vel = tuple(0.5 * ((current_pos[i] + target_pos[i] - agent.body.position[i])) for i in range(2))\n",
        "                    agent.set_position_target_vel(vel)\n",
        "            elif mm.hit_agent is not None:\n",
        "                vel = tuple(0.5 * ((current_pos[i] + target_pos[i] - mm.hit_agent.body.position[i])) for i in range(2))\n",
        "                mm.hit_agent.set_position_target_vel(vel)\n",
        "\n",
        "        # Process caster velocity set.\n",
        "        cvs = changes.caster_velocity_set\n",
        "        if cvs is not None and cvs.active:\n",
        "            angle_rad = math.radians(cvs.directionDeg)\n",
        "            vel = (math.cos(angle_rad) * cvs.magnitude, -math.sin(angle_rad) * cvs.magnitude)\n",
        "            vel = (vel[0] * int(mm.move_facing_direction), vel[1])\n",
        "            self.body.velocity = pymunk.Vec2d(vel[0], vel[1])\n",
        "\n",
        "        # Process caster velocity set XY.\n",
        "        cvsxy = changes.caster_velocity_set_xy\n",
        "        if cvsxy is not None:\n",
        "            vx, vy = self.body.velocity\n",
        "            if getattr(cvsxy, 'activeX', False):\n",
        "                vx = cvsxy.magnitudeX * int(mm.move_facing_direction)\n",
        "            if getattr(cvsxy, 'activeY', False):\n",
        "                vy = cvsxy.magnitudeY\n",
        "            self.body.velocity = pymunk.Vec2d(vx, vy)\n",
        "\n",
        "        # Process caster velocity damp XY.\n",
        "        cvdxy = changes.caster_velocity_damp_xy\n",
        "        if cvdxy is not None:\n",
        "            vx, vy = self.body.velocity\n",
        "            if getattr(cvdxy, 'activeX', False):\n",
        "                vx *= cvdxy.dampX\n",
        "            if getattr(cvdxy, 'activeY', False):\n",
        "                vy *= cvdxy.dampY\n",
        "            self.body.velocity = pymunk.Vec2d(vx, vy)\n",
        "\n",
        "    def get_move(self) -> MoveType:\n",
        "        # Assuming that 'p' is a Player instance and that p.input is an instance of PlayerInputHandler.\n",
        "        # Also assume that p.input.update(action) has already been called.\n",
        "\n",
        "        # Determine move types:\n",
        "        heavy_move = self.input.key_status['k'].held         # heavy move if key 'k' is held\n",
        "        light_move = (not heavy_move) and self.input.key_status['j'].held  # light move if not heavy and key 'j' is held\n",
        "        throw_move = (not heavy_move) and (not light_move) and self.input.key_status['h'].held  # throw if pickup key 'h' is held\n",
        "\n",
        "        # Determine directional keys:\n",
        "        left_key = self.input.key_status[\"A\"].held            # left key (A)\n",
        "        right_key = self.input.key_status[\"D\"].held           # right key (D)\n",
        "        up_key = self.input.key_status[\"W\"].held              # aim up (W)\n",
        "        down_key = self.input.key_status[\"S\"].held            # aim down (S)\n",
        "\n",
        "        # Calculate combined directions:\n",
        "        side_key = left_key or right_key\n",
        "\n",
        "        # Calculate move direction:\n",
        "        neutral_move = ((not side_key) and (not down_key)) or up_key\n",
        "        down_move = (not neutral_move) and down_key\n",
        "        side_move = (not neutral_move) and (not down_key) and side_key\n",
        "\n",
        "        # Check if any move key (light, heavy, or throw) is pressed:\n",
        "        hitting_any_move_key = light_move or heavy_move or throw_move\n",
        "        if not hitting_any_move_key:\n",
        "            move_type = MoveType.NONE\n",
        "        else:\n",
        "            # (Optional) Print the results:\n",
        "            # print(\"heavy_move:\", heavy_move)\n",
        "            # print(\"light_move:\", light_move)\n",
        "            # print(\"throw_move:\", throw_move)\n",
        "            # print(\"neutral_move:\", neutral_move)\n",
        "            # print(\"down_move:\", down_move)\n",
        "            # print(\"side_move:\", side_move)\n",
        "            # print(\"hitting_any_move_key:\", hitting_any_move_key)\n",
        "            cms = CompactMoveState(self.is_on_floor(), heavy_move, 0 if neutral_move else (1 if down_move else 2))\n",
        "            move_type = m_state_to_move[cms]\n",
        "            #print(move_type)\n",
        "        return move_type\n",
        "\n",
        "    def pre_process(self) -> None:\n",
        "        self.damage_taken_this_frame = 0\n",
        "\n",
        "    def process(self, action: np.ndarray) -> None:\n",
        "        self.cur_action = action\n",
        "        if not hasattr(self, 'opponent'):\n",
        "            self.opponent = self.env.players[1-self.agent_id]\n",
        "        #if self.env.steps == 2: self.animation_sprite_2d.play('altroll')\n",
        "        # Process inputs\n",
        "        self.input.update(action)\n",
        "        #self.direction = [action[0] - action[1], action[2] - action[3]]\n",
        "\n",
        "        # Reward: TO DELETE\n",
        "        multiple = 1 if self.body.position.x < 0 else -1\n",
        "        self.env.add_reward(self.agent_id, multiple * (self.body.position.x - self.prev_x))\n",
        "\n",
        "    def physics_process(self, delta: float) -> None:\n",
        "        new_state: PlayerObjectState = self.state.physics_process(delta)\n",
        "        self.hurtbox_collider.center = self.body.position\n",
        "        self.body.velocity = (self.body.velocity.x + self.damage_velocity[0] + self.target_vel[0],\n",
        "                              self.body.velocity.y + self.damage_velocity[1] + self.target_vel[1])\n",
        "\n",
        "\n",
        "        if new_state is not None:\n",
        "            new_state.reset(self.state)\n",
        "            self.state.exit()\n",
        "            self.state_str = f'{type(self.state).__name__} -> {type(new_state).__name__}'\n",
        "\n",
        "            #print()\n",
        "            self.state = new_state\n",
        "            self.state.enter()\n",
        "        log = {\n",
        "            'transition': self.state_str\n",
        "        }\n",
        "\n",
        "        if hasattr(self.state, 'move_type'):\n",
        "            log['move_type'] = self.state.move_type\n",
        "        self.env.logger[self.agent_id] = log\n",
        "\n",
        "        #self.body.velocity = pymunk.Vec2d(self.direction[0] * self.move_speed, self.body.velocity.y)\n",
        "        #self.body.velocity = pymunk.Vec2d(self.direction[0] * self.move_speed, self.direction[1] * self.move_speed)\n",
        "\n",
        "        self.prev_x = self.body.position.x\n",
        "        self.prev_y = self.body.position.y\n",
        "        self.damage_velocity = (0, 0)\n",
        "        self.target_vel = (0, 0)\n",
        "\n",
        "    def set_position_target_vel(self, vel: Tuple[float, float]) -> None:\n",
        "        self.target_vel = vel\n",
        "\n",
        "\n",
        "    @staticmethod\n",
        "    def move_toward(current: float, target: float, delta: float) -> float:\n",
        "        \"\"\"\n",
        "        Moves 'current' toward 'target' by 'delta' amount, but will not overshoot 'target'.\n",
        "        If delta is negative, it moves away from 'target'.\n",
        "\n",
        "        Examples:\n",
        "        move_toward(5, 10, 4)    -> 9\n",
        "        move_toward(10, 5, 4)    -> 6\n",
        "        move_toward(5, 10, 9)    -> 10\n",
        "        move_toward(10, 5, -1.5) -> 11.5\n",
        "        \"\"\"\n",
        "        # If current already equals target, return target immediately.\n",
        "        if current == target:\n",
        "            return target\n",
        "\n",
        "        # Calculate the difference and determine the movement direction.\n",
        "        diff = target - current\n",
        "        direction = diff / abs(diff)  # +1 if target > current, -1 if target < current\n",
        "\n",
        "        if delta >= 0:\n",
        "            # Move toward target: add (delta * direction)\n",
        "            candidate = current + delta * direction\n",
        "            # Clamp so we do not overshoot target.\n",
        "            if direction > 0:\n",
        "                return min(candidate, target)\n",
        "            else:\n",
        "                return max(candidate, target)\n",
        "        else:\n",
        "            # Move away from target: subtract (|delta| * direction)\n",
        "            # (This reverses the movement direction relative to the vector toward target.)\n",
        "            return current - abs(delta) * direction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYrlmB7ymkBY"
      },
      "source": [
        "### Hitbox and Hurtbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "fJBceI7V8fJO"
      },
      "outputs": [],
      "source": [
        "import pygame\n",
        "import math\n",
        "\n",
        "class Capsule():\n",
        "\n",
        "    def __init__(self):\n",
        "        pass\n",
        "\n",
        "    @staticmethod\n",
        "    def drawArc(surface, center, r, th, start, stop, color):\n",
        "        x, y = center\n",
        "        points_outer = []\n",
        "        points_inner = []\n",
        "        n = round(r*abs(stop-start))\n",
        "        if n<2:\n",
        "            n = 2\n",
        "        if n>30: n = 30\n",
        "        for i in range(n):\n",
        "            delta = i/(n-1)\n",
        "            phi0 = start + (stop-start)*delta\n",
        "            x0 = round(x+r*math.cos(phi0))\n",
        "            y0 = round(y+r*math.sin(phi0))\n",
        "            points_outer.append([x0,y0])\n",
        "            phi1 = stop + (start-stop)*delta\n",
        "            x1 = round(x+(r-th)*math.cos(phi1))\n",
        "            y1 = round(y+(r-th)*math.sin(phi1))\n",
        "            points_inner.append([x1,y1])\n",
        "        points = points_outer + points_inner\n",
        "        pygame.gfxdraw.aapolygon(surface, points, color)\n",
        "        pygame.gfxdraw.filled_polygon(surface, points, color)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_hitbox_offset(x_offset, y_offset):\n",
        "        \"\"\"\n",
        "        Converts offset values into world coordinates.\n",
        "        \"\"\"\n",
        "        return (x_offset * 2 * WarehouseBrawl.BRAWL_TO_UNITS,\n",
        "                y_offset * 2 * WarehouseBrawl.BRAWL_TO_UNITS)\n",
        "\n",
        "    @staticmethod\n",
        "    def get_hitbox_size(width, height):\n",
        "        \"\"\"\n",
        "        Converts hitbox width and height into world coordinates.\n",
        "        \"\"\"\n",
        "        return (width * 2 * WarehouseBrawl.BRAWL_TO_UNITS,\n",
        "                height * 2 * WarehouseBrawl.BRAWL_TO_UNITS)\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_hitbox(camera: Camera, hitbox: np.ndarray, pos):\n",
        "        \"\"\"\n",
        "        Draws a rounded rectangle (capsule) on the screen using PyGame.\n",
        "        \"\"\"\n",
        "        Capsule.draw_hithurtbox(camera, hitbox, pos, color=(255, 0, 0))\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_hurtbox(camera: Camera, hitbox: np.ndarray, pos):\n",
        "        \"\"\"\n",
        "        Draws a rounded rectangle (capsule) on the screen using PyGame.\n",
        "        \"\"\"\n",
        "        Capsule.draw_hithurtbox(camera, hitbox, pos, color=(247, 215, 5))\n",
        "\n",
        "    @staticmethod\n",
        "    def draw_hithurtbox(camera: Camera, hitbox: np.ndarray, pos: bool, color=(255, 0, 0)):\n",
        "        \"\"\"\n",
        "        Draws a rounded rectangle (capsule) on the screen using PyGame.\n",
        "        \"\"\"\n",
        "\n",
        "        # Get canvas\n",
        "        canvas = camera.canvas\n",
        "\n",
        "        # Hitbox: [x_offset, y_offset, width, height]\n",
        "        x_offset, y_offset, width, height = hitbox\n",
        "\n",
        "        # Convert from brawl units to game units\n",
        "        size = Capsule.get_hitbox_size(width, height)\n",
        "        x_offset, y_offset = Capsule.get_hitbox_offset(x_offset, y_offset)\n",
        "\n",
        "        # Combine offset and position\n",
        "        pos = np.array(pos) + np.array([x_offset, y_offset])\n",
        "\n",
        "        # Convert to pixels using camera intrinsics\n",
        "        scale_cst = camera.scale_gtp()\n",
        "        size = (size[0] * scale_cst, size[1] * scale_cst)\n",
        "        pos = camera.gtp(pos)\n",
        "\n",
        "        rect = pygame.Rect(pos[0] - size[0] // 2,\n",
        "                           pos[1] - size[1] // 2,\n",
        "                           size[0], size[1])\n",
        "\n",
        "        if width < height:\n",
        "            # Vertical Capsule\n",
        "            radius = size[0] // 2\n",
        "            half_height = size[1] // 2\n",
        "            circle_height = half_height - radius\n",
        "\n",
        "            Capsule.drawArc(canvas, (pos[0], pos[1] - circle_height), radius, 2, math.pi, 2 * math.pi, color)\n",
        "            Capsule.drawArc(canvas, (pos[0], pos[1] + circle_height), radius, 2, 0, math.pi, color)\n",
        "            pygame.draw.line(canvas, color, (rect.left, rect.top + radius), (rect.left, rect.bottom - radius), 2)\n",
        "            pygame.draw.line(canvas, color, (rect.right-2, rect.top + radius), (rect.right-2, rect.bottom - radius), 2)\n",
        "\n",
        "        elif width == height:\n",
        "            # Circular Capsule\n",
        "            pygame.draw.circle(canvas, color, (rect.centerx, rect.centery), size[0] // 2, 2)\n",
        "\n",
        "        else:\n",
        "            # Horizontal Capsule\n",
        "            radius = size[1] // 2\n",
        "            half_width = size[0] // 2\n",
        "            circle_width = half_width - radius\n",
        "\n",
        "            Capsule.drawArc(canvas, (pos[0] + circle_width, pos[1]), radius, 2, 1.5 * math.pi, 2.5 * math.pi, color)\n",
        "            Capsule.drawArc(canvas, (pos[0] - circle_width, pos[1]), radius, 2, 0.5 * math.pi, 1.5 * math.pi, color)\n",
        "            pygame.draw.line(canvas, color, (rect.left + radius, rect.top), (rect.right - radius, rect.top), 2)\n",
        "            pygame.draw.line(canvas, color, (rect.left + radius, rect.bottom-2), (rect.right - radius, rect.bottom-2), 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def check_collision(hitbox_pos, width, height, collidables):\n",
        "        \"\"\"\n",
        "        Checks for collision between the hitbox and a list of collidable objects.\n",
        "\n",
        "        :param hitbox_pos: (x, y) position of the hitbox center.\n",
        "        :param width: Width of the hitbox.\n",
        "        :param height: Height of the hitbox.\n",
        "        :param collidables: A list of PyGame Rect objects representing collidable objects.\n",
        "        :return: List of colliding objects.\n",
        "        \"\"\"\n",
        "        size = Capsule.get_hitbox_size(width, height)\n",
        "        hitbox_rect = pygame.Rect(hitbox_pos[0] - size[0] // 2,\n",
        "                                  hitbox_pos[1] - size[1] // 2,\n",
        "                                  size[0], size[1])\n",
        "\n",
        "        collisions = [obj for obj in collidables if hitbox_rect.colliderect(obj)]\n",
        "        return collisions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "4-HHm2qHwDm0"
      },
      "outputs": [],
      "source": [
        "class CapsuleCollider():\n",
        "    def __init__(self, center, width, height, is_hurtbox=False):\n",
        "        \"\"\"\n",
        "        :param center: (x, y) position of the capsule's center.\n",
        "        :param width: Width of the capsule.\n",
        "        :param height: Height of the capsule.\n",
        "        \"\"\"\n",
        "        self.center = pygame.Vector2(center)\n",
        "        self.width = width\n",
        "        self.height = height\n",
        "        self.radius = min(width, height) / 2  # Radius of cap circles\n",
        "        self.is_circle = width == height  # If it's a perfect circle\n",
        "\n",
        "    def draw(self, camera) -> None:\n",
        "        # use Capsule to draw this\n",
        "        Capsule.draw_hitbox(camera, [0, 0, self.width, self.height], self.center, facing_right=True)\n",
        "\n",
        "    def __str__(self) -> str:\n",
        "        return f\"CapsuleCollider(center={self.center}, width={self.width}, height={self.height})\"\n",
        "\n",
        "    def update(self):\n",
        "        # Define the main body rectangle\n",
        "        center, width, height = self.center, self.width, self.height\n",
        "        if not self.is_circle:\n",
        "            if width < height:\n",
        "                self.rect = pygame.Rect(center[0] - width / 2, center[1] - (height / 2 - self.radius),\n",
        "                                        width, height - 2 * self.radius)\n",
        "                self.cap1 = pygame.Vector2(center[0], center[1] - (height / 2 - self.radius))  # Top circle\n",
        "                self.cap2 = pygame.Vector2(center[0], center[1] + (height / 2 - self.radius))  # Bottom circle\n",
        "            else:\n",
        "                self.rect = pygame.Rect(center[0] - (width / 2 - self.radius), center[1] - height / 2,\n",
        "                                        width - 2 * self.radius, height)\n",
        "                self.cap1 = pygame.Vector2(center[0] - (width / 2 - self.radius), center[1])  # Left circle\n",
        "                self.cap2 = pygame.Vector2(center[0] + (width / 2 - self.radius), center[1])  # Right circle\n",
        "        else:\n",
        "            self.rect = None\n",
        "            self.cap1 = self.center  # Single circle\n",
        "\n",
        "    def intersects(self, other):\n",
        "        \"\"\"\n",
        "        Checks if this capsule collider intersects with another.\n",
        "\n",
        "        :param other: Another CapsuleCollider object.\n",
        "        :return: True if colliding, False otherwise.\n",
        "        \"\"\"\n",
        "        self.update()\n",
        "        other.update()\n",
        "\n",
        "\n",
        "        # Case 1: If both are circles (width == height)\n",
        "        if self.is_circle and other.is_circle:\n",
        "            collided = self._circle_circle_collision(self.cap1, self.radius, other.cap1, other.radius)\n",
        "\n",
        "        # Case 2: If this is a circle but the other is a capsule\n",
        "        elif self.is_circle:\n",
        "            collided = (self._circle_circle_collision(self.cap1, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap1, self.radius, other.cap2, other.radius) or\n",
        "                    self._circle_rectangle_collision(self.cap1, self.radius, other.rect))\n",
        "\n",
        "        # Case 3: If the other is a circle but this is a capsule\n",
        "        elif other.is_circle:\n",
        "            collided = (self._circle_circle_collision(self.cap1, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap2, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_rectangle_collision(other.cap1, other.radius, self.rect))\n",
        "\n",
        "        # Case 4: Both are capsules\n",
        "        else:\n",
        "            collided = (self._circle_circle_collision(self.cap1, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap1, self.radius, other.cap2, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap2, self.radius, other.cap1, other.radius) or\n",
        "                    self._circle_circle_collision(self.cap2, self.radius, other.cap2, other.radius) or\n",
        "                    self._rectangle_rectangle_collision(self.rect, other.rect) or\n",
        "                    self._circle_rectangle_collision(self.cap1, self.radius, other.rect) or\n",
        "                    self._circle_rectangle_collision(self.cap2, self.radius, other.rect) or\n",
        "                    self._circle_rectangle_collision(other.cap1, other.radius, self.rect) or\n",
        "                    self._circle_rectangle_collision(other.cap2, other.radius, self.rect))\n",
        "        #if collided:\n",
        "            #print(self, other)\n",
        "        return collided\n",
        "\n",
        "    def _circle_circle_collision(self, center1, radius1, center2, radius2):\n",
        "        \"\"\"Check if two circles intersect.\"\"\"\n",
        "        return center1.distance_to(center2) < (radius1 + radius2)\n",
        "\n",
        "    def _rectangle_rectangle_collision(self, rect1, rect2):\n",
        "        \"\"\"Check if two rectangles overlap.\"\"\"\n",
        "        return rect1.colliderect(rect2)\n",
        "\n",
        "    def _circle_rectangle_collision(self, circle_center, circle_radius, rect):\n",
        "        \"\"\"Check if a circle and a rectangle overlap.\"\"\"\n",
        "        if rect is None:\n",
        "            return False  # If one of them is a pure circle, no need to check rectangle\n",
        "\n",
        "        # Find the closest point on the rectangle to the circle center\n",
        "        closest_x = max(rect.left, min(circle_center.x, rect.right))\n",
        "        closest_y = max(rect.top, min(circle_center.y, rect.bottom))\n",
        "\n",
        "        # Calculate the distance from this closest point to the circle center\n",
        "        return circle_center.distance_to(pygame.Vector2(closest_x, closest_y)) < circle_radius"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bGcNyfuaZWZL"
      },
      "source": [
        "### Animation Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "ZaH1TRFxmmY7"
      },
      "outputs": [],
      "source": [
        "class Particle(GameObject):\n",
        "    ENV_FPS = 30  # Environment FPS\n",
        "\n",
        "    def __init__(self, env, position, gif_path: str, scale: float = 1.0):\n",
        "        \"\"\"\n",
        "        A temporary particle that plays an animation once and deletes itself.\n",
        "\n",
        "        - `position`: The world position where the animation should be played.\n",
        "        - `gif_path`: Path to the GIF animation.\n",
        "        - `scale`: Scale factor for resizing frames.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.env = env\n",
        "        self.position = position\n",
        "        self.finished = False\n",
        "        self.scale = scale\n",
        "        self.current_frame_index = 0\n",
        "        self.frame_timer = 0\n",
        "\n",
        "        # Load GIF and extract frames\n",
        "        gif = Image.open(gif_path)\n",
        "        self.frames = []\n",
        "        self.frame_durations = []  # Store frame durations in milliseconds\n",
        "        total_duration = 0\n",
        "\n",
        "        for frame in ImageSequence.Iterator(gif):\n",
        "            # Convert and scale frame\n",
        "            pygame_frame = pygame.image.fromstring(frame.convert(\"RGBA\").tobytes(), frame.size, \"RGBA\")\n",
        "            scaled_frame = pygame.transform.scale(pygame_frame, (int(frame.width * scale), int(frame.height * scale)))\n",
        "            self.frames.append(scaled_frame)\n",
        "\n",
        "            # Extract frame duration\n",
        "            duration = frame.info.get('duration', 100)  # Default 100ms if missing\n",
        "            self.frame_durations.append(duration)\n",
        "            total_duration += duration\n",
        "\n",
        "        # Compute how many game steps each GIF frame should last\n",
        "        self.frames_per_step = [max(1, round((duration / 1000) * self.ENV_FPS)) for duration in self.frame_durations]\n",
        "\n",
        "    def process(self):\n",
        "        \"\"\"\n",
        "        Advances the animation, ensuring it syncs properly with a 30 FPS game loop.\n",
        "        \"\"\"\n",
        "        self.position = self.env.objects['opponent'].body.position\n",
        "        if not self.finished:\n",
        "            self.frame_timer += 1  # Increment frame timer (game steps)\n",
        "\n",
        "            # Move to the next frame only when enough game steps have passed\n",
        "            if self.frame_timer >= self.frames_per_step[self.current_frame_index]:\n",
        "                self.frame_timer = 0\n",
        "                self.current_frame_index += 1\n",
        "                if self.current_frame_index >= len(self.frames):\n",
        "                    self.current_frame_index = 0\n",
        "                    #self.finished = True  # Mark for deletion\n",
        "\n",
        "    def render(self, canvas: pygame.Surface, camera: Camera) -> None:\n",
        "        \"\"\"\n",
        "        Draws the current animation frame on the screen at a fixed position.\n",
        "        \"\"\"\n",
        "\n",
        "        # Define collidable objects (e.g., players)\n",
        "        player_rect = pygame.Rect(300, 400, 50, 50)  # A player hitbox\n",
        "        collidables = [player_rect]\n",
        "\n",
        "        # Define a hitbox\n",
        "        hitbox_pos = (0, 3)\n",
        "        hitbox_pos = self.position\n",
        "        hitbox = np.array([0, 0, 32, 480])\n",
        "\n",
        "        # Draw the hitbox\n",
        "        #Capsule.draw_hitbox(camera, hitbox, hitbox_pos)\n",
        "\n",
        "        # Check for collisions\n",
        "        #colliding_objects = BrawlHitboxUtility.check_collision(hitbox_pos, hitbox_width, hitbox_height, collidables)\n",
        "        #if colliding_objects:\n",
        "        #    print(\"Collision detected!\")\n",
        "\n",
        "        if not self.finished:\n",
        "            screen_pos = camera.gtp(self.position)\n",
        "            screen_pos = (0,0)\n",
        "            #canvas.blit(self.frames[self.current_frame_index], screen_pos)\n",
        "            self.draw_image(canvas, self.frames[self.current_frame_index], self.position, 2, camera)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2khY6i4Iq297"
      },
      "source": [
        "## Agents"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlEWjV-ere9z"
      },
      "source": [
        "### Agent Abstract Base Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "Q4c8fEMfreUF"
      },
      "outputs": [],
      "source": [
        "SelfAgent = TypeVar(\"SelfAgent\", bound=\"Agent\")\n",
        "\n",
        "class Agent(ABC):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            file_path: Optional[str] = None\n",
        "        ):\n",
        "\n",
        "        # If no supplied file_path, load from gdown (optional file_path returned)\n",
        "        if file_path is None:\n",
        "            file_path = self._gdown()\n",
        "\n",
        "        self.file_path: Optional[str] = file_path\n",
        "        self.initialized = False\n",
        "\n",
        "    def get_env_info(self, env):\n",
        "        if isinstance(env, Monitor):\n",
        "            self_env = env.env\n",
        "        else:\n",
        "            self_env = env\n",
        "        self.observation_space = self_env.observation_space\n",
        "        self.obs_helper = self_env.obs_helper\n",
        "        self.action_space = self_env.action_space\n",
        "        self.act_helper = self_env.act_helper\n",
        "        self.env = env\n",
        "        self._initialize()\n",
        "        self.initialized = True\n",
        "\n",
        "    def get_num_timesteps(self) -> int:\n",
        "        if hasattr(self, 'model'):\n",
        "            return self.model.num_timesteps\n",
        "        else:\n",
        "            return 0\n",
        "\n",
        "    def update_num_timesteps(self, num_timesteps: int) -> None:\n",
        "        if hasattr(self, 'model'):\n",
        "            self.model.num_timesteps = num_timesteps\n",
        "\n",
        "    @abstractmethod\n",
        "    def predict(self, obs) -> spaces.Space:\n",
        "        pass\n",
        "\n",
        "    def save(self, file_path: str) -> None:\n",
        "        return\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        return\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        \"\"\"\n",
        "\n",
        "        \"\"\"\n",
        "        return\n",
        "\n",
        "    def _gdown(self) -> Optional[str]:\n",
        "        \"\"\"\n",
        "        Loads the necessary file from Google Drive, returning a file path.\n",
        "        Or, returns None, if the agent does not require loaded files.\n",
        "\n",
        "        :return:\n",
        "        \"\"\"\n",
        "        return"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83MTxzRvh58N"
      },
      "source": [
        "### Agent Classes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2XLXjSF-h4EQ"
      },
      "outputs": [],
      "source": [
        "class ConstantAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = np.zeros_like(self.action_space.sample())\n",
        "        return action\n",
        "\n",
        "class RandomAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = self.action_space.sample()\n",
        "        return action"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bpulsG6R6eGG"
      },
      "source": [
        "## StableBaselines3 Integration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MRa5uPgibFAm"
      },
      "source": [
        "### Reward Configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "ysWnnzGza-P7"
      },
      "outputs": [],
      "source": [
        "@dataclass\n",
        "class RewTerm():\n",
        "    \"\"\"Configuration for a reward term.\"\"\"\n",
        "\n",
        "    func: Callable[..., torch.Tensor] = MISSING\n",
        "    \"\"\"The name of the function to be called.\n",
        "\n",
        "    This function should take the environment object and any other parameters\n",
        "    as input and return the reward signals as torch float tensors of\n",
        "    shape (num_envs,).\n",
        "    \"\"\"\n",
        "\n",
        "    weight: float = MISSING\n",
        "    \"\"\"The weight of the reward term.\n",
        "\n",
        "    This is multiplied with the reward term's value to compute the final\n",
        "    reward.\n",
        "\n",
        "    Note:\n",
        "        If the weight is zero, the reward term is ignored.\n",
        "    \"\"\"\n",
        "\n",
        "    params: dict[str, Any] = field(default_factory=dict)\n",
        "    \"\"\"The parameters to be passed to the function as keyword arguments. Defaults to an empty dict.\n",
        "\n",
        "    .. note::\n",
        "        If the value is a :class:`SceneEntityCfg` object, the manager will query the scene entity\n",
        "        from the :class:`InteractiveScene` and process the entity's joints and bodies as specified\n",
        "        in the :class:`SceneEntityCfg` object.\n",
        "    \"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "w35uogzHqVVL"
      },
      "outputs": [],
      "source": [
        "class RewardManager():\n",
        "    \"\"\"Reward terms for the MDP.\"\"\"\n",
        "\n",
        "    # (1) Constant running reward\n",
        "    def __init__(self,\n",
        "                 reward_functions: Optional[Dict[str, RewTerm]]=None,\n",
        "                 signal_subscriptions: Optional[Dict[str, Tuple[str, RewTerm]]]=None) -> None:\n",
        "        self.reward_functions = reward_functions\n",
        "        self.signal_subscriptions = signal_subscriptions\n",
        "        self.total_reward = 0.0\n",
        "        self.collected_signal_rewards = 0.0\n",
        "\n",
        "    def subscribe_signals(self, env) -> None:\n",
        "        if self.signal_subscriptions is None:\n",
        "            return\n",
        "        for _, (name, term_cfg) in self.signal_subscriptions.items():\n",
        "            getattr(env, name).connect(partial(self._signal_func, term_cfg))\n",
        "\n",
        "    def _signal_func(self, term_cfg: RewTerm, *args, **kwargs):\n",
        "        term_partial = partial(term_cfg.func, **term_cfg.params)\n",
        "        self.collected_signal_rewards += term_partial(*args, **kwargs) * term_cfg.weight\n",
        "\n",
        "\n",
        "    def process(self, env, dt) -> float:\n",
        "        # reset computation\n",
        "        reward_buffer = 0.0\n",
        "        # iterate over all the reward terms\n",
        "        if self.reward_functions is not None:\n",
        "            for name, term_cfg in self.reward_functions.items():\n",
        "                # skip if weight is zero (kind of a micro-optimization)\n",
        "                if term_cfg.weight == 0.0:\n",
        "                    continue\n",
        "                # compute term's value\n",
        "                value = term_cfg.func(env, **term_cfg.params) * term_cfg.weight\n",
        "                # update total reward\n",
        "                reward_buffer += value\n",
        "\n",
        "        reward = reward_buffer + self.collected_signal_rewards\n",
        "        self.collected_signal_rewards = 0.0\n",
        "\n",
        "        self.total_reward += reward\n",
        "\n",
        "        log = env.logger[0]\n",
        "        log['reward'] = f'{reward_buffer:.3f}'\n",
        "        log['total_reward'] = f'{self.total_reward:.3f}'\n",
        "        env.logger[0] = log\n",
        "        return reward\n",
        "\n",
        "    def reset(self):\n",
        "        self.total_reward = 0\n",
        "        self.collected_signal_rewards"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxD2EcJGrjvj"
      },
      "source": [
        "### Save, Self-play, and Opponents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "5U4IT7f4rj8a"
      },
      "outputs": [],
      "source": [
        "class SaveHandlerMode(Enum):\n",
        "    FORCE = 0\n",
        "    RESUME = 1\n",
        "\n",
        "class SaveHandler():\n",
        "    \"\"\"Handles saving.\n",
        "\n",
        "    Args:\n",
        "        agent (Agent): Agent to save.\n",
        "        save_freq (int): Number of steps between saving.\n",
        "        max_saved (int): Maximum number of saved models.\n",
        "        save_dir (str): Directory to save models.\n",
        "        name_prefix (str): Prefix for saved models.\n",
        "    \"\"\"\n",
        "\n",
        "    # System for saving to internet\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            agent: Agent,\n",
        "            save_freq: int=10_000,\n",
        "            max_saved: int=20,\n",
        "            run_name: str='experiment_1',\n",
        "            save_path: str='checkpoints',\n",
        "            name_prefix: str = \"rl_model\",\n",
        "            mode: SaveHandlerMode=SaveHandlerMode.FORCE\n",
        "        ):\n",
        "        self.agent = agent\n",
        "        self.save_freq = save_freq\n",
        "        self.run_name = run_name\n",
        "        self.max_saved = max_saved\n",
        "        self.save_path = save_path\n",
        "        self.name_prefix = name_prefix\n",
        "        self.mode = mode\n",
        "\n",
        "        self.steps_until_save = save_freq\n",
        "        # Get model paths from exp_path, if it exists\n",
        "        exp_path = self._experiment_path()\n",
        "        self.history: List[str] = []\n",
        "        if self.mode == SaveHandlerMode.FORCE:\n",
        "            # Clear old dir\n",
        "            if os.path.exists(exp_path) and len(os.listdir(exp_path)) != 0:\n",
        "                while True:\n",
        "                    answer = input(f\"Would you like to clear the folder {exp_path} (SaveHandlerMode.FORCE): yes (y) or no (n): \").strip().lower()\n",
        "                    if answer in ('y', 'n'):\n",
        "                        break\n",
        "                    else:\n",
        "                        print(\"Invalid input, please enter 'y' or 'n'.\")\n",
        "\n",
        "                if answer == 'n':\n",
        "                    raise ValueError('Please switch to SaveHandlerMode.FORCE or use a new run_name.')\n",
        "                print(f'Clearing {exp_path}...')\n",
        "                if os.path.exists(exp_path):\n",
        "                    shutil.rmtree(exp_path)\n",
        "            else:\n",
        "                print(f'{exp_path} empty or does not exist. Creating...')\n",
        "\n",
        "            if not os.path.exists(exp_path):\n",
        "                os.makedirs(exp_path)\n",
        "        elif self.mode == SaveHandlerMode.RESUME:\n",
        "            if os.path.exists(exp_path):\n",
        "                # Get all model paths\n",
        "                self.history = [os.path.join(exp_path, f) for f in os.listdir(exp_path) if os.path.isfile(os.path.join(exp_path, f))]\n",
        "                # Filter any non .csv\n",
        "                self.history = [f for f in self.history if f.endswith('.zip')]\n",
        "                if len(self.history) != 0:\n",
        "                    self.history.sort(key=lambda x: int(os.path.basename(x).split('_')[-2].split('.')[0]))\n",
        "                    if max_saved != -1: self.history = self.history[-max_saved:]\n",
        "                    print(f'Best model is {self.history[-1]}')\n",
        "                else:\n",
        "                    print(f'No models found in {exp_path}.')\n",
        "                    raise FileNotFoundError\n",
        "            else:\n",
        "                print(f'No file found at {exp_path}')\n",
        "\n",
        "\n",
        "    def update_info(self) -> None:\n",
        "        self.num_timesteps = self.agent.get_num_timesteps()\n",
        "\n",
        "    def _experiment_path(self) -> str:\n",
        "        \"\"\"\n",
        "        Helper to get experiment path for each type of checkpoint.\n",
        "\n",
        "        :param extension: Checkpoint file extension (zip for model, pkl for others)\n",
        "        :return: Path to the checkpoint\n",
        "        \"\"\"\n",
        "        return os.path.join(self.save_path, self.run_name)\n",
        "\n",
        "    def _checkpoint_path(self, extension: str = '') -> str:\n",
        "        \"\"\"\n",
        "        Helper to get checkpoint path for each type of checkpoint.\n",
        "\n",
        "        :param extension: Checkpoint file extension (zip for model, pkl for others)\n",
        "        :return: Path to the checkpoint\n",
        "        \"\"\"\n",
        "        return os.path.join(self._experiment_path(), f\"{self.name_prefix}_{self.num_timesteps}_steps.{extension}\")\n",
        "\n",
        "    def save_agent(self) -> None:\n",
        "        print(f\"Saving agent to {self._checkpoint_path()}\")\n",
        "        model_path = self._checkpoint_path('zip')\n",
        "        self.agent.save(model_path)\n",
        "        self.history.append(model_path)\n",
        "        if self.max_saved != -1 and len(self.history) > self.max_saved:\n",
        "            os.remove(self.history.pop(0))\n",
        "\n",
        "    def process(self) -> bool:\n",
        "        self.num_timesteps += 1\n",
        "\n",
        "        if self.steps_until_save <= 0:\n",
        "            # Save agent\n",
        "            self.steps_until_save = self.save_freq\n",
        "            self.save_agent()\n",
        "            return True\n",
        "        self.steps_until_save -= 1\n",
        "\n",
        "        return False\n",
        "\n",
        "    def get_random_model_path(self) -> str:\n",
        "        if len(self.history) == 0:\n",
        "            return None\n",
        "        return random.choice(self.history)\n",
        "\n",
        "    def get_latest_model_path(self) -> str:\n",
        "        if len(self.history) == 0:\n",
        "            return None\n",
        "        return self.history[-1]\n",
        "\n",
        "class SelfPlayHandler(ABC):\n",
        "    \"\"\"Handles self-play.\"\"\"\n",
        "\n",
        "    def __init__(self, agent_partial: partial):\n",
        "        self.agent_partial = agent_partial\n",
        "\n",
        "    def get_model_from_path(self, path) -> Agent:\n",
        "        if path:\n",
        "            try:\n",
        "                opponent = self.agent_partial(file_path=path)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Warning: Self-play file {path} not found. Defaulting to constant agent.\")\n",
        "                opponent = ConstantAgent()\n",
        "        else:\n",
        "            print(\"Warning: No self-play model saved. Defaulting to constant agent.\")\n",
        "            opponent = ConstantAgent()\n",
        "        opponent.get_env_info(self.env)\n",
        "        return opponent\n",
        "\n",
        "    @abstractmethod\n",
        "    def get_opponent(self) -> Agent:\n",
        "        pass\n",
        "\n",
        "\n",
        "class SelfPlayLatest(SelfPlayHandler):\n",
        "    def __init__(self, agent_partial: partial):\n",
        "        super().__init__(agent_partial)\n",
        "\n",
        "    def get_opponent(self) -> Agent:\n",
        "        assert self.save_handler is not None, \"Save handler must be specified for self-play\"\n",
        "        chosen_path = self.save_handler.get_latest_model_path()\n",
        "        return self.get_model_from_path(chosen_path)\n",
        "\n",
        "class SelfPlayDynamic(SelfPlayHandler):\n",
        "    def __init__(self, agent_partial: partial):\n",
        "        super().__init__(agent_partial)\n",
        "\n",
        "    # @NotImplementedError\n",
        "    def get_opponent(self) -> Agent:\n",
        "        assert self.save_handler is not None, \"Save handler must be specified for self-play\"\n",
        "        assert self.save_handler.max_saved == -1, \"Save handler must have max_saved=-1 for dynamic self-play (save all past opponents)\"\n",
        "        chosen_path = self.save_handler.get_random_model_path()\n",
        "        return self.get_model_from_path(chosen_path)\n",
        "\n",
        "class SelfPlayRandom(SelfPlayHandler):\n",
        "    def __init__(self, agent_partial: partial):\n",
        "        super().__init__(agent_partial)\n",
        "\n",
        "    def get_opponent(self) -> Agent:\n",
        "        assert self.save_handler is not None, \"Save handler must be specified for self-play\"\n",
        "        chosen_path = self.save_handler.get_random_model_path()\n",
        "        return self.get_model_from_path(chosen_path)\n",
        "\n",
        "@dataclass\n",
        "class OpponentsCfg():\n",
        "    \"\"\"Configuration for opponents.\n",
        "\n",
        "    Args:\n",
        "        swap_steps (int): Number of steps between swapping opponents.\n",
        "        opponents (dict): Dictionary specifying available opponents and their selection probabilities.\n",
        "    \"\"\"\n",
        "    swap_steps: int = 10_000\n",
        "    opponents: dict[str, Any] = field(default_factory=lambda: {\n",
        "                'random_agent': (0.8, partial(RandomAgent)),\n",
        "                'constant_agent': (0.2, partial(ConstantAgent)),\n",
        "                #'recurrent_agent': (0.1, partial(RecurrentPPOAgent, file_path='skibidi')),\n",
        "            })\n",
        "\n",
        "    def validate_probabilities(self) -> None:\n",
        "        total_prob = sum(prob if isinstance(prob, float) else prob[0] for prob in self.opponents.values())\n",
        "\n",
        "        if abs(total_prob - 1.0) > 1e-5:\n",
        "            print(f\"Warning: Probabilities do not sum to 1 (current sum = {total_prob}). Normalizing...\")\n",
        "            self.opponents = {\n",
        "                key: (value / total_prob if isinstance(value, float) else (value[0] / total_prob, value[1]))\n",
        "                for key, value in self.opponents.items()\n",
        "            }\n",
        "\n",
        "    def process(self) -> None:\n",
        "        pass\n",
        "\n",
        "    def on_env_reset(self) -> Agent:\n",
        "\n",
        "        agent_name = random.choices(\n",
        "            list(self.opponents.keys()),\n",
        "            weights=[prob if isinstance(prob, float) else prob[0] for prob in self.opponents.values()]\n",
        "        )[0]\n",
        "\n",
        "        # If self-play is selected, return the trained model\n",
        "        print(f'Selected {agent_name}')\n",
        "        if agent_name == \"self_play\":\n",
        "            selfplay_handler: SelfPlayHandler = self.opponents[agent_name][1]\n",
        "            return selfplay_handler.get_opponent()\n",
        "        else:\n",
        "            # Otherwise, return an instance of the selected agent class\n",
        "            opponent = self.opponents[agent_name][1]()\n",
        "\n",
        "        opponent.get_env_info(self.env)\n",
        "        return opponent"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "404LO62Oqsqz"
      },
      "source": [
        "### Self-Play Warehouse Brawl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "gr1VxOWM5pBp"
      },
      "outputs": [],
      "source": [
        "class SelfPlayWarehouseBrawl(gymnasium.Env):\n",
        "    \"\"\"Custom Environment that follows gym interface.\"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 30}\n",
        "\n",
        "    def __init__(self,\n",
        "                 reward_manager: Optional[RewardManager]=None,\n",
        "                 opponent_cfg: OpponentsCfg=OpponentsCfg(),\n",
        "                 save_handler: Optional[SaveHandler]=None,\n",
        "                 render_every: int | None = None,\n",
        "                 resolution: CameraResolution=CameraResolution.LOW):\n",
        "        \"\"\"\n",
        "        Initializes the environment.\n",
        "\n",
        "        Args:\n",
        "            reward_manager (Optional[RewardManager]): Reward manager.\n",
        "            opponent_cfg (OpponentCfg): Configuration for opponents.\n",
        "            save_handler (SaveHandler): Configuration for self-play.\n",
        "            render_every (int | None): Number of steps between a demo render (None if no rendering).\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.reward_manager = reward_manager\n",
        "        self.save_handler = save_handler\n",
        "        self.opponent_cfg = opponent_cfg\n",
        "        self.render_every = render_every\n",
        "        self.resolution = resolution\n",
        "\n",
        "        self.games_done = 0\n",
        "\n",
        "\n",
        "        # Give OpponentCfg references, and normalize probabilities\n",
        "        self.opponent_cfg.env = self\n",
        "        self.opponent_cfg.validate_probabilities()\n",
        "\n",
        "        # Check if using self-play\n",
        "        if (selfplay_data := self.opponent_cfg.opponents.get('self_play')) is not None:\n",
        "            assert self.save_handler is not None, \"Save handler must be specified for self-play\"\n",
        "\n",
        "            # Give SelfPlayHandler references\n",
        "            selfplay_handler: SelfPlayHandler = selfplay_data[1]\n",
        "            selfplay_handler.save_handler = self.save_handler\n",
        "            selfplay_handler.env = self\n",
        "\n",
        "        self.raw_env = WarehouseBrawl(resolution=resolution)\n",
        "        self.action_space = self.raw_env.action_space\n",
        "        self.act_helper = self.raw_env.act_helper\n",
        "        self.observation_space = self.raw_env.observation_space\n",
        "        self.obs_helper = self.raw_env.obs_helper\n",
        "\n",
        "    def on_training_start(self):\n",
        "        # Update SaveHandler\n",
        "        if self.save_handler is not None:\n",
        "            self.save_handler.update_info()\n",
        "\n",
        "    def on_training_end(self):\n",
        "        if self.save_handler is not None:\n",
        "            self.save_handler.agent.update_num_timesteps(self.save_handler.num_timesteps)\n",
        "            self.save_handler.save_agent()\n",
        "\n",
        "    def step(self, action):\n",
        "\n",
        "        full_action = {\n",
        "            0: action,\n",
        "            1: self.opponent_agent.predict(self.opponent_obs),\n",
        "        }\n",
        "\n",
        "        observations, rewards, terminated, truncated, info = self.raw_env.step(full_action)\n",
        "\n",
        "        if self.save_handler is not None:\n",
        "            self.save_handler.process()\n",
        "\n",
        "        if self.reward_manager is None:\n",
        "            reward = rewards[0]\n",
        "        else:\n",
        "            reward = self.reward_manager.process(self.raw_env, 1 / 30.0)\n",
        "\n",
        "        return observations[0], reward, terminated, truncated, info\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        # Reset MalachiteEnv\n",
        "        observations, info = self.raw_env.reset()\n",
        "\n",
        "        self.reward_manager.reset()\n",
        "\n",
        "        # Select agent\n",
        "        new_agent: Agent = self.opponent_cfg.on_env_reset()\n",
        "        if new_agent is not None:\n",
        "            self.opponent_agent: Agent = new_agent\n",
        "        self.opponent_obs = observations[1]\n",
        "\n",
        "\n",
        "        self.games_done += 1\n",
        "        #if self.games_done % self.render_every == 0:\n",
        "            #self.render_out_video()\n",
        "\n",
        "        return observations[0], info\n",
        "\n",
        "    def render(self):\n",
        "        img = self.raw_env.render()\n",
        "        return img\n",
        "\n",
        "    def close(self):\n",
        "        pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyCz9XRL0tLW"
      },
      "source": [
        "## Run Match"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "ed2wOQ-S0zUv"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "from tqdm import tqdm\n",
        "\n",
        "def run_match(agent_1: Agent | partial,\n",
        "              agent_2: Agent | partial,\n",
        "              max_timesteps=30*90,\n",
        "              video_path: Optional[str]=None,\n",
        "              agent_1_name: Optional[str]=None,\n",
        "              agent_2_name: Optional[str]=None,\n",
        "              resolution = CameraResolution.LOW,\n",
        "              reward_manager: Optional[RewardManager]=None,\n",
        "              train_mode=False\n",
        "              ) -> MatchStats:\n",
        "    # Initialize env\n",
        "    env = WarehouseBrawl(resolution=resolution, train_mode=train_mode)\n",
        "    observations, infos = env.reset()\n",
        "    obs_1 = observations[0]\n",
        "    obs_2 = observations[1]\n",
        "\n",
        "    if reward_manager is not None:\n",
        "        reward_manager.reset()\n",
        "        reward_manager.subscribe_signals(env)\n",
        "\n",
        "    if agent_1_name is None:\n",
        "        agent_1_name = 'agent_1'\n",
        "    if agent_2_name is None:\n",
        "        agent_2_name = 'agent_2'\n",
        "\n",
        "    env.agent_1_name = agent_1_name\n",
        "    env.agent_2_name = agent_2_name\n",
        "\n",
        "\n",
        "    writer = None\n",
        "    if video_path is None:\n",
        "        print(\"video_path=None -> Not rendering\")\n",
        "    else:\n",
        "        print(f\"video_path={video_path} -> Rendering\")\n",
        "        # Initialize video writer\n",
        "        writer = skvideo.io.FFmpegWriter(video_path, outputdict={\n",
        "            '-vcodec': 'libx264',  # Use H.264 for Windows Media Player\n",
        "            '-pix_fmt': 'yuv420p',  # Compatible with both WMP & Colab\n",
        "            '-preset': 'fast',  # Faster encoding\n",
        "            '-crf': '20',  # Quality-based encoding (lower = better quality)\n",
        "            '-r': '30'  # Frame rate\n",
        "        })\n",
        "\n",
        "    # If partial\n",
        "    if callable(agent_1):\n",
        "        agent_1 = agent_1()\n",
        "    if callable(agent_2):\n",
        "        agent_2 = agent_2()\n",
        "\n",
        "    # Initialize agents\n",
        "    if not agent_1.initialized: agent_1.get_env_info(env)\n",
        "    if not agent_2.initialized: agent_2.get_env_info(env)\n",
        "    # 596, 336\n",
        "\n",
        "    for _ in tqdm(range(max_timesteps), total=max_timesteps):\n",
        "        # actions = {agent: agents[agent].predict(None) for agent in range(2)}\n",
        "\n",
        "        # observations, rewards, terminations, truncations, infos\n",
        "\n",
        "        full_action = {\n",
        "            0: agent_1.predict(obs_1),\n",
        "            1: agent_2.predict(obs_2)\n",
        "        }\n",
        "\n",
        "        observations, rewards, terminated, truncated, info = env.step(full_action)\n",
        "        obs_1 = observations[0]\n",
        "        obs_2 = observations[1]\n",
        "\n",
        "        if reward_manager is not None:\n",
        "            reward_manager.process(env, 1 / env.fps)\n",
        "\n",
        "        if video_path is not None:\n",
        "            img = env.render()\n",
        "            writer.writeFrame(img)\n",
        "            del img\n",
        "\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "        #env.show_image(img)\n",
        "\n",
        "    if video_path is not None:\n",
        "        writer.close()\n",
        "\n",
        "    env.close()\n",
        "\n",
        "\n",
        "    # visualize\n",
        "    # Video(video_path, embed=True, width=800) if video_path is not None else None\n",
        "    player_1_stats = env.get_stats(0)\n",
        "    player_2_stats = env.get_stats(1)\n",
        "    match_stats = MatchStats(\n",
        "        match_time=env.steps / env.fps,\n",
        "        player1=player_1_stats,\n",
        "        player2=player_2_stats,\n",
        "        player1_result=Result.WIN if player_1_stats.lives_left > player_2_stats.lives_left else Result.LOSS\n",
        "    )\n",
        "\n",
        "    del env\n",
        "\n",
        "    return match_stats"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdErHuFqNSFl"
      },
      "source": [
        "# SUBMISSION: Additional Imports\n",
        "Note that all the imports up to this point (for the Malachite Env, WarehouseBrawl, etc...) will be automatically included in the submission, so you need not write them.\n",
        "\n",
        "Requirements:\n",
        "- **DO NOT** import any modules beyond the following code block. They will not be parsed and may cause your submission to fail validation.\n",
        "- Only write imports that have not been used above this code block\n",
        "- Only write imports that are from libraries listed here"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "K_Ln88TlNTxX"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import PPO, A2C, SAC # Sample RL Algo imports\n",
        "from sb3_contrib import RecurrentPPO # Importing an LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8HOfcO1p-TK"
      },
      "source": [
        "# SUBMISSION: Agent\n",
        "This will be the Agent class we run in the 1v1. We've started you off with a functioning RL agent (`SB3Agent(Agent)`) and if-statement agent (`BasedAgent(Agent)`). Feel free to copy either to `SubmittedAgent(Agent)` then begin modifying.\n",
        "\n",
        "Requirements:\n",
        "- Your submission **MUST** be of type `SubmittedAgent(Agent)`\n",
        "- Any instantiated classes **MUST** be defined within and below this code block.\n",
        "\n",
        "Remember, your agent can be either machine learning, OR if-statement based. I've seen many successful agents arising purely from if-statements - give them a shot as well, if ML is too complicated at first!!\n",
        "\n",
        "Also PLEASE ask us questions in the Discord server if any of the API is confusing. We'd be more than happy to clarify and get the team on the right track.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class SubmittedAgent(Agent):\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_path: Optional[str] = None,\n",
        "        learning_rate: float = 3e-4,\n",
        "        n_steps: int = 2048,\n",
        "        batch_size: int = 128,\n",
        "        n_epochs: int = 10,\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        ent_coef: float = 0.03,     \n",
        "        clip_range: float = 0.2,\n",
        "        verbose: int = 1,\n",
        "        max_grad_norm: float = 0.5,\n",
        "        device: str = \"auto\",\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_steps = n_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.ent_coef = ent_coef\n",
        "        self.clip_range = clip_range\n",
        "        self.verbose = verbose\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.device = device\n",
        "\n",
        "        self.lstm_states = None\n",
        "        self.episode_starts = True\n",
        "        self.time = 0\n",
        "\n",
        "        super().__init__(file_path)\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        \"\"\"Set up PPO model\"\"\"\n",
        "        if self.file_path is None:\n",
        "            # Deep MLP architecture optimized for fighting games\n",
        "            policy_kwargs = dict(\n",
        "                net_arch=dict(\n",
        "                    pi=[64, 128, 256, 128, 64],  # Deep policy network\n",
        "                    vf=[64, 128, 256, 128, 64],  # Deep value network\n",
        "                ),\n",
        "                activation_fn=torch.nn.SiLU,\n",
        "                ortho_init=True,\n",
        "            )\n",
        "\n",
        "            self.model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                self.env,\n",
        "                learning_rate=self.learning_rate,\n",
        "                n_steps=self.n_steps,\n",
        "                batch_size=self.batch_size,\n",
        "                n_epochs=self.n_epochs,\n",
        "                gamma=self.gamma,\n",
        "                gae_lambda=self.gae_lambda,\n",
        "                ent_coef=self.ent_coef,\n",
        "                clip_range=self.clip_range,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=self.verbose,\n",
        "                normalize_advantage=True,\n",
        "                max_grad_norm=self.max_grad_norm,\n",
        "                device=self.device,\n",
        "            )\n",
        "            del self.env\n",
        "        else:\n",
        "            self.model = PPO.load(\n",
        "                self.file_path, n_steps=30 * 90 * 3, batch_size=128, device=self.device\n",
        "            )\n",
        "\n",
        "    # def predict(self, obs):\n",
        "    #     action, _ = self.model.predict(obs, deterministic=True)\n",
        "    #     return action\n",
        "\n",
        "    def predict(self, obs):\n",
        "        \"\"\"Get action with deterministic prediction\"\"\"\n",
        "        # First check if we're about to fall off the platform\n",
        "        player_pos = self.obs_helper.get_section(obs, \"player_pos\")\n",
        "        player_vel = self.obs_helper.get_section(obs, \"player_vel\")\n",
        "        player_grounded = self.obs_helper.get_section(obs, \"player_grounded\")\n",
        "\n",
        "        # Platform boundaries\n",
        "        platform_left = -10.67 / 2\n",
        "        platform_right = 10.67 / 2\n",
        "        platform_top = 1.75\n",
        "\n",
        "        # Initialize action\n",
        "        action = np.zeros_like(self.action_space.sample())\n",
        "\n",
        "        # Check if we're near the edge or above the platform\n",
        "        danger_zone = 2  # Buffer zone from the edge\n",
        "\n",
        "        # If we're in danger of falling off or already off\n",
        "        if (\n",
        "            (player_pos[0] < platform_left + danger_zone)\n",
        "            or (player_pos[0] > platform_right - danger_zone)\n",
        "            or (player_pos[1] > platform_top)\n",
        "        ):\n",
        "\n",
        "            # Move toward center of platform\n",
        "            if player_pos[0] < 0:\n",
        "                action = self.act_helper.press_keys([\"d\"])\n",
        "            else:\n",
        "                action = self.act_helper.press_keys([\"a\"])\n",
        "\n",
        "            # Jump if we're not on the ground\n",
        "            action = self.act_helper.press_keys([\"space\"], action)\n",
        "\n",
        "            return action\n",
        "\n",
        "        # If we're safe, use the model's prediction\n",
        "        model_action, _ = self.model.predict(obs, deterministic=True)\n",
        "        return model_action\n",
        "\n",
        "    \n",
        "    def save(self, file_path: str) -> None:\n",
        "        self.model.save(file_path)\n",
        "\n",
        "    def learn(self, env, total_timesteps, log_interval: int = 1, verbose=0):\n",
        "        self.model.set_env(env)\n",
        "        self.model.verbose = verbose\n",
        "\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            log_interval=log_interval,\n",
        "        )\n",
        "\n",
        "    # def _gdown(self) -> str:\n",
        "    #     data_path = \"rl-model.zip\"\n",
        "    #     if not os.path.isfile(data_path):\n",
        "    #         print(f\"Downloading {data_path}...\")\n",
        "    #         url = \"https://drive.google.com/file/d/1mYJ07TYJ3Qo_wYLjYT6pQIlaRDepuAav/view?usp=sharing\"\n",
        "    #         gdown.download(url, output=data_path, fuzzy=True)\n",
        "    #     return data_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfKVniFCp1ux"
      },
      "source": [
        "# Training\n",
        "\n",
        "Here, you can set the reward functions and train your agent. If you'd like to write a heuristic (if-statement) agent, you can also reference the Example Agents here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MMYCnbOrXs6"
      },
      "source": [
        "## Example Agent Classes\n",
        "Reference these to design a gamut of opponents for your model to face off against!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "QCST5qk0wnf8"
      },
      "outputs": [],
      "source": [
        "# Recall the possible observations\n",
        "# Set name='player', or name='opponent'\n",
        "# obs_helper.get_section(obs, f\"{name}_pos\") # low=[-1, -1], high=[1, 1]\n",
        "# obs_helper.get_section(obs, f\"{name}_facing\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_vel\") # low=[-1, -1], high=[1, 1]\n",
        "# obs_helper.get_section(obs, f\"{name}_grounded\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_aerial\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_jumps_left\") # low=[0], high=[2]\n",
        "# obs_helper.get_section(obs, f\"{name}_state\") # low=[0], high=[12]\n",
        "# obs_helper.get_section(obs, f\"{name}_recoveries_left\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_dodge_timer\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_stun_frames\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_damage\") # low=[0], high=[1]\n",
        "# obs_helper.get_section(obs, f\"{name}_stocks\") # low=[0], high=[3]\n",
        "# obs_helper.get_section(obs, f\"{name}_move_type\") # low=[0], high=[11]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "DBpCptggHEdK"
      },
      "outputs": [],
      "source": [
        "# player_state and opponent_state map to this\n",
        "# state_mapping = {\n",
        "#             'WalkingState': 0,\n",
        "#             'StandingState': 1,\n",
        "#             'TurnaroundState': 2,\n",
        "#             'AirTurnaroundState': 3,\n",
        "#             'SprintingState': 4,\n",
        "#             'StunState': 5,\n",
        "#             'InAirState': 6,\n",
        "#             'DodgeState': 7,\n",
        "#             'AttackState': 8,\n",
        "#             'DashState': 9,\n",
        "#             'BackDashState': 10,\n",
        "#             'KOState': 11,\n",
        "#             'TauntState': 12,\n",
        "#         }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {},
      "outputs": [],
      "source": [
        "# observations from the environment. These observations provide information about the current state of the environment, \n",
        "# including the positions, velocities, states, and other attributes of the entities (such as players and opponents) \n",
        "# within the environment.\n",
        "\n",
        "# example of obs:\n",
        "# obs = {\n",
        "#     \"player_pos\": [0.5, 0.2],\n",
        "#     \"player_facing\": [1],\n",
        "#     \"player_vel\": [0.1, -0.2],\n",
        "#     \"player_grounded\": [1],\n",
        "#     \"player_aerial\": [0],\n",
        "#     \"player_jumps_left\": [2],\n",
        "#     \"player_state\": [3],\n",
        "#     \"player_recoveries_left\": [1],\n",
        "#     \"player_dodge_timer\": [0.5],\n",
        "#     \"player_stun_frames\": [0],\n",
        "#     \"player_damage\": [0.3],\n",
        "#     \"player_stocks\": [3],\n",
        "#     \"player_move_type\": [5],\n",
        "#     \"opponent_pos\": [-0.5, 0.2],\n",
        "#     \"opponent_facing\": [0],\n",
        "#     \"opponent_vel\": [-0.1, 0.2],\n",
        "#     \"opponent_grounded\": [1],\n",
        "#     \"opponent_aerial\": [0],\n",
        "#     \"opponent_jumps_left\": [1],\n",
        "#     \"opponent_state\": [2],\n",
        "#     \"opponent_recoveries_left\": [1],\n",
        "#     \"opponent_dodge_timer\": [0.3],\n",
        "#     \"opponent_stun_frames\": [0],\n",
        "#     \"opponent_damage\": [0.4],\n",
        "#     \"opponent_stocks\": [2],\n",
        "#     \"opponent_move_type\": [6]\n",
        "# }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [],
      "source": [
        "# self.stage_width_tiles: float = 29.8\n",
        "# self.stage_height_tiles: float = 16.8\n",
        "\n",
        "# origin at (0, 0) (center of the environment)\n",
        "\n",
        "# platform width = 10.67\n",
        "# right = 5.335\n",
        "# left = -5.335\n",
        "\n",
        "# KO happens when abs(x) > 14 or abs(y) > 8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "id": "zNS45A00n82h"
      },
      "outputs": [],
      "source": [
        "class ConstantAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = np.zeros_like(self.action_space.sample())\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "xKEpMi3Qo81L"
      },
      "outputs": [],
      "source": [
        "class RandomAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = self.action_space.sample()\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "id": "ivsxuwzXwfU5"
      },
      "outputs": [],
      "source": [
        "class BasedAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        self.time = 0\n",
        "\n",
        "    def predict(self, obs):\n",
        "        self.time += 1\n",
        "        pos = self.obs_helper.get_section(obs, 'player_pos')\n",
        "        opp_pos = self.obs_helper.get_section(obs, 'opponent_pos')\n",
        "        opp_KO = self.obs_helper.get_section(obs, 'opponent_state') in [5, 11]\n",
        "        action = self.act_helper.zeros()\n",
        "\n",
        "        # If off the edge, come back\n",
        "        if pos[0] > 10.67/2:\n",
        "            action = self.act_helper.press_keys(['a'])\n",
        "        elif pos[0] < -10.67/2:\n",
        "            action = self.act_helper.press_keys(['d'])\n",
        "        elif not opp_KO:\n",
        "            # Head toward opponent\n",
        "            if (opp_pos[0] > pos[0]):\n",
        "                action = self.act_helper.press_keys(['d'])\n",
        "            else:\n",
        "                action = self.act_helper.press_keys(['a'])\n",
        "\n",
        "        # Note: Passing in partial action\n",
        "        # Jump if below map or opponent is above you\n",
        "        if (pos[1] > 1.6 or pos[1] > opp_pos[1]) and self.time % 2 == 0:\n",
        "            action = self.act_helper.press_keys(['space'], action)\n",
        "\n",
        "        # Attack if near\n",
        "        if (pos[0] - opp_pos[0])**2 + (pos[1] - opp_pos[1])**2 < 4.0:\n",
        "            action = self.act_helper.press_keys(['j'], action)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "mKpJyPUenUk6"
      },
      "outputs": [],
      "source": [
        "class UserInputAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action = self.act_helper.zeros()\n",
        "\n",
        "        keys = pygame.key.get_pressed()\n",
        "        if keys[pygame.K_w]:\n",
        "            action = self.act_helper.press_keys(['w'], action)\n",
        "        if keys[pygame.K_a]:\n",
        "            action = self.act_helper.press_keys(['a'], action)\n",
        "        if keys[pygame.K_s]:\n",
        "            action = self.act_helper.press_keys(['s'], action)\n",
        "        if keys[pygame.K_d]:\n",
        "            action = self.act_helper.press_keys(['d'], action)\n",
        "        if keys[pygame.K_SPACE]:\n",
        "            action = self.act_helper.press_keys(['space'], action)\n",
        "        # h j k l\n",
        "        if keys[pygame.K_h]:\n",
        "            action = self.act_helper.press_keys(['h'], action)\n",
        "        if keys[pygame.K_j]:\n",
        "            action = self.act_helper.press_keys(['j'], action)\n",
        "        if keys[pygame.K_k]:\n",
        "            action = self.act_helper.press_keys(['k'], action)\n",
        "        if keys[pygame.K_l]:\n",
        "            action = self.act_helper.press_keys(['l'], action)\n",
        "        if keys[pygame.K_g]:\n",
        "            action = self.act_helper.press_keys(['g'], action)\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_5yPp_TRuT4P"
      },
      "outputs": [],
      "source": [
        "# Custom ClockworkAgent\n",
        "action_sheet = [\n",
        "    (20, ['a']),\n",
        "    (15, []),\n",
        "    (10, ['s', 'j']),\n",
        "    (20, []),\n",
        "    (10, ['w', 'k']),\n",
        "    (20, []),\n",
        "    #(10, ['w', 'j']),\n",
        "    (10, ['space']),\n",
        "    (1, ['s', 'j']),\n",
        "    (20, ['a']),\n",
        "    (3, ['a', 'j']),\n",
        "    (30, []),\n",
        "    (7, ['d']),\n",
        "    (1, ['a']),\n",
        "    (4, ['a','l']),\n",
        "    (1, ['a']),\n",
        "    (4, ['a','l']),\n",
        "    (1, ['a']),\n",
        "    (4, ['a','k']),\n",
        "    (20, []),\n",
        "    (4, ['d','k']),\n",
        "    (20, []),\n",
        "    (15, ['space']),\n",
        "    (5, []),\n",
        "    (15, ['space']),\n",
        "    (5, []),\n",
        "    (15, ['space']),\n",
        "    (5, []),\n",
        "    (15, ['space']),\n",
        "    (5, []),\n",
        "    (15, ['space']),\n",
        "    (5, []),\n",
        "]\n",
        "# agent1 = ClockworkAgent(action_sheet=action_sheet)\n",
        "# agent2 = ClockworkAgent(action_sheet=action_sheet)\n",
        "# run_match(\n",
        "#     agent1,\n",
        "#     agent_2=agent2,\n",
        "#     max_timesteps=40,\n",
        "#     video_path=\"vis.mp4\",\n",
        "#     resolution=CameraResolution.LOW,\n",
        "# )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "eBHjq00zkD0b"
      },
      "outputs": [],
      "source": [
        "class ClockworkAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            action_sheet: Optional[List[Tuple[int, List[str]]]] = None,\n",
        "            *args,\n",
        "            **kwargs\n",
        "    ):\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        self.steps = 0\n",
        "        self.current_action_end = 0  # Tracks when the current action should stop\n",
        "        self.current_action_data = None  # Stores the active action\n",
        "        self.action_index = 0  # Index in the action sheet\n",
        "\n",
        "        if action_sheet is None:\n",
        "            self.action_sheet = [\n",
        "                (10, ['a']),\n",
        "                (1, ['l']),\n",
        "                (20, ['a']),\n",
        "                (3, ['a', 'j']),\n",
        "                (30, []),\n",
        "                (7, ['d']),\n",
        "                (1, ['a']),\n",
        "                (4, ['a','l']),\n",
        "                (1, ['a']),\n",
        "                (4, ['a','l']),\n",
        "                (1, ['a']),\n",
        "                (4, ['a','l']),\n",
        "                (1, ['a']),\n",
        "                (4, ['a','l']),\n",
        "                (20, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "                (15, ['space']),\n",
        "                (5, []),\n",
        "            ]\n",
        "        else:\n",
        "            self.action_sheet = action_sheet\n",
        "\n",
        "\n",
        "    def predict(self, obs):\n",
        "        \"\"\"\n",
        "        Returns an action vector based on the predefined action sheet.\n",
        "        \"\"\"\n",
        "        # Check if the current action has expired\n",
        "        if self.steps >= self.current_action_end and self.action_index < len(self.action_sheet):\n",
        "            hold_time, action_data = self.action_sheet[self.action_index]\n",
        "            self.current_action_data = action_data  # Store the action\n",
        "            self.current_action_end = self.steps + hold_time  # Set duration\n",
        "            self.action_index += 1  # Move to the next action\n",
        "\n",
        "        # Apply the currently active action\n",
        "        action = self.act_helper.press_keys(self.current_action_data)\n",
        "\n",
        "\n",
        "        self.steps += 1  # Increment step counter\n",
        "        return action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "y4JPHficefKk"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3 import A2C, PPO\n",
        "from stable_baselines3.common.base_class import BaseAlgorithm\n",
        "\n",
        "class SB3Agent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            sb3_class: Optional[Type[BaseAlgorithm]] = PPO,\n",
        "            file_path: Optional[str] = None\n",
        "    ):\n",
        "        self.sb3_class = sb3_class\n",
        "        super().__init__(file_path)\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        if self.file_path is None:\n",
        "            self.model = self.sb3_class(\"MlpPolicy\", self.env, verbose=0, n_steps=30*90*3, batch_size=128, ent_coef=0.01)\n",
        "            del self.env\n",
        "        else:\n",
        "            self.model = self.sb3_class.load(self.file_path)\n",
        "\n",
        "    def _gdown(self) -> str:\n",
        "        # Call gdown to your link\n",
        "        return\n",
        "\n",
        "    #def set_ignore_grad(self) -> None:\n",
        "        #self.model.set_ignore_act_grad(True)\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action, _ = self.model.predict(obs)\n",
        "        return action\n",
        "\n",
        "    def save(self, file_path: str) -> None:\n",
        "        self.model.save(file_path, include=['num_timesteps'])\n",
        "\n",
        "    def learn(self, env, total_timesteps, log_interval: int = 1, verbose=0):\n",
        "        self.model.set_env(env)\n",
        "        self.model.verbose = verbose\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            log_interval=log_interval,\n",
        "        )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "eiXaIv8rcyds"
      },
      "outputs": [],
      "source": [
        "from sb3_contrib import RecurrentPPO\n",
        "\n",
        "class RecurrentPPOAgent(Agent):\n",
        "\n",
        "    def __init__(\n",
        "            self,\n",
        "            file_path: Optional[str] = None\n",
        "    ):\n",
        "        super().__init__(file_path)\n",
        "        self.lstm_states = None\n",
        "        self.episode_starts = np.ones((1,), dtype=bool)\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        if self.file_path is None:\n",
        "            self.model = RecurrentPPO(\"MlpLstmPolicy\", self.env, verbose=0, n_steps=30*90*3, batch_size=128, ent_coef=0.01)\n",
        "            del self.env\n",
        "        else:\n",
        "            self.model = RecurrentPPO.load(self.file_path)\n",
        "\n",
        "    def reset(self) -> None:\n",
        "        self.episode_starts = True\n",
        "\n",
        "    def predict(self, obs):\n",
        "        action, self.lstm_states = self.model.predict(obs, state=self.lstm_states, episode_start=self.episode_starts, deterministic=True)\n",
        "        if self.episode_starts: self.episode_starts = False\n",
        "        return action\n",
        "\n",
        "    def save(self, file_path: str) -> None:\n",
        "        self.model.save(file_path)\n",
        "\n",
        "    def learn(self, env, total_timesteps, log_interval: int = 1, verbose=0):\n",
        "        self.model.set_env(env)\n",
        "        self.model.verbose = verbose\n",
        "        self.model.learn(total_timesteps=total_timesteps, log_interval=log_interval)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {},
      "outputs": [],
      "source": [
        "class AdibsAgent(Agent):\n",
        "    def __init__(\n",
        "        self,\n",
        "        file_path: Optional[str] = None,\n",
        "        learning_rate: float = 3e-4,\n",
        "        n_steps: int = 2048,\n",
        "        batch_size: int = 128,\n",
        "        n_epochs: int = 10,\n",
        "        gamma: float = 0.99,\n",
        "        gae_lambda: float = 0.95,\n",
        "        ent_coef: float = 0.01,\n",
        "        clip_range: float = 0.2,\n",
        "        verbose: int = 1,\n",
        "        max_grad_norm: float = 0.5,\n",
        "        device: str = \"auto\",\n",
        "    ):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.n_steps = n_steps\n",
        "        self.batch_size = batch_size\n",
        "        self.n_epochs = n_epochs\n",
        "        self.gamma = gamma\n",
        "        self.gae_lambda = gae_lambda\n",
        "        self.ent_coef = ent_coef\n",
        "        self.clip_range = clip_range\n",
        "        self.verbose = verbose\n",
        "        self.max_grad_norm = max_grad_norm\n",
        "        self.device = device\n",
        "        super().__init__(file_path)\n",
        "\n",
        "    def _initialize(self) -> None:\n",
        "        if self.file_path is None:\n",
        "            # Deep MLP architecture optimized for fighting games\n",
        "            policy_kwargs = dict(\n",
        "                net_arch=dict(\n",
        "                    pi=[64, 128, 256, 128, 64],  # Deep policy network\n",
        "                    vf=[64, 128, 256, 128, 64],  # Deep value network\n",
        "                ),\n",
        "                activation_fn=torch.nn.ReLU,\n",
        "                ortho_init=True,\n",
        "            )\n",
        "\n",
        "            self.model = PPO(\n",
        "                \"MlpPolicy\",\n",
        "                self.env,\n",
        "                learning_rate=self.learning_rate,\n",
        "                n_steps=self.n_steps,\n",
        "                batch_size=self.batch_size,\n",
        "                n_epochs=self.n_epochs,\n",
        "                gamma=self.gamma,\n",
        "                gae_lambda=self.gae_lambda,\n",
        "                ent_coef=self.ent_coef,\n",
        "                clip_range=self.clip_range,\n",
        "                policy_kwargs=policy_kwargs,\n",
        "                verbose=self.verbose,\n",
        "                normalize_advantage=True,\n",
        "                max_grad_norm=self.max_grad_norm,\n",
        "                device=self.device,\n",
        "            )\n",
        "            del self.env\n",
        "        else:\n",
        "            self.model = PPO.load(\n",
        "                self.file_path, n_steps=30 * 90 * 3, batch_size=128, device=self.device\n",
        "            )\n",
        "\n",
        "    def predict(self, obs):\n",
        "        \"\"\"Get action with deterministic prediction\"\"\"\n",
        "        # Convert observation to tensor on correct device if needed\n",
        "        if isinstance(obs, np.ndarray):\n",
        "            obs = torch.from_numpy(obs).to(self.model.device)\n",
        "        action, _ = self.model.predict(obs, deterministic=True)\n",
        "        return action\n",
        "\n",
        "    def save(self, file_path: str) -> None:\n",
        "        \"\"\"Save model with timesteps\"\"\"\n",
        "        self.model.save(file_path, include=[\"num_timesteps\"])\n",
        "\n",
        "    def learn(self, env, total_timesteps, log_interval: int = 1, verbose: int = 1):\n",
        "        \"\"\"Train model with checkpoint callback\"\"\"\n",
        "        self.model.set_env(env)\n",
        "        self.model.verbose = verbose\n",
        "\n",
        "        self.model.learn(\n",
        "            total_timesteps=total_timesteps,\n",
        "            log_interval=log_interval,\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b_3pL3gk9gI"
      },
      "source": [
        "## Training Function\n",
        "A helper function for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "id": "j3sYq4l_k9Bp"
      },
      "outputs": [],
      "source": [
        "from stable_baselines3.common.monitor import Monitor\n",
        "from stable_baselines3.common.results_plotter import load_results, ts2xy\n",
        "\n",
        "class TrainLogging(Enum):\n",
        "    NONE = 0\n",
        "    TO_FILE = 1\n",
        "    PLOT = 2\n",
        "\n",
        "def plot_results(log_folder, title=\"Learning Curve\"):\n",
        "    \"\"\"\n",
        "    plot the results\n",
        "\n",
        "    :param log_folder: (str) the save location of the results to plot\n",
        "    :param title: (str) the title of the task to plot\n",
        "    \"\"\"\n",
        "    x, y = ts2xy(load_results(log_folder), \"timesteps\")\n",
        "\n",
        "    weights = np.repeat(1.0, 50) / 50\n",
        "    print(weights, y)\n",
        "    y = np.convolve(y, weights, \"valid\")\n",
        "    # Truncate x\n",
        "    x = x[len(x) - len(y) :]\n",
        "\n",
        "    fig = plt.figure(title)\n",
        "    plt.plot(x, y)\n",
        "    plt.xlabel(\"Number of Timesteps\")\n",
        "    plt.ylabel(\"Rewards\")\n",
        "    plt.title(title + \" Smoothed\")\n",
        "    plt.show()\n",
        "\n",
        "def train(agent: Agent,\n",
        "          reward_manager: RewardManager,\n",
        "          save_handler: Optional[SaveHandler]=None,\n",
        "          opponent_cfg: OpponentsCfg=OpponentsCfg(),\n",
        "          resolution: CameraResolution=CameraResolution.LOW,\n",
        "          train_timesteps: int=400_000,\n",
        "          train_logging: TrainLogging=TrainLogging.PLOT\n",
        "          ):\n",
        "    # Create environment\n",
        "    env = SelfPlayWarehouseBrawl(reward_manager=reward_manager,\n",
        "                                 opponent_cfg=opponent_cfg,\n",
        "                                 save_handler=save_handler,\n",
        "                                 resolution=resolution\n",
        "                                 )\n",
        "    reward_manager.subscribe_signals(env.raw_env)\n",
        "    if train_logging != TrainLogging.NONE:\n",
        "        # Create log dir\n",
        "        log_dir = f\"{save_handler._experiment_path()}/\" if save_handler is not None else \"/tmp/gym/\"\n",
        "        os.makedirs(log_dir, exist_ok=True)\n",
        "\n",
        "        # Logs will be saved in log_dir/monitor.csv\n",
        "        env = Monitor(env, log_dir)\n",
        "\n",
        "    base_env = env.unwrapped if hasattr(env, 'unwrapped') else env\n",
        "    try:\n",
        "        agent.get_env_info(base_env)\n",
        "        base_env.on_training_start()\n",
        "        agent.learn(env, total_timesteps=train_timesteps, verbose=1)\n",
        "        base_env.on_training_end()\n",
        "    except KeyboardInterrupt:\n",
        "        pass\n",
        "\n",
        "    env.close()\n",
        "\n",
        "    if save_handler is not None:\n",
        "        save_handler.save_agent()\n",
        "\n",
        "    if train_logging == TrainLogging.PLOT:\n",
        "        plot_results(log_dir)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7wH-mhyfDpZ"
      },
      "source": [
        "## Example Reward Functions\n",
        "Find more [here](https://colab.research.google.com/drive/1qMs336DclBwdn6JBASa5ioDIfvenW8Ha?usp=sharing#scrollTo=-XAOXXMPTiHJ)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Reward params\n",
        "# Modes:\n",
        "#     - ASYMMETRIC_OFFENSIVE (0): Reward is based only on damage dealt to the opponent\n",
        "#     - SYMMETRIC (1): Reward is based on both dealing damage to the opponent and avoiding damage\n",
        "#     - ASYMMETRIC_DEFENSIVE (2): Reward is based only on avoiding damage\n",
        "\n",
        "class RewardMode(Enum):\n",
        "    ASYMMETRIC_OFFENSIVE = 0\n",
        "    SYMMETRIC = 1\n",
        "    ASYMMETRIC_DEFENSIVE = 2\n",
        "\n",
        "class RewardMode(Enum):\n",
        "    ASYMMETRIC_OPPONENT = 0\n",
        "    SYMMETRIC = 1\n",
        "    ASYMMETRIC_PLAYER = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### *Position Functions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {},
      "outputs": [],
      "source": [
        "def border_penalty(env: WarehouseBrawl,\n",
        "    x_danger_threshold: float = 11.0,\n",
        "    y_danger_threshold: float = 5.5,       \n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Calculates a penalty to discourage the agent from getting too close to the environment borders.\n",
        "    \"\"\"\n",
        "\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    pos_x = abs(player.body.position.x)\n",
        "    pos_y = abs(player.body.position.y)\n",
        "    penalty = 0.0\n",
        "\n",
        "    # Penalty for getting close to left or right borders\n",
        "    if pos_x > x_danger_threshold:\n",
        "        x_distance_to_border = 14.0 - pos_x\n",
        "        \n",
        "        # Increase penalty as the agent gets closer to the border\n",
        "        penalty += -2.0 + 1.0 * (x_distance_to_border / (14.0 - x_danger_threshold))\n",
        "    \n",
        "    # Penalty for getting close to top or bottom borders\n",
        "    if pos_y > y_danger_threshold:\n",
        "        y_distance_to_border = 8.0 - pos_y\n",
        "        \n",
        "        # Increase penalty as the agent gets closer to the border\n",
        "        penalty += -1.5 + 0.7 * (y_distance_to_border / (8.0 - y_danger_threshold))\n",
        "\n",
        "    return penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {},
      "outputs": [],
      "source": [
        "def danger_zone_penalty(\n",
        "    env: WarehouseBrawl,\n",
        "    zone_penalty: int = 1,\n",
        "    zone_height: float = 4.2\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Applies a penalty for every time frame player surpases a certain height threshold in the environment.\n",
        "    \"\"\"\n",
        "    # Get player object from the environment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "\n",
        "    # Apply penalty if the player is in the danger zone\n",
        "    reward = -zone_penalty if player.body.position.y >= zone_height else 0.0\n",
        "\n",
        "    return reward * env.dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {},
      "outputs": [],
      "source": [
        "def head_to_middle_reward(env: WarehouseBrawl) -> float:\n",
        "    \"\"\"\n",
        "    Encourages the player to stay near the center of the platform (0,0).\n",
        "\n",
        "    - The agent is rewarded for moving toward the center.\n",
        "    - The agent is punished if it moves closer to the platform edges (x < -5.335 or x > 5.335).\n",
        "    - No reward or penalty when inside the platform's safe zone.\n",
        "    \"\"\"\n",
        "\n",
        "    # Get player object from the environment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    current_x = abs(player.body.position.x)\n",
        "    prev_x = abs(player.prev_x)\n",
        "\n",
        "    # Define platform boundaries\n",
        "    platform_edge = 5.335\n",
        "\n",
        "    # Movement direction check (positive if moving toward center)\n",
        "    moving_toward_center = current_x < prev_x\n",
        "\n",
        "    # Base reward: Encourage movement toward the center\n",
        "    move_reward = (prev_x- current_x) * 0.5 if moving_toward_center else -0.1\n",
        "\n",
        "    # Edge penalty: Stronger penalty if closer to edges than the center\n",
        "    edge_penalty = 0  # No penalty if inside central zone\n",
        "    if current_x > 2.6:  # Outside the central zone\n",
        "        edge_penalty = -0.3 * (current_x / platform_edge)\n",
        "\n",
        "    return move_reward + edge_penalty\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def move_to_opponent(env: WarehouseBrawl) -> float:\n",
        "    \"\"\" \n",
        "    Computes the reward based on whether the agent is moving toward the opponent.\n",
        "    The reward is calculated by taking the dot product of the agent's normalized velocity\n",
        "    with the normalized direction vector toward the opponent.\n",
        "    \"\"\"\n",
        "\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Extracting player velocity and position from environment\n",
        "    x_change = player.body.position.x - player.prev_x\n",
        "    y_change = player.body.position.y - player.prev_y\n",
        "    player_position_dif = np.array([x_change, y_change])\n",
        "\n",
        "    direction_to_opponent = np.array([opponent.body.position.x - player.body.position.x,\n",
        "                                      opponent.body.position.y - player.body.position.y])\n",
        "\n",
        "    # Prevent division by zero or extremely small values\n",
        "    direc_to_opp_norm = np.linalg.norm(direction_to_opponent)\n",
        "    player_pos_dif_norm = np.linalg.norm(player_position_dif)\n",
        "\n",
        "    if direc_to_opp_norm < 1e-6 or player_pos_dif_norm < 1e-6:\n",
        "        return 0.0\n",
        "    \n",
        "    # Normalize vectors and compute dot product\n",
        "    normalized_movement = player_position_dif / player_pos_dif_norm\n",
        "    normalized_direction = direction_to_opponent / direc_to_opp_norm\n",
        "    reward = np.dot(normalized_movement, normalized_direction)\n",
        "    \n",
        "    return reward\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {},
      "outputs": [],
      "source": [
        "def movement_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    direction_weight: float = 0.6,   \n",
        "    speed_weight: float = 0.35,\n",
        "    proximity_weight: float = 0.15,    \n",
        "    min_direct_threshold: float = 0.2,\n",
        "    max_distance: float = 14.0       \n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Computes movement reward combining directional accuracy, speed, and proximity.\n",
        "    \n",
        "    Args:\n",
        "    - direction_weight: Weight for moving in the correct direction (0.6 default)\n",
        "    - speed_weight: Weight for movement speed bonus (0.3 default)\n",
        "    - proximity_weight: Weight for proximity bonus (0.1 default)\n",
        "    - min_speed_threshold: Minimum speed to consider movement valid\n",
        "    - max_distance: Distance at which proximity bonus becomes zero (half arena width)\n",
        "    \n",
        "    Returns:\n",
        "    Float between -1 and 1, where:\n",
        "    - 1.0: Perfect movement (directly toward opponent at good speed when close)\n",
        "    - 0.0: No movement or perpendicular movement\n",
        "    - -1.0: Moving directly away from opponent\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Prevents rewards for moving towards dying agent\n",
        "    if opponent.body.position.y > 5.5:\n",
        "        return 0.0\n",
        "\n",
        "    # Calculate 2D movement vector\n",
        "    movement = np.array([\n",
        "        player.body.position.x - player.prev_x,\n",
        "        player.body.position.y - player.prev_y\n",
        "    ])\n",
        "    \n",
        "    # Calculate direction to opponent\n",
        "    direction_to_opponent = np.array([\n",
        "        opponent.body.position.x - player.body.position.x,\n",
        "        opponent.body.position.y - player.body.position.y\n",
        "    ])\n",
        "    \n",
        "    # Calculate norms (magnitudes)\n",
        "    movement_norm = np.linalg.norm(movement)\n",
        "    direction_norm = np.linalg.norm(direction_to_opponent)\n",
        "    \n",
        "    # Initialize reward components\n",
        "    direction_reward = 0.0\n",
        "    speed_reward = 0.0\n",
        "    proximity_reward = 0.0\n",
        "    \n",
        "    # 1. Direction Reward\n",
        "    if movement_norm > min_direct_threshold and direction_norm > min_direct_threshold:\n",
        "        normalized_movement = movement / movement_norm\n",
        "        normalized_direction = direction_to_opponent / direction_norm\n",
        "        direction_reward = np.dot(normalized_movement, normalized_direction)\n",
        "    \n",
        "    # 2. Speed Reward\n",
        "    max_speed = player.dash_speed\n",
        "    speed_reward = min(movement_norm / max_speed, 1.0)\n",
        "    \n",
        "    # 3. Proximity Reward\n",
        "    # Scale based on arena size - max_distance is half arena width\n",
        "    if movement_norm > min_direct_threshold and direction_norm > min_direct_threshold:\n",
        "        distance = np.linalg.norm(direction_to_opponent)\n",
        "        proximity_reward = 1.0 - min(distance / max_distance, 1.0)\n",
        "    \n",
        "    # Combine rewards with weights\n",
        "    total_reward = (\n",
        "        direction_weight * direction_reward +\n",
        "        speed_weight * speed_reward +\n",
        "        proximity_weight * proximity_reward\n",
        "    )\n",
        "    \n",
        "    return total_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tactical_positioning(env: WarehouseBrawl) -> float:\n",
        "    \"\"\"\n",
        "    Computes a normalized reward for tactical positioning in combat.\n",
        "\n",
        "    The reward system encourages:\n",
        "    - Moving toward the opponent when the player has an advantage (more stocks, less damage).\n",
        "    - Moving away when at a disadvantage.\n",
        "    - Rewarding movement that aligns with input direction.\n",
        "    \n",
        "    Reward Components:\n",
        "    - `base_reward`: Encourages approaching the opponent if winning, retreating if losing.\n",
        "    - `movement_reward`: Rewards movement, with a higher multiplier if the correct key is held.\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Normalize advantages to [-1, 1] range\n",
        "    stock_advantage = (player.stocks - opponent.stocks) / 3.0\n",
        "    damage_advantage = (opponent.damage - player.damage) / 100.0\n",
        "\n",
        "    # Weighted sum with normalized values\n",
        "        # A positive total_advantage means the player is ahead (more stocks or less damage)\n",
        "        # A negative total_advantage means the opponent is ahead (fewer stocks or more damage\n",
        "    total_advantage = stock_advantage * 0.7 + damage_advantage * 0.3\n",
        "\n",
        "    # Movement reward based on direction and input\n",
        "    movement = player.body.position.x - player.prev_x\n",
        "    correct_input = False\n",
        "\n",
        "    # Safely check key status with null checks\n",
        "    right_key = player.input.key_status.get(\"right\")\n",
        "    left_key = player.input.key_status.get(\"left\")\n",
        "\n",
        "    if movement > 0 and right_key is not None and right_key.held:\n",
        "        correct_input = True\n",
        "    elif movement < 0 and left_key is not None and left_key.held:\n",
        "        correct_input = True\n",
        "\n",
        "    # Base reward from original logic\n",
        "    base_reward = (\n",
        "        0.05\n",
        "        if (total_advantage > 0)\n",
        "        == (movement * (opponent.body.position.x - player.body.position.x) > 0)\n",
        "        else -0.05\n",
        "    )\n",
        "\n",
        "    # Add movement reward\n",
        "    movement_reward = abs(movement) * 0.5 * (3 if correct_input else 1)\n",
        "\n",
        "    return base_reward + movement_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {},
      "outputs": [],
      "source": [
        "def edge_guard_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    success_value: float = 0.2,  # Base reward for correct positioning\n",
        "    fail_value: float = -0.1,    # Penalty for when opponent is off-stage but player isn't in position\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward given for every time step your agent is edge guarding the opponent.\n",
        "    \n",
        "    Edge guarding occurs when:\n",
        "    1. The opponent is off the main platform\n",
        "    2. The player is between the opponent and the nearest platform edge\n",
        "    3. The opponent is trying to return to the platform\n",
        "\n",
        "    Rewards are structured to encourage:\n",
        "    - Positioning between opponent and edge\n",
        "    - Maintaining height advantage\n",
        "    - Dealing damage while edge guarding\n",
        "    - Avoiding taking damage while edge guarding\n",
        "    \"\"\"\n",
        "\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "    \n",
        "    # Get positions and states\n",
        "    player_pos = player.body.position\n",
        "    opponent_pos = opponent.body.position\n",
        "    \n",
        "    # Check if opponent is off platform    \n",
        "    if abs(opponent_pos.x) > 5.335:\n",
        "\n",
        "        # Check if player is between edge and opponent\n",
        "        if not (opponent_pos.x * player_pos.x > 0 and abs(opponent_pos.x) > abs(player_pos.x) > 4.3):\n",
        "            return fail_value\n",
        "\n",
        "        # Reward based on relative height\n",
        "        height_bonus = 0.2 if player_pos.y > opponent_pos.y + 1 else -0.1\n",
        "\n",
        "        # Reward for dealing damage, penalty for taking damage\n",
        "        damage_bonus = 0.5 if opponent.damage_taken_this_frame else -0.3 if player.damage_taken_this_frame else 0.0\n",
        "\n",
        "        return success_value + height_bonus + damage_bonus\n",
        "            \n",
        "    return 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {},
      "outputs": [],
      "source": [
        "def jump_reward(env: WarehouseBrawl, base_reward: float = 0.2) -> float:\n",
        "    \"\"\"\n",
        "    Reward for jumping / being in the air\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "    \n",
        "    if player.state == player.states['in_air'] and opponent.state == opponent.states['attack']:\n",
        "        return base_reward\n",
        "        \n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {},
      "outputs": [],
      "source": [
        "def escape_reward(\n",
        "    env: WarehouseBrawl, \n",
        "    sideways_value: float = 0.3,\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Rewards the agent from moving away from an aerial attack\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    is_opponent_above = (opponent.body.position.y < player.body.position.y - 0.7)\n",
        "    is_opponent_attacking = (opponent.state == opponent.states['attack'])\n",
        "\n",
        "    # Reward for dodging sideways or jumping when the opponent is above and attacking\n",
        "    if is_opponent_above and is_opponent_attacking:\n",
        "        # Check for sideways movement\n",
        "        if player.body.position.x != player.prev_x:\n",
        "            return sideways_value\n",
        "        \n",
        "    return 0.0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### *Damage Functions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "first_hit_bool = False\n",
        "def first_hit(env: WarehouseBrawl, agent: str = \"player\", success_value: float = 7.0, fail_value: float = -3.0,) -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward based on who lands the first hit\n",
        "    \"\"\"\n",
        "    first_hit_bool = True\n",
        "    if (first_hit_bool):\n",
        "        return 0.0\n",
        "    reward = success_value if agent == \"player\" else fail_value\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dodge_reward(\n",
        "    env: WarehouseBrawl, \n",
        "    dodge_value: float = 2,         # Reward for a successful dodge\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Rewards the agent for dodging attacks strategically.\n",
        "    Only gives a reward if the opponent was attacking.\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    if opponent.state == player.states['attack'] and player.state == player.states['dodge'] and player.dodge_time <= 0:\n",
        "        if player.damage_taken_this_frame == 0:\n",
        "            return dodge_value\n",
        "\n",
        "    return 0.0  # No reward if dodge was unsuccessful\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {},
      "outputs": [],
      "source": [
        "def dodge_buggy(\n",
        "    env: WarehouseBrawl, \n",
        "    dodge_value: float = 2,         # Reward for a successful dodge\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Rewards the agent for dodging attacks strategically.\n",
        "    Only gives a reward if the opponent was attacking.\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    if opponent.state == opponent.states['attack'] and player.input.key_status[\"l\"].just_pressed and player.dodge_time <= 0:\n",
        "        if player.damage_taken_this_frame == 0:\n",
        "            return dodge_value\n",
        "\n",
        "    return 0.0  # No reward if dodge was unsuccessful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def damage_interaction_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    mode: RewardMode = RewardMode.SYMMETRIC,\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward based on damage interactions between players.\n",
        "\n",
        "    Modes:\n",
        "    - ASYMMETRIC_OFFENSIVE (0): Reward is based only on damage dealt to the opponent\n",
        "    - SYMMETRIC (1): Reward is based on both dealing damage to the opponent and avoiding damage\n",
        "    - ASYMMETRIC_DEFENSIVE (2): Reward is based only on avoiding damage\n",
        "    \"\"\"\n",
        "    # Getting player and opponent from the enviornment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Reward dependent on the mode\n",
        "    damage_taken = player.damage_taken_this_frame\n",
        "    damage_dealt = opponent.damage_taken_this_frame\n",
        "\n",
        "    if mode == RewardMode.ASYMMETRIC_OFFENSIVE:\n",
        "        reward = damage_dealt\n",
        "    elif mode == RewardMode.SYMMETRIC:\n",
        "        reward = damage_dealt - damage_taken\n",
        "    elif mode == RewardMode.ASYMMETRIC_DEFENSIVE:\n",
        "        reward = -damage_taken\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "    return reward / 140  # 140 is around where they start to die, so if they do damage enough to start killing they’ll get total 1 reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [],
      "source": [
        "def damage_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    hit_reward: float = 1.0,\n",
        "    damage_scale: float = 15.0,         # Scale based on average damage per hit\n",
        "    edge_bonus: float = 0.5,            # Bonus for hitting near the edge\n",
        "    edge_threshold: float = 4.2,        # Distance to edge to qualify for bonus\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Rewards the agent for dealing damage to the opponent.\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "    reward = 0.0\n",
        "\n",
        "    # Reward for landing a hit\n",
        "    if not opponent.damage_taken_this_frame:\n",
        "        return reward\n",
        "    \n",
        "    normalized_damage = opponent.damage_taken_this_frame / damage_scale\n",
        "    reward += hit_reward * normalized_damage\n",
        "\n",
        "    # TODO: remove this part later in the training process\n",
        "    # Edge Bonus\n",
        "    if 6.2 > abs(opponent.body.position.x) > edge_threshold:  # Arena borders at ±14\n",
        "        reward += edge_bonus\n",
        "\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {},
      "outputs": [],
      "source": [
        "def combo_reward(\n",
        "    env: WarehouseBrawl, \n",
        "    max_combo_multiplier: float = 7.0,  # Cap the combo reward\n",
        "    combo_base: float = 1.3,            # Base for exponential combo scaling\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Rewards stringing together consecutive hits with increasing rewards.\n",
        "    Resets if too much time passes between hits.\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    # Initialize combo tracking\n",
        "    if not hasattr(player, \"combo_counter\"):\n",
        "        player.combo_counter = 0\n",
        "        player.last_hit_time = 0\n",
        "\n",
        "    current_time = env.steps\n",
        "    damage_dealt = opponent.damage_taken_this_frame\n",
        "\n",
        "    if damage_dealt:\n",
        "        player.last_hit_time = current_time\n",
        "        player.combo_counter += 1\n",
        "\n",
        "    # Reset combo if too much time passed\n",
        "    if current_time - player.last_hit_time > 40:  # 1.3 seconds at 30 FPS\n",
        "        player.combo_counter = 0\n",
        "        return 0.0\n",
        "\n",
        "    return min(combo_base ** (player.combo_counter - 1), max_combo_multiplier)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {},
      "outputs": [],
      "source": [
        "def damage_penalty(\n",
        "    env: WarehouseBrawl, \n",
        "    multiplier: int = 1.0, \n",
        "    base_penalty: float = 1.1,\n",
        "    damage_scale: float = 25.0,         # Scale based on average damage per hit\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Calculates penalty for taking damage with context-aware scaling\n",
        "    \"\"\"\n",
        "    # Getting player and opponent from the enviornment\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "    \n",
        "    # Get damage taken this frame\n",
        "    damage_taken = player.damage_taken_this_frame / damage_scale  # Normalize\n",
        "    if damage_taken == 0:\n",
        "        return 0.0\n",
        "        \n",
        "    # Base penalty for any damage\n",
        "    penalty = -damage_taken * base_penalty\n",
        "    \n",
        "    # Punish more if the agent is at high damage (risk of KO)\n",
        "    if player.damage > 30:\n",
        "        multiplier *= 1.5\n",
        "    \n",
        "    # Stock disadvantage increases penalty\n",
        "    if player.stocks < opponent.stocks:\n",
        "        multiplier *= 1.5\n",
        "        \n",
        "    return penalty * multiplier"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def health_disadvantage_time(env: WarehouseBrawl) -> float:\n",
        "    \"\"\"\n",
        "    Penalize time spent with equal or worse health than opponent\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "\n",
        "    if player.damage >= opponent.damage:\n",
        "        return -0.1 * env.dt\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {},
      "outputs": [],
      "source": [
        "def health_comparison(\n",
        "    env: WarehouseBrawl,\n",
        "    reward_scale: float = 0.5,    # Scale for positive reward\n",
        "    penalty_scale: float = 0.5,   # Scale for penalty\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Rewards the agent for having less total damage taken than the opponent.\n",
        "    Penalizes for having more total damage taken than the opponent.\n",
        "    \n",
        "    Scales the reward dynamically based on current stock count and normalizes \n",
        "    using logarithmic adjustment for consistent training.\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    opponent: Player = env.objects[\"opponent\"]\n",
        "    reward = 0.0\n",
        "\n",
        "    # Calculate total damage taken by both players\n",
        "    player_damage_taken = player.damage_taken_total\n",
        "    opponent_damage_taken = opponent.damage_taken_total\n",
        "\n",
        "    # Get the maximum damage possible based on the current stock count\n",
        "    max_damage = max(player.stocks, opponent.stocks) * 700\n",
        "\n",
        "    # Logarithmic normalization to dampen high values\n",
        "    normalized_player_damage = np.log(player_damage_taken + 1) / np.log(max_damage + 1)\n",
        "    normalized_opponent_damage = np.log(opponent_damage_taken + 1) / np.log(max_damage + 1)\n",
        "    damage_difference = normalized_opponent_damage - normalized_player_damage\n",
        "\n",
        "    # Reward for taking less damage than the opponent\n",
        "    if damage_difference > 0:\n",
        "        reward += reward_scale * damage_difference\n",
        "    \n",
        "    # Penalty for taking more damage than the opponent\n",
        "    elif damage_difference < 0:\n",
        "        reward += penalty_scale * damage_difference  # Negative value -> penalty\n",
        "\n",
        "    return reward\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### *Score Functions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knockout_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    agent: str = \"player\",\n",
        "    mode: RewardMode = RewardMode.SYMMETRIC,\n",
        "    knockout_value_opponent: float = 60.0,\n",
        "    knockout_value_player: float = 80.0,\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Computes the reward based on who won the match.\n",
        "\n",
        "    Modes:\n",
        "    - ASYMMETRIC_OPPONENT (0): Reward is based only on the opponent being knocked out\n",
        "    - SYMMETRIC (1): Reward is based on both agents being knocked out\n",
        "    - ASYMMETRIC_PLAYER (2): Reward is based only on your own plauyer being knocked out\n",
        "    \"\"\"\n",
        "    reward = 0.0\n",
        "\n",
        "    # Mode logic to compute reward\n",
        "    if mode == RewardMode.ASYMMETRIC_OPPONENT:\n",
        "        if agent == \"opponent\":\n",
        "            reward = knockout_value_opponent # Reward for opponent being knocked out\n",
        "    elif mode == RewardMode.SYMMETRIC:\n",
        "        if agent == \"player\":\n",
        "            reward = -knockout_value_player  # Penalty for player getting knocoked out\n",
        "        elif agent == \"opponent\":\n",
        "            reward = knockout_value_opponent # Reward for opponent being knocked out\n",
        "    elif mode == RewardMode.ASYMMETRIC_PLAYER:\n",
        "        if agent == \"player\":\n",
        "            reward = -knockout_value_player  # Penalty for player getting knocked out\n",
        "\n",
        "    return reward\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def on_win_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    agent: str = \"player\",\n",
        "    win_value: float = 90.0,\n",
        "    lose_value: float = 110.0,\n",
        ") -> float:\n",
        "\n",
        "    \"\"\"\n",
        "    Computes the reward based on knockouts.\n",
        "    \"\"\"\n",
        "    reward = win_value if agent == \"player\" else -lose_value\n",
        "    return reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### *Other Functions*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {},
      "outputs": [],
      "source": [
        "def cute_reward(\n",
        "    env: WarehouseBrawl,\n",
        "    ) -> float:\n",
        "    \"\"\"\n",
        "    Rewards the player for taunting\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "\n",
        "    return 0.1 if player.states['taunt'] else 0.0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def keyboard_efficiency(env: WarehouseBrawl) -> float:\n",
        "    \"\"\"\n",
        "    Penalize excessive button pressing\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "\n",
        "    # Count number of pressed buttons this frame\n",
        "    pressed = sum(1 for key in player.input.key_status.values() if key.held)\n",
        "\n",
        "    # Penalize pressing more than 2 buttons at once\n",
        "    if pressed > 3:\n",
        "        return -0.1 * pressed\n",
        "    return 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {},
      "outputs": [],
      "source": [
        "def stun_penalty(env: WarehouseBrawl, base_penalty: float = -0.5) -> float:\n",
        "    \"\"\"\n",
        "    Penalty for being stunned\n",
        "    \"\"\"\n",
        "    player: Player = env.objects[\"player\"]\n",
        "    \n",
        "    if player.state != 'StunState':\n",
        "        return 0.0\n",
        "    \n",
        "    if player.damage > 140:\n",
        "        return base_penalty * 1.5\n",
        "        \n",
        "    return base_penalty"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l9EqQkRvfLRZ"
      },
      "source": [
        "## Run Training\n",
        "\n",
        "Run this cell to run training. Be sure to set your agent under the `my_agent` variable, and modify the training using the `reward_manager`, `selfplay_handler`, `save_handler`, and `opponent_cfg`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# First Training Cycle\n",
        "my_agent = SubmittedAgent(device=\"cuda\")\n",
        "\n",
        "SAVE_CONFIG = {\n",
        "    \"save_freq\": 25_000,\n",
        "    \"max_saved\": 3,\n",
        "    \"save_path\": \"checkpoints\",\n",
        "}\n",
        "\n",
        "reward_functions = {\n",
        "    \"keyboard_efficiency\": RewTerm(func=keyboard_efficiency, weight=0.25),\n",
        "    'move_to_opponent_reward': RewTerm(func=move_to_opponent, weight=2),\n",
        "    'damage_interaction_reward': RewTerm(func=damage_interaction_reward, weight=1.5),\n",
        "    'head_to_middle_reward': RewTerm(func=head_to_middle_reward, weight=1),\n",
        "    'border_penalty': RewTerm(func=border_penalty, weight=1),\n",
        "    'attack_reward': RewTerm(func=attack_reward, weight=0.5),\n",
        "}\n",
        "signal_subscriptions = {\n",
        "    \"on_win\": (\"win_signal\", RewTerm(func=on_win_reward, weight=1)),\n",
        "    \"on_knockout\": (\"knockout_signal\",RewTerm(func=knockout_reward, weight=1)),\n",
        "}\n",
        "reward_manager = RewardManager(reward_functions, signal_subscriptions)\n",
        "\n",
        "opponents = {\n",
        "    \"constant_agent\": (0.5, partial(ConstantAgent)),\n",
        "    \"random_agent\": (0.3, partial(RandomAgent)),\n",
        "    \"based_agent\": (0.2, partial(BasedAgent)),\n",
        "}\n",
        "opponent_cfg = OpponentsCfg(opponents=opponents)\n",
        "\n",
        "# Save settings\n",
        "save_handler = SaveHandler(\n",
        "    agent=my_agent, # Agent to save\n",
        "    run_name='experiment_1',\n",
        "    mode=SaveHandlerMode.FORCE # Save mode, FORCE or RESUME\n",
        ")\n",
        "\n",
        "train(\n",
        "    my_agent,\n",
        "    reward_manager,\n",
        "    save_handler,\n",
        "    opponent_cfg,\n",
        "    CameraResolution.LOW,\n",
        "    train_timesteps=200_000,\n",
        "    train_logging=TrainLogging.PLOT,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training on device: cpu\n",
            "Best model is checkpoints\\experiment_1\\rl_model_0_steps.zip\n",
            "Warning: Probabilities do not sum to 1 (current sum = 1.2000000000000002). Normalizing...\n",
            "Obs space [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11]\n",
            "Action space [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Selected jumping_agent\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 821      |\n",
            "|    ep_rew_mean     | 605      |\n",
            "| time/              |          |\n",
            "|    fps             | 782      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 10       |\n",
            "|    total_timesteps | 8100     |\n",
            "---------------------------------\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected sb3_agent1\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 886         |\n",
            "|    ep_rew_mean          | 570         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 352         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 45          |\n",
            "|    total_timesteps      | 16200       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017062686 |\n",
            "|    clip_fraction        | 0.176       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.4       |\n",
            "|    explained_variance   | 0.907       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 34.6        |\n",
            "|    n_updates            | 14200       |\n",
            "|    policy_gradient_loss | -0.0227     |\n",
            "|    std                  | 96.9        |\n",
            "|    value_loss           | 52.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected based_agent\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_162455_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 957         |\n",
            "|    ep_rew_mean          | 637         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 237         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 102         |\n",
            "|    total_timesteps      | 24300       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015838262 |\n",
            "|    clip_fraction        | 0.17        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.5       |\n",
            "|    explained_variance   | 0.905       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 14.1        |\n",
            "|    n_updates            | 14210       |\n",
            "|    policy_gradient_loss | -0.0206     |\n",
            "|    std                  | 96.8        |\n",
            "|    value_loss           | 45.7        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 824        |\n",
            "|    ep_rew_mean          | 586        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 244        |\n",
            "|    iterations           | 4          |\n",
            "|    time_elapsed         | 132        |\n",
            "|    total_timesteps      | 32400      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01621969 |\n",
            "|    clip_fraction        | 0.175      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -59.5      |\n",
            "|    explained_variance   | 0.849      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 10.4       |\n",
            "|    n_updates            | 14220      |\n",
            "|    policy_gradient_loss | -0.0192    |\n",
            "|    std                  | 97.3       |\n",
            "|    value_loss           | 47.7       |\n",
            "----------------------------------------\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected random_agent\n",
            "Selected jumping_agent\n",
            "Selected sb3_agent1\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_182456_steps.\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 852         |\n",
            "|    ep_rew_mean          | 595         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 210         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 192         |\n",
            "|    total_timesteps      | 40500       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016103001 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.5       |\n",
            "|    explained_variance   | 0.903       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.5        |\n",
            "|    n_updates            | 14230       |\n",
            "|    policy_gradient_loss | -0.0207     |\n",
            "|    std                  | 97.2        |\n",
            "|    value_loss           | 67.7        |\n",
            "-----------------------------------------\n",
            "Selected jumping_agent\n",
            "Selected sb3_agent1\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected sb3_agent1\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 860         |\n",
            "|    ep_rew_mean          | 665         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 190         |\n",
            "|    iterations           | 6           |\n",
            "|    time_elapsed         | 254         |\n",
            "|    total_timesteps      | 48600       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014358933 |\n",
            "|    clip_fraction        | 0.138       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.5       |\n",
            "|    explained_variance   | 0.898       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 29.4        |\n",
            "|    n_updates            | 14240       |\n",
            "|    policy_gradient_loss | -0.0209     |\n",
            "|    std                  | 97.5        |\n",
            "|    value_loss           | 92.7        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected sb3_agent1\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 856         |\n",
            "|    ep_rew_mean          | 661         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 183         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 309         |\n",
            "|    total_timesteps      | 56700       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015876718 |\n",
            "|    clip_fraction        | 0.153       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.5       |\n",
            "|    explained_variance   | 0.917       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.6        |\n",
            "|    n_updates            | 14250       |\n",
            "|    policy_gradient_loss | -0.0208     |\n",
            "|    std                  | 97.8        |\n",
            "|    value_loss           | 57.2        |\n",
            "-----------------------------------------\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_202457_steps.\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected adibs_good_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 808        |\n",
            "|    ep_rew_mean          | 623        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 182        |\n",
            "|    iterations           | 8          |\n",
            "|    time_elapsed         | 354        |\n",
            "|    total_timesteps      | 64800      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01890498 |\n",
            "|    clip_fraction        | 0.194      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -59.6      |\n",
            "|    explained_variance   | 0.871      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 13         |\n",
            "|    n_updates            | 14260      |\n",
            "|    policy_gradient_loss | -0.0263    |\n",
            "|    std                  | 98         |\n",
            "|    value_loss           | 41.7       |\n",
            "----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected adibs_good_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected random_agent\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 779         |\n",
            "|    ep_rew_mean          | 610         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 184         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 396         |\n",
            "|    total_timesteps      | 72900       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019852407 |\n",
            "|    clip_fraction        | 0.208       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.6       |\n",
            "|    explained_variance   | 0.91        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.02        |\n",
            "|    n_updates            | 14270       |\n",
            "|    policy_gradient_loss | -0.0191     |\n",
            "|    std                  | 98.7        |\n",
            "|    value_loss           | 29.5        |\n",
            "-----------------------------------------\n",
            "Selected jumping_agent\n",
            "Selected based_agent\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected adibs_good_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_222458_steps.\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 770         |\n",
            "|    ep_rew_mean          | 618         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 196         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 411         |\n",
            "|    total_timesteps      | 81000       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021113249 |\n",
            "|    clip_fraction        | 0.199       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.7       |\n",
            "|    explained_variance   | 0.929       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 11.6        |\n",
            "|    n_updates            | 14280       |\n",
            "|    policy_gradient_loss | -0.0242     |\n",
            "|    std                  | 99.1        |\n",
            "|    value_loss           | 31.2        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected sb3_agent1\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 741         |\n",
            "|    ep_rew_mean          | 621         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 207         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 428         |\n",
            "|    total_timesteps      | 89100       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017285798 |\n",
            "|    clip_fraction        | 0.174       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.8       |\n",
            "|    explained_variance   | 0.903       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.39        |\n",
            "|    n_updates            | 14290       |\n",
            "|    policy_gradient_loss | -0.0185     |\n",
            "|    std                  | 99.8        |\n",
            "|    value_loss           | 34.3        |\n",
            "-----------------------------------------\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 732       |\n",
            "|    ep_rew_mean          | 627       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 218       |\n",
            "|    iterations           | 12        |\n",
            "|    time_elapsed         | 444       |\n",
            "|    total_timesteps      | 97200     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0148502 |\n",
            "|    clip_fraction        | 0.132     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -59.8     |\n",
            "|    explained_variance   | 0.888     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 15.1      |\n",
            "|    n_updates            | 14300     |\n",
            "|    policy_gradient_loss | -0.019    |\n",
            "|    std                  | 100       |\n",
            "|    value_loss           | 57.5      |\n",
            "---------------------------------------\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected random_agent\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_242459_steps.\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected adibs_good_agent\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 722         |\n",
            "|    ep_rew_mean          | 602         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 227         |\n",
            "|    iterations           | 13          |\n",
            "|    time_elapsed         | 462         |\n",
            "|    total_timesteps      | 105300      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014756488 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.9       |\n",
            "|    explained_variance   | 0.913       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.3         |\n",
            "|    n_updates            | 14310       |\n",
            "|    policy_gradient_loss | -0.0188     |\n",
            "|    std                  | 101         |\n",
            "|    value_loss           | 40.5        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected adibs_good_agent\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 690         |\n",
            "|    ep_rew_mean          | 563         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 237         |\n",
            "|    iterations           | 14          |\n",
            "|    time_elapsed         | 476         |\n",
            "|    total_timesteps      | 113400      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021764068 |\n",
            "|    clip_fraction        | 0.23        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.9       |\n",
            "|    explained_variance   | 0.911       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.4         |\n",
            "|    n_updates            | 14320       |\n",
            "|    policy_gradient_loss | -0.0267     |\n",
            "|    std                  | 101         |\n",
            "|    value_loss           | 23.6        |\n",
            "-----------------------------------------\n",
            "Selected jumping_agent\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_262460_steps.\n",
            "Selected random_agent\n",
            "Selected adibs_good_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 676         |\n",
            "|    ep_rew_mean          | 547         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 246         |\n",
            "|    iterations           | 15          |\n",
            "|    time_elapsed         | 492         |\n",
            "|    total_timesteps      | 121500      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016419042 |\n",
            "|    clip_fraction        | 0.161       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -59.9       |\n",
            "|    explained_variance   | 0.877       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 10.1        |\n",
            "|    n_updates            | 14330       |\n",
            "|    policy_gradient_loss | -0.0158     |\n",
            "|    std                  | 101         |\n",
            "|    value_loss           | 42.6        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected based_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 675        |\n",
            "|    ep_rew_mean          | 560        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 254        |\n",
            "|    iterations           | 16         |\n",
            "|    time_elapsed         | 508        |\n",
            "|    total_timesteps      | 129600     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01609886 |\n",
            "|    clip_fraction        | 0.162      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -60        |\n",
            "|    explained_variance   | 0.919      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 27.5       |\n",
            "|    n_updates            | 14340      |\n",
            "|    policy_gradient_loss | -0.0213    |\n",
            "|    std                  | 102        |\n",
            "|    value_loss           | 43.7       |\n",
            "----------------------------------------\n",
            "Selected self_play\n",
            "Selected adibs_good_agent\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected adibs_good_agent\n",
            "Selected self_play\n",
            "Selected adibs_good_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 720         |\n",
            "|    ep_rew_mean          | 622         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 262         |\n",
            "|    iterations           | 17          |\n",
            "|    time_elapsed         | 524         |\n",
            "|    total_timesteps      | 137700      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011582201 |\n",
            "|    clip_fraction        | 0.111       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60         |\n",
            "|    explained_variance   | 0.919       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 59.8        |\n",
            "|    n_updates            | 14350       |\n",
            "|    policy_gradient_loss | -0.017      |\n",
            "|    std                  | 102         |\n",
            "|    value_loss           | 85.6        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected adibs_good_agent\n",
            "Selected self_play\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_282461_steps.\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 747         |\n",
            "|    ep_rew_mean          | 654         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 268         |\n",
            "|    iterations           | 18          |\n",
            "|    time_elapsed         | 542         |\n",
            "|    total_timesteps      | 145800      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012351417 |\n",
            "|    clip_fraction        | 0.11        |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60         |\n",
            "|    explained_variance   | 0.903       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 41.8        |\n",
            "|    n_updates            | 14360       |\n",
            "|    policy_gradient_loss | -0.0155     |\n",
            "|    std                  | 102         |\n",
            "|    value_loss           | 79.8        |\n",
            "-----------------------------------------\n",
            "Selected self_play\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected random_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected adibs_good_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 735         |\n",
            "|    ep_rew_mean          | 627         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 273         |\n",
            "|    iterations           | 19          |\n",
            "|    time_elapsed         | 562         |\n",
            "|    total_timesteps      | 153900      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014718532 |\n",
            "|    clip_fraction        | 0.155       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60.1       |\n",
            "|    explained_variance   | 0.946       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.79        |\n",
            "|    n_updates            | 14370       |\n",
            "|    policy_gradient_loss | -0.0157     |\n",
            "|    std                  | 103         |\n",
            "|    value_loss           | 39.6        |\n",
            "-----------------------------------------\n",
            "Selected sb3_agent1\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_302462_steps.\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 702         |\n",
            "|    ep_rew_mean          | 586         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 279         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 578         |\n",
            "|    total_timesteps      | 162000      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.019883577 |\n",
            "|    clip_fraction        | 0.2         |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60.1       |\n",
            "|    explained_variance   | 0.917       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 16.8        |\n",
            "|    n_updates            | 14380       |\n",
            "|    policy_gradient_loss | -0.0255     |\n",
            "|    std                  | 103         |\n",
            "|    value_loss           | 40.2        |\n",
            "-----------------------------------------\n",
            "Selected adibs_good_agent\n",
            "Selected random_agent\n",
            "Selected jumping_agent\n",
            "Selected sb3_agent1\n",
            "Selected random_agent\n",
            "Selected random_agent\n",
            "Selected jumping_agent\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 726         |\n",
            "|    ep_rew_mean          | 594         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 285         |\n",
            "|    iterations           | 21          |\n",
            "|    time_elapsed         | 595         |\n",
            "|    total_timesteps      | 170100      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018602457 |\n",
            "|    clip_fraction        | 0.205       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -60.1       |\n",
            "|    explained_variance   | 0.903       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 12.9        |\n",
            "|    n_updates            | 14390       |\n",
            "|    policy_gradient_loss | -0.0182     |\n",
            "|    std                  | 103         |\n",
            "|    value_loss           | 24.8        |\n",
            "-----------------------------------------\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected random_agent\n",
            "Selected based_agent\n",
            "Selected sb3_agent1\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "Selected jumping_agent\n",
            "Selected self_play\n",
            "Selected jumping_agent\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 743        |\n",
            "|    ep_rew_mean          | 598        |\n",
            "| time/                   |            |\n",
            "|    fps                  | 291        |\n",
            "|    iterations           | 22         |\n",
            "|    time_elapsed         | 610        |\n",
            "|    total_timesteps      | 178200     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01852377 |\n",
            "|    clip_fraction        | 0.171      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -60.1      |\n",
            "|    explained_variance   | 0.837      |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 26.5       |\n",
            "|    n_updates            | 14400      |\n",
            "|    policy_gradient_loss | -0.0245    |\n",
            "|    std                  | 103        |\n",
            "|    value_loss           | 61.4       |\n",
            "----------------------------------------\n",
            "Saving agent to checkpoints\\experiment_1\\rl_model_320901_steps.\n",
            "[0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n",
            " 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n",
            " 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02\n",
            " 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02] [ 226.4328    222.749883  327.685361  637.377818  275.448786  624.25155\n",
            " 2463.742484  316.151505  347.984402 1142.576245  280.652088 1390.550336\n",
            "  284.720187  305.854052  289.36111   297.759992  251.430751 1365.665474\n",
            "  295.464471  139.384615  302.468156 2077.320091  301.306695  302.601769\n",
            " 1447.370861  327.882386  161.634526  292.844474  240.867646 3455.089767\n",
            "  262.238432  330.270234  195.384615  280.236002  251.430751  240.190263\n",
            "  353.957952  216.261609  347.211227  302.026466  219.840651  271.260493\n",
            " 1596.752039 1201.330008  327.687343 3796.139002  323.861196 1029.033734\n",
            " 1197.136802  274.561453  231.189845 2502.005339  338.447969  246.753398\n",
            "  302.099501  195.384615 2713.857239  347.020591  180.911799  328.512398\n",
            "  302.306103  258.818209  328.79365   314.687803  216.265807 1414.397259\n",
            " 2474.178441  309.665636  303.821541  272.514854  340.311953  269.969268\n",
            "  221.361361  251.430751  301.93691   301.859005  251.430751  280.255123\n",
            "  316.696492  293.043458  323.00352  2568.921719  269.882799  139.384615\n",
            "  280.793392  195.384615  254.739085  315.537288  201.210878  314.973654\n",
            "  327.967909 1406.875757  330.283335  235.334187  316.815015  195.384615\n",
            " 3121.424558  280.396193  225.413994 1154.600647  299.888146  274.359455\n",
            "  301.93691   322.351736  317.290373  276.293753  155.08212   250.796513\n",
            "  336.019519  231.675749 3224.357788  328.307117 2084.088901 2177.805854\n",
            "  316.200608  139.384615  139.384615  318.355476  317.373565 3155.153001\n",
            "  298.392362  316.093191  195.384615  195.384615  329.172879  280.429304\n",
            "  234.940554  271.361655  195.384615  315.614243  950.918701  333.314237\n",
            "  139.384615  342.008446  293.839394  240.023507  234.306316  251.24954\n",
            "  302.431581  313.740935  345.888742  273.199916  272.68949  2303.818385\n",
            "  250.101276  302.838501 1854.338047  270.149073  351.989719  195.384615\n",
            "  352.235554  236.330848 2190.70126   302.326517  330.666989  258.722949\n",
            "  352.063702  333.622903  278.745074  315.514747  327.924156  213.31278\n",
            "  292.76348  1181.152589  195.384615 2805.141936  234.944316  331.020879\n",
            "  324.209256  349.296528 2396.401436  251.552568 2185.474549  275.623552\n",
            " 2173.351122 2700.208312  301.820237  195.384615  301.93691  2208.845488\n",
            "  274.511121  302.446395  156.86914   279.889484  251.643173  302.566192\n",
            "  293.839394  313.383445 3771.919665  298.004187  251.552568  313.745193\n",
            "  331.233374 1152.753821  258.519285  285.325056 1688.220691  345.776358\n",
            "  206.315662  263.097246  258.28121   258.409559  236.059031  269.737386\n",
            "  251.461962  301.93691  2353.747645  251.340146  269.023586  301.158827\n",
            "  320.722848  352.454463  280.411849  312.799589  250.977724  270.917928\n",
            "  251.24954   302.503664  358.217889  317.523512 1155.308002  266.190709\n",
            " 1745.36943   433.371712 1389.204059  335.340248  344.503315  303.122606\n",
            " 1006.489491  275.135361  115.77009   324.115953  328.647616  236.149637\n",
            "  251.874997  362.6014    352.826109  236.059031]\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### NOTE ####\n",
        "### CHANGE THE CHECKPOINT FILE NAME STEPS TO 0 ###\n",
        "### NOTE ####\n",
        "\n",
        "# Basic Training Cycle\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "my_agent = SubmittedAgent(file_path=\"checkpoints/experiment_1/rl_model_0_steps\", device=device)\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "SAVE_CONFIG = {\n",
        "    \"save_freq\": 20_000,\n",
        "    \"max_saved\": 50,\n",
        "    \"save_path\": \"checkpoints\",\n",
        "}\n",
        "\n",
        "# 2. Training setup\n",
        "reward_functions = {\n",
        "    'combo_reward': RewTerm(func=combo_reward, weight=0.5),\n",
        "    'damage_reward': RewTerm(func=damage_reward, weight=0.3),\n",
        "    'damage_penalty': RewTerm(func=damage_penalty, weight=3),\n",
        "    'dodge_reward': RewTerm(func=dodge_reward, weight=4),\n",
        "    'health_comparison': RewTerm(func=health_comparison, weight=1),\n",
        "    'jump_reward': RewTerm(func=jump_reward, weight=1.5),\n",
        "    'escape_reward': RewTerm(func=escape_reward, weight=1),\n",
        "}\n",
        "signal_subscriptions = {\n",
        "    \"on_win\": (\"win_signal\", RewTerm(func=on_win_reward, weight=0.6)),\n",
        "    \"on_knockout\": (\"knockout_signal\",RewTerm(func=knockout_reward, weight=0.7)),\n",
        "}\n",
        "reward_manager = RewardManager(reward_functions, signal_subscriptions)\n",
        "\n",
        "selfplay_classes = [SelfPlayLatest, SelfPlayDynamic, SelfPlayRandom]\n",
        "weights = [3, 1, 1]  # 3:1:1 ratio\n",
        "selected_class = random.choices(selfplay_classes, weights=weights, k=1)[0]\n",
        "selfplay_handler = selected_class(partial(SubmittedAgent))\n",
        "\n",
        "opponents = {\n",
        "    \"self_play\": (0.5, selfplay_handler),\n",
        "    \"based_agent\": (0.05, partial(BasedAgent)),\n",
        "    \"random_agent\": (0.05, partial(RandomAgent)),\n",
        "    \"sb3_agent1\": (0.15, partial(SB3Agent, file_path=\"checkpoints/sb3_1/rl_model_10850567_steps\")),\n",
        "    \"adibs_good_agent\": (0.05, partial(AdibsAgent, file_path=\"checkpoints/adibs_model/rl_model_7605929_steps_good3\")),\n",
        "    \"jumping_agent\": (0.4, partial(SubmittedAgent, file_path=\"checkpoints/experiment_1_0/rl_model_2558357_steps_jumperer\")),\n",
        "}\n",
        "opponent_cfg = OpponentsCfg(opponents=opponents)\n",
        "\n",
        "save_handler = SaveHandler(\n",
        "    agent=my_agent,\n",
        "    run_name='experiment_1',\n",
        "    mode=SaveHandlerMode.RESUME,\n",
        "    **SAVE_CONFIG,\n",
        ")\n",
        "\n",
        "train(my_agent,\n",
        "    reward_manager,\n",
        "    save_handler,\n",
        "    opponent_cfg,\n",
        "    CameraResolution.LOW,\n",
        "    train_timesteps=1_000_000,\n",
        "    train_logging=TrainLogging.PLOT,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Other Model\n",
        "device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "my_agent = SubmittedAgent(file_path=\"checkpoints/experiment_1/rl_model_0_steps\", device=device)\n",
        "print(f\"Training on device: {device}\")\n",
        "\n",
        "SAVE_CONFIG = {\n",
        "    \"save_freq\": 30_000,\n",
        "    \"max_saved\": 7,\n",
        "    \"save_path\": \"checkpoints\",\n",
        "}\n",
        "\n",
        "# 2. Training setup\n",
        "reward_functions = {\n",
        "    'health_comparison': RewTerm(func=health_comparison, weight=1),\n",
        "    'dodge_reward': RewTerm(func=dodge_reward, weight=3.5),\n",
        "    'dodge_buggy': RewTerm(func=dodge_buggy, weight=3.5),\n",
        "    'cute_reward': RewTerm(func=cute_reward, weight=1),\n",
        "    'damage_reward': RewTerm(func=damage_reward, weight=1),\n",
        "    'damage_penalty': RewTerm(func=damage_penalty, weight=2),\n",
        "    'movement_reward': RewTerm(func=movement_reward, weight=1)\n",
        "}\n",
        "signal_subscriptions = {\n",
        "    \"on_win\": (\"win_signal\", RewTerm(func=on_win_reward, weight=1)),\n",
        "    \"on_knockout\": (\"knockout_signal\",RewTerm(func=knockout_reward, weight=1)),\n",
        "}\n",
        "reward_manager = RewardManager(reward_functions, signal_subscriptions)\n",
        "\n",
        "selfplay_classes = [SelfPlayLatest, SelfPlayDynamic, SelfPlayRandom]\n",
        "weights = [3, 1, 1]  # 3:1:1 ratio\n",
        "selected_class = random.choices(selfplay_classes, weights=weights, k=1)[0]\n",
        "selfplay_handler = selected_class(partial(SubmittedAgent))\n",
        "\n",
        "opponents = {\n",
        "    \"self_play\": (0.2, selfplay_handler),\n",
        "    \"based_agent\": (0.1, partial(BasedAgent)),\n",
        "    \"adibs_good_agent\": (0.2, partial(AdibsAgent, file_path=\"checkpoints/adibs_model/rl_model_7605929_steps_good3\")),\n",
        "    \"cutie\": (0.3, partial(SubmittedAgent, file_path=\"checkpoints/experiment_1_0/rl_model_71085114_steps_agressive\")),\n",
        "    \"jumper\": (0.1, partial(SubmittedAgent, file_path=\"checkpoints/experiment_1_0/rl_model_1503561_steps_jumping\")),\n",
        "    \"sb3\": (0.1, partial(SB3Agent, file_path=\"checkpoints/sb3_1/rl_model_10850567_steps\"))\n",
        "}\n",
        "opponent_cfg = OpponentsCfg(opponents=opponents)\n",
        "\n",
        "save_handler = SaveHandler(\n",
        "    agent=my_agent,\n",
        "    run_name='experiment_1',\n",
        "    mode=SaveHandlerMode.RESUME,\n",
        "    **SAVE_CONFIG,\n",
        ")\n",
        "\n",
        "train(my_agent,\n",
        "    reward_manager,\n",
        "    save_handler,\n",
        "    opponent_cfg,\n",
        "    CameraResolution.LOW,\n",
        "    train_timesteps=2_000_000,\n",
        "    train_logging=TrainLogging.PLOT,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jD_M6pfp_Ue"
      },
      "source": [
        "# Evaluation / Inference\n",
        "\n",
        "Test your agents here!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ktPXnaXsqBIs",
        "outputId": "cd31b4f2-0e72-4881-c91c-04f9c8dcbc18"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Obs space [-1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, -1, -1, -1, -1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11, 1, 1, 1, 1, 1, 1, 1, 2, 12, 1, 1, 1, 1, 3, 11]\n",
            "Action space [0, 0, 0, 0, 0, 0, 0, 0, 0, 0] [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
            "video_path=cutie_v_cutie.mp4 -> Rendering\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 2699/2700 [00:51<00:00, 52.00it/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# --- AGENT 1 ---\n",
        "agent1 = \"experiment_1/rl_model_282461_steps\"\n",
        "agent1 = SubmittedAgent(file_path=f\"checkpoints/{agent1}\")\n",
        "\n",
        "# agent1 = \"adibs_model/rl_model_2870138_steps_good2\"\n",
        "# agent1 = AdibsAgent(file_path=f\"checkpoints/{agent1}\")\n",
        "\n",
        "# --- AGENT 2 ---\n",
        "# agent2 = \"adibs_model/rl_model_7605929_steps_good3\"\n",
        "# agent2 = SubmittedAgent(file_path=f\"checkpoints/{agent2}\")\n",
        "\n",
        "agent2 = \"experiment_1_0/rl_model_81688323_steps_cracked\"\n",
        "agent2 = SubmittedAgent(file_path=f\"checkpoints/{agent2}\")\n",
        "# \n",
        "# agent2 = \"experiment_1_0/rl_model_2558357_steps_jumperer\"\n",
        "# agent2 = SubmittedAgent(file_path=f\"checkpoints/{agent2}\")\n",
        "\n",
        "# agent2 = \"adibs_model/rl_model_7605929_steps_good3\"\n",
        "# agent2 = AdibsAgent(file_path=f\"checkpoints/{agent2}\")\n",
        "\n",
        "# agent2 = \"sb3_1/rl_model_10850567_steps\"\n",
        "# agent2 = SB3Agent(file_path=f\"checkpoints/{agent2}\")\n",
        "\n",
        "# agent2 = RandomAgent()\n",
        "\n",
        "# agent2 = UserInputAgent()\n",
        "\n",
        "# agent2 = BasedAgent()\n",
        "\n",
        "# agent2 = \"losmodel/rl_model_1004400_steps\"\n",
        "# agent2 = RecurrentPPOAgent(file_path=f\"checkpoints/{agent2}\")\n",
        "\n",
        "match_time = 90\n",
        "\n",
        "run_match(\n",
        "    agent_1=agent1,\n",
        "    agent_2=agent2,\n",
        "    agent_1_name='Cutie :3',\n",
        "    agent_2_name='Cutie ^.^',\n",
        "    video_path=\"cutie_v_cutie.mp4\",\n",
        "    resolution=CameraResolution.LOW,\n",
        "    reward_manager=reward_manager,\n",
        "    max_timesteps=30 * match_time,\n",
        ")\n",
        "os.system(\"start C:/Users/klamb/OneDrive/Documents/PythonCode/POETS/cutie_v_cutie.mp4\")\n",
        "\n",
        "# run_match(\n",
        "#     agent_1=agent1,\n",
        "#     agent_2=agent2,\n",
        "#     agent_1_name='Cutie :3',\n",
        "#     video_path=\"cutie_v_adibs.mp4\",\n",
        "#     agent_2_name='Adibs',\n",
        "#     resolution=CameraResolution.LOW,\n",
        "#     reward_manager=reward_manager,\n",
        "#     max_timesteps=30 * match_time,\n",
        "# )\n",
        "# os.system(\"start C:/Users/klamb/OneDrive/Documents/PythonCode/POETS/cutie_v_adibs.mp4\")\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "vNxUlQXyFNKb",
        "XjJxbq1oq8qj",
        "XL4UJviLrA9D",
        "oFPXjtaYpiPY",
        "z7ekY33Ka_h8",
        "hkU-Jg8iZSTC",
        "p7_2qwFINY3f",
        "JbPFpq8lNXdN",
        "ygKmVZpfsB8c",
        "3o43Fo3ssDPj",
        "yZwI-S1X3P9L",
        "ECELO0ke3LGO",
        "jww67QlGd4GL",
        "GHx_GIg5a8x0",
        "PNweZj-Wc_zc",
        "JwnWPBG_nQeg",
        "j7lJmFQncGw7",
        "UzQW3ZFWjYdP",
        "z_joKiMJlVSC",
        "6YTVrhr0dRvv",
        "OPOX5xjac5XC",
        "d6fV-UuVEsGz",
        "vRxHMA07ZKE0",
        "J_-olBqx4-J7",
        "ZYrlmB7ymkBY",
        "bGcNyfuaZWZL",
        "2khY6i4Iq297",
        "YlEWjV-ere9z",
        "83MTxzRvh58N",
        "bpulsG6R6eGG",
        "MRa5uPgibFAm",
        "WxD2EcJGrjvj",
        "404LO62Oqsqz",
        "tyCz9XRL0tLW",
        "NdErHuFqNSFl",
        "-MMYCnbOrXs6",
        "4b_3pL3gk9gI",
        "i7wH-mhyfDpZ"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
